# Redis Refresh: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å –¥–ª—è DevOps/SysAdmin

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ Redis –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞)**: –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
2. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è**: –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
3. **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞)**: –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

**–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- redis-cli —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–∏–ª–∏ –¥–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ Docker)
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è CLI
- Python –∏–ª–∏ –¥—Ä—É–≥–æ–π —è–∑—ã–∫ –¥–ª—è –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

---

## –ú–æ–¥—É–ª—å 1: –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Redis - —á—Ç–æ —ç—Ç–æ:**
```
Redis (Remote Dictionary Server)
‚îú‚îÄ‚îÄ In-Memory Data Structure Store
‚îú‚îÄ‚îÄ Key-Value –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –û–¥–Ω–æ–ø–æ—Ç–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (single-threaded)
‚îî‚îÄ‚îÄ Persistence –æ–ø—Ü–∏–∏ (RDB, AOF)
```

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
```bash
# –°–∫–æ—Ä–æ—Å—Ç—å
- –û–ø–µ—Ä–∞—Ü–∏–∏: ~100,000 ops/sec –Ω–∞ –æ–¥–Ω–æ–º —è–¥—Ä–µ
- Latency: sub-millisecond
- In-Memory: –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ RAM

# Persistence
- RDB: Point-in-time snapshots
- AOF: Append-Only File (–ª–æ–≥ –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π)
- Hybrid: RDB + AOF

# –†–µ–ø–ª–∏–∫–∞—Ü–∏—è
- Master-Slave (—Ç–µ–ø–µ—Ä—å Primary-Replica)
- Async —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è
- Redis Sentinel –¥–ª—è HA
- Redis Cluster –¥–ª—è —à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏—è
```

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫:**
```bash
# Docker (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏)
docker run -d --name redis-test \
  -p 6379:6379 \
  redis:7-alpine

# –° persistent storage
docker run -d --name redis-persist \
  -p 6379:6379 \
  -v redis-data:/data \
  redis:7-alpine redis-server --appendonly yes

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker exec -it redis-test redis-cli ping
# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å: PONG

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ
docker stop redis-test
docker rm redis-test
docker volume rm redis-data
```

**redis-cli –æ—Å–Ω–æ–≤—ã:**
```bash
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
redis-cli
redis-cli -h localhost -p 6379
redis-cli -a password  # –° –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π

# –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã
PING                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
INFO                    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–µ—Ä–µ
INFO stats              # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
INFO memory             # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
CONFIG GET *            # –í—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG GET maxmemory    # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
DBSIZE                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π
KEYS *                  # –í—Å–µ –∫–ª—é—á–∏ (–æ–ø–∞—Å–Ω–æ –≤ prod!)
SCAN 0 COUNT 10         # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–π
FLUSHDB                 # –û—á–∏—Å—Ç–∏—Ç—å —Ç–µ–∫—É—â—É—é –ë–î
FLUSHALL                # –û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ –ë–î
SHUTDOWN                # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ—Ä–≤–µ—Ä
```

**–ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –∫–ª—é—á–∞–º–∏:**
```bash
# SET/GET
SET key "value"
GET key
SETNX key "value"       # SET if Not eXists
SETEX key 60 "value"    # SET —Å TTL –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
GETSET key "new"        # GET —Å—Ç–∞—Ä–æ–µ –∏ SET –Ω–æ–≤–æ–µ
MSET k1 "v1" k2 "v2"   # Multiple SET
MGET k1 k2             # Multiple GET

# TTL (Time To Live)
EXPIRE key 60           # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å TTL –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
EXPIREAT key 1735689600 # Expire –≤ Unix timestamp
TTL key                 # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è
PERSIST key             # –£–±—Ä–∞—Ç—å TTL
PTTL key               # TTL –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

# –£–¥–∞–ª–µ–Ω–∏–µ
DEL key                 # –£–¥–∞–ª–∏—Ç—å –∫–ª—é—á
DEL key1 key2 key3     # –£–¥–∞–ª–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ
UNLINK key             # Async —É–¥–∞–ª–µ–Ω–∏–µ (–ª—É—á—à–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–ª—é—á–µ–π)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
EXISTS key              # 1 –µ—Å–ª–∏ –µ—Å—Ç—å, 0 –µ—Å–ª–∏ –Ω–µ—Ç
EXISTS k1 k2 k3        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
RENAME old new          # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å
RENAMENX old new       # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å if new –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

# –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö
TYPE key               # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∏–ø –∑–Ω–∞—á–µ–Ω–∏—è
```

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
```bash
# redis.conf –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
bind 0.0.0.0                    # –ù–∞ –∫–∞–∫–∏—Ö IP —Å–ª—É—à–∞—Ç—å
port 6379                       # –ü–æ—Ä—Ç
daemonize yes                   # –ó–∞–ø—É—Å–∫ –∫–∞–∫ –¥–µ–º–æ–Ω
pidfile /var/run/redis.pid      # PID —Ñ–∞–π–ª
loglevel notice                 # debug, verbose, notice, warning
logfile /var/log/redis.log      # –õ–æ–≥ —Ñ–∞–π–ª

# Memory
maxmemory 2gb                   # –ú–∞–∫—Å–∏–º—É–º –ø–∞–º—è—Ç–∏
maxmemory-policy allkeys-lru    # –ü–æ–ª–∏—Ç–∏–∫–∞ eviction

# Persistence
save 900 1                      # RDB: save –ø–æ—Å–ª–µ 900 —Å–µ–∫ –µ—Å–ª–∏ 1 –∏–∑–º–µ–Ω–µ–Ω–∏–µ
save 300 10                     # RDB: save –ø–æ—Å–ª–µ 300 —Å–µ–∫ –µ—Å–ª–∏ 10 –∏–∑–º–µ–Ω–µ–Ω–∏–π
save 60 10000                   # RDB: save –ø–æ—Å–ª–µ 60 —Å–µ–∫ –µ—Å–ª–∏ 10000 –∏–∑–º–µ–Ω–µ–Ω–∏–π
appendonly yes                  # –í–∫–ª—é—á–∏—Ç—å AOF
appendfsync everysec            # AOF sync: always, everysec, no

# Security
requirepass yourpassword        # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–æ–ª—å
rename-command CONFIG ""        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å/–æ—Ç–∫–ª—é—á–∏—Ç—å –∫–æ–º–∞–Ω–¥—ã
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ü–æ–¥–≥–æ—Ç–æ–≤—å —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:

1. –ó–∞–ø—É—Å—Ç–∏ Redis –≤ Docker:
   ```bash
   docker run -d --name redis-practice \
     -p 6379:6379 \
     redis:7-alpine redis-server --loglevel verbose
   ```

2. –ü–æ–¥–∫–ª—é—á–∏—Å—å –∫ Redis:
   ```bash
   docker exec -it redis-practice redis-cli
   ```

3. –í—ã–ø–æ–ª–Ω–∏ –±–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:
   ```bash
   # –ü—Ä–æ–≤–µ—Ä—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
   PING
   
   # –£—Å—Ç–∞–Ω–æ–≤–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–π
   SET user:1000:name "John Doe"
   SET user:1000:email "john@example.com"
   SET user:1000:age 30
   
   # –£—Å—Ç–∞–Ω–æ–≤–∏ –∫–ª—é—á —Å TTL
   SETEX session:abc123 3600 "session_data"
   
   # –ü—Ä–æ–≤–µ—Ä—å TTL
   TTL session:abc123
   
   # –ü–æ–ª—É—á–∏ –∑–Ω–∞—á–µ–Ω–∏—è
   GET user:1000:name
   MGET user:1000:name user:1000:email user:1000:age
   
   # –ü—Ä–æ–≤–µ—Ä—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Ä–≤–µ—Ä–µ
   INFO server
   INFO stats
   INFO memory
   
   # –ü–æ—Å–º–æ—Ç—Ä–∏ –≤—Å–µ –∫–ª—é—á–∏
   KEYS *
   
   # –£–∑–Ω–∞–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π
   DBSIZE
   ```

4. –ü—Ä–æ–≤–µ—Ä—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:
   ```bash
   INFO memory
   # –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞:
   # - used_memory_human
   # - used_memory_peak_human
   # - mem_fragmentation_ratio
   ```

5. –ü–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π —Å TTL:
   ```bash
   SET temp "test"
   EXPIRE temp 10
   TTL temp
   # –ü–æ–¥–æ–∂–¥–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥
   TTL temp
   GET temp
   # –ü–æ–¥–æ–∂–¥–∏ –ø–æ–∫–∞ –∏—Å—Ç–µ—á–µ—Ç
   GET temp  # –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å (nil)
   ```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò—Å–ø–æ–ª—å–∑—É–π Redis Stack** (Redis —Å –º–æ–¥—É–ª—è–º–∏):
```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏ —Å—Ç–∞—Ä—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker stop redis-practice
docker rm redis-practice

# –ó–∞–ø—É—Å—Ç–∏ Redis Stack (–≤–∫–ª—é—á–∞–µ—Ç JSON, Search, TimeSeries –∏ –¥—Ä.)
docker run -d --name redis-stack \
  -p 6379:6379 \
  -p 8001:8001 \
  redis/redis-stack:latest

# –ü–æ–¥–∫–ª—é—á–∏—Å—å
docker exec -it redis-stack redis-cli

# –ü–æ–ø—Ä–æ–±—É–π RedisJSON
JSON.SET user:2000 $ '{"name":"Jane","age":25,"city":"NYC"}'
JSON.GET user:2000
JSON.GET user:2000 $.name
```

**2. –ù–∞—Å—Ç—Ä–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é redis-cli:**
```bash
# MONITOR - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—Å–µ –∫–æ–º–∞–Ω–¥—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
redis-cli MONITOR

# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –≤—ã–ø–æ–ª–Ω–∏ –∫–æ–º–∞–Ω–¥—ã –∏ —É–≤–∏–¥–∏—à—å –∏—Ö –≤ MONITOR

# SLOWLOG - –ª–æ–≥–∏ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
CONFIG SET slowlog-log-slower-than 1000  # –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã
CONFIG SET slowlog-max-len 128
SLOWLOG GET 10  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
SLOWLOG LEN     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –ª–æ–≥–µ
SLOWLOG RESET   # –û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥
```

**3. –ò—Å–ø–æ–ª—å–∑—É–π redis-cli –≤ —Å–∫—Ä–∏–ø—Ç–∞—Ö:**
```bash
# –ú–∞—Å—Å–æ–≤–∞—è –≤—Å—Ç–∞–≤–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
cat <<EOF > data.txt
SET key1 "value1"
SET key2 "value2"
SET key3 "value3"
EOF

cat data.txt | redis-cli --pipe

# –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –∫–ª—é—á–µ–π
redis-cli --scan --pattern 'user:*' | while read key; do
  echo "SET $key \"$(redis-cli GET $key)\""
done > backup.redis

# Benchmark
redis-cli --intrinsic-latency 100  # Latency –±–∞–∑–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
redis-bench -t set,get -n 100000 -q  # Benchmark SET/GET
```

---

## –ú–æ–¥—É–ª—å 2: –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö Redis:**

**1. String (—Å—Ç—Ä–æ–∫–∏ –∏ —á–∏—Å–ª–∞):**
```bash
# –°—Ç—Ä–æ–∫–∏
SET name "John"
GET name
APPEND name " Doe"          # "John Doe"
STRLEN name                 # 8

# –ß–∏—Å–ª–∞
SET counter 0
INCR counter                # 1
INCRBY counter 5            # 6
DECR counter                # 5
DECRBY counter 2            # 3
INCRBYFLOAT price 0.5       # –î–ª—è float

# Bits
SETBIT key 10 1             # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–∏—Ç
GETBIT key 10               # –ü–æ–ª—É—á–∏—Ç—å –±–∏—Ç
BITCOUNT key                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –±–∏—Ç–æ–≤
```

**2. List (—Å–ø–∏—Å–∫–∏):**
```bash
# –û–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–µ–≤–∞ (head)
LPUSH tasks "task1"         # –î–æ–±–∞–≤–∏—Ç—å —Å–ª–µ–≤–∞
LPUSH tasks "task2" "task3" # –î–æ–±–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ
LPOP tasks                  # –ò–∑–≤–ª–µ—á—å —Å–ª–µ–≤–∞

# –û–ø–µ—Ä–∞—Ü–∏–∏ —Å–ø—Ä–∞–≤–∞ (tail)
RPUSH tasks "task4"         # –î–æ–±–∞–≤–∏—Ç—å —Å–ø—Ä–∞–≤–∞
RPOP tasks                  # –ò–∑–≤–ª–µ—á—å —Å–ø—Ä–∞–≤–∞

# –ß—Ç–µ–Ω–∏–µ
LRANGE tasks 0 -1           # –í—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
LRANGE tasks 0 10           # –ü–µ—Ä–≤—ã–µ 10
LINDEX tasks 0              # –≠–ª–µ–º–µ–Ω—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É
LLEN tasks                  # –î–ª–∏–Ω–∞ —Å–ø–∏—Å–∫–∞

# –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è
LSET tasks 0 "new_task"     # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É
LINSERT tasks BEFORE "task1" "new"  # –í—Å—Ç–∞–≤–∏—Ç—å –ø–µ—Ä–µ–¥
LTRIM tasks 0 99            # –û–±—Ä–µ–∑–∞—Ç—å –¥–æ 100 —ç–ª–µ–º–µ–Ω—Ç–æ–≤

# –ë–ª–æ–∫–∏—Ä—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (–¥–ª—è –æ—á–µ—Ä–µ–¥–µ–π)
BLPOP tasks 5               # –ë–ª–æ–∫–∏—Ä—É—é—â–∏–π pop, timeout 5 —Å–µ–∫
BRPOP tasks 5
BRPOPLPUSH source dest 5    # Atomic move –º–µ–∂–¥—É —Å–ø–∏—Å–∫–∞–º–∏
```

**3. Set (–º–Ω–æ–∂–µ—Å—Ç–≤–∞):**
```bash
# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ/—É–¥–∞–ª–µ–Ω–∏–µ
SADD tags "redis" "database" "nosql"
SREM tags "nosql"           # –£–¥–∞–ª–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç
SPOP tags                   # –ò–∑–≤–ª–µ—á—å —Å–ª—É—á–∞–π–Ω—ã–π
SPOP tags 2                 # –ò–∑–≤–ª–µ—á—å 2 —Å–ª—É—á–∞–π–Ω—ã—Ö

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —á—Ç–µ–Ω–∏–µ
SISMEMBER tags "redis"      # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ
SMEMBERS tags               # –í—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
SCARD tags                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
SRANDMEMBER tags 3          # 3 —Å–ª—É—á–∞–π–Ω—ã—Ö –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è

# –û–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏
SADD set1 "a" "b" "c"
SADD set2 "b" "c" "d"
SUNION set1 set2            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: a,b,c,d
SINTER set1 set2            # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ: b,c
SDIFF set1 set2             # –†–∞–∑–Ω–æ—Å—Ç—å: a

# –° —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
SUNIONSTORE result set1 set2
SINTERSTORE result set1 set2
SDIFFSTORE result set1 set2
```

**4. Hash (—Ö–µ—à-—Ç–∞–±–ª–∏—Ü—ã):**
```bash
# –û–ø–µ—Ä–∞—Ü–∏–∏
HSET user:1000 name "John"
HSET user:1000 email "john@example.com" age 30
HMSET user:1001 name "Jane" email "jane@example.com"  # Deprecated, –∏—Å–ø–æ–ª—å–∑—É–π HSET

# –ß—Ç–µ–Ω–∏–µ
HGET user:1000 name
HMGET user:1000 name email
HGETALL user:1000           # –í—Å–µ –ø–æ–ª—è
HKEYS user:1000             # –í—Å–µ –∫–ª—é—á–∏
HVALS user:1000             # –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è
HLEN user:1000              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π

# –ü—Ä–æ–≤–µ—Ä–∫–∞
HEXISTS user:1000 name      # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª—è
HSETNX user:1000 name "New" # SET if Not eXists

# –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç
HINCRBY user:1000 age 1
HINCRBYFLOAT user:1000 balance 10.50

# –£–¥–∞–ª–µ–Ω–∏–µ
HDEL user:1000 age
```

**5. Sorted Set (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞):**
```bash
# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ (score member)
ZADD leaderboard 100 "player1"
ZADD leaderboard 200 "player2" 150 "player3"

# –ß—Ç–µ–Ω–∏–µ –ø–æ —Ä–∞–Ω–≥—É (–ø–æ–∑–∏—Ü–∏–∏)
ZRANGE leaderboard 0 -1              # –í—Å–µ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é score
ZRANGE leaderboard 0 -1 WITHSCORES   # –° –æ—á–∫–∞–º–∏
ZREVRANGE leaderboard 0 9 WITHSCORES # Top 10 –ø–æ —É–±—ã–≤–∞–Ω–∏—é

# –ß—Ç–µ–Ω–∏–µ –ø–æ score
ZRANGEBYSCORE leaderboard 100 200    # score –æ—Ç 100 –¥–æ 200
ZRANGEBYSCORE leaderboard -inf +inf  # –í—Å–µ
ZREVRANGEBYSCORE leaderboard 200 100 # –í –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ
ZCARD leaderboard               # –í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
ZCOUNT leaderboard 100 200      # –° score –æ—Ç 100 –¥–æ 200

# Score –æ–ø–µ—Ä–∞—Ü–∏–∏
ZSCORE leaderboard "player1"    # –ü–æ–ª—É—á–∏—Ç—å score
ZINCRBY leaderboard 50 "player1" # –£–≤–µ–ª–∏—á–∏—Ç—å score

# –†–∞–Ω–≥ (–ø–æ–∑–∏—Ü–∏—è)
ZRANK leaderboard "player1"     # –ü–æ–∑–∏—Ü–∏—è (—Å 0)
ZREVRANK leaderboard "player1"  # –ü–æ–∑–∏—Ü–∏—è –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ

# –£–¥–∞–ª–µ–Ω–∏–µ
ZREM leaderboard "player1"      # –ü–æ member
ZREMRANGEBYRANK leaderboard 0 2 # –ü–æ —Ä–∞–Ω–≥—É
ZREMRANGEBYSCORE leaderboard 0 100  # –ü–æ score

# –û–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏
ZUNIONSTORE result 2 zset1 zset2 WEIGHTS 2 3  # Union —Å –≤–µ—Å–∞–º–∏
ZINTERSTORE result 2 zset1 zset2               # Intersection
```

**–ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∫–ª—é—á–µ–π:**
```bash
# –ò—Å–ø–æ–ª—å–∑—É–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
user:1000:profile
user:1000:settings
user:1000:sessions:abc123

# –î–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–π
users:active        # Set –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
users:by_country:US # Set –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ US

# –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
session:abc123:data
cache:user:1000:feed

# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
user:1000:created_at
user:1000:last_login
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–†–µ–∞–ª–∏–∑—É–π –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ—Ç—å":

1. **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (Hash):**
```bash
# –°–æ–∑–¥–∞–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
HSET user:1000 username "john_doe" email "john@example.com" name "John Doe" followers 0 following 0
HSET user:1001 username "jane_smith" email "jane@example.com" name "Jane Smith" followers 0 following 0
HSET user:1002 username "bob_wilson" email "bob@example.com" name "Bob Wilson" followers 0 following 0

# –°–æ–∑–¥–∞–π –∏–Ω–¥–µ–∫—Å username -> id
SET username:john_doe 1000
SET username:jane_smith 1001
SET username:bob_wilson 1002
```

2. **–ü–æ–¥–ø–∏—Å–∫–∏ (Set):**
```bash
# john –ø–æ–¥–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –Ω–∞ jane –∏ bob
SADD user:1000:following 1001 1002
SADD user:1001:followers 1000
SADD user:1002:followers 1000

# –û–±–Ω–æ–≤–∏ —Å—á–µ—Ç—á–∏–∫–∏
HINCRBY user:1000 following 2
HINCRBY user:1001 followers 1
HINCRBY user:1002 followers 1

# –ü—Ä–æ–≤–µ—Ä—å –≤–∑–∞–∏–º–Ω—É—é –ø–æ–¥–ø–∏—Å–∫—É
SINTER user:1000:following user:1001:following

# –ù–∞–π–¥–∏ –æ–±—â–∏—Ö –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
SADD user:1003:followers 1000 1001
SINTER user:1002:followers user:1003:followers
```

3. **–ü–æ—Å—Ç—ã (List + Hash):**
```bash
# –°–æ–∑–¥–∞–π –ø–æ—Å—Ç
INCR post:id  # –ì–µ–Ω–µ—Ä–∏—Ä—É–π ID
HSET post:1 user_id 1000 content "Hello Redis!" likes 0 created_at "2024-01-15T10:00:00Z"

# –î–æ–±–∞–≤—å –≤ –ª–µ–Ω—Ç—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
LPUSH user:1000:posts 1

# –î–æ–±–∞–≤—å –≤ –ª–µ–Ω—Ç—ã –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤ (fan-out on write)
LPUSH user:1001:feed 1
LPUSH user:1002:feed 1

# –ü–æ–ª—É—á–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –ø–æ—Å—Ç–æ–≤
LRANGE user:1001:feed 0 9
```

4. **–õ–∞–π–∫–∏ (Set):**
```bash
# jane –ª–∞–π–∫–∞–µ—Ç –ø–æ—Å—Ç
SADD post:1:likes 1001
HINCRBY post:1 likes 1

# –ü—Ä–æ–≤–µ—Ä—å, –ª–∞–π–∫–Ω—É–ª –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
SISMEMBER post:1:likes 1001

# –ü–æ–ª—É—á–∏ –≤—Å–µ—Ö –∫—Ç–æ –ª–∞–π–∫–Ω—É–ª
SMEMBERS post:1:likes
```

5. **–¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º (Sorted Set):**
```bash
# –î–æ–±–∞–≤—å –≤ —Ä–µ–π—Ç–∏–Ω–≥ (score = followers)
ZADD users:by_followers 1 1002
ZADD users:by_followers 1 1001
ZADD users:by_followers 2 1000

# –¢–æ–ø 10
ZREVRANGE users:by_followers 0 9 WITHSCORES

# –û–±–Ω–æ–≤–∏ –∫–æ–≥–¥–∞ followers –º–µ–Ω—è–µ—Ç—Å—è
ZINCRBY users:by_followers 1 1001
```

6. **–ü—Ä–æ–≤–µ—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:**
```bash
# –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
HGETALL user:1000

# –ü–æ–¥–ø–∏—Å—á–∏–∫–∏
SMEMBERS user:1000:followers
SCARD user:1000:followers

# –õ–µ–Ω—Ç–∞
LRANGE user:1001:feed 0 -1

# –ü–æ—Å—Ç —Å –¥–µ—Ç–∞–ª—è–º–∏
HGETALL post:1

# –¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
ZREVRANGE users:by_followers 0 -1 WITHSCORES
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò—Å–ø–æ–ª—å–∑—É–π Streams –¥–ª—è –ª–µ–Ω—Ç—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (Redis 5.0+):**
```bash
# –°–æ–∑–¥–∞–π stream —Å–æ–±—ã—Ç–∏–π
XADD user:1000:activity * action "posted" post_id 1 timestamp "2024-01-15T10:00:00Z"
XADD user:1000:activity * action "liked" post_id 2 timestamp "2024-01-15T10:05:00Z"

# –ß–∏—Ç–∞–π –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
XRANGE user:1000:activity - + COUNT 10
XREVRANGE user:1000:activity + - COUNT 10

# –ß–∏—Ç–∞–π –Ω–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è (–∫–∞–∫ consumer)
XREAD COUNT 10 STREAMS user:1000:activity 0

# Consumer groups –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
XGROUP CREATE user:1000:activity mygroup 0
XREADGROUP GROUP mygroup consumer1 COUNT 10 STREAMS user:1000:activity >

# –î–ª–∏–Ω–∞ stream
XLEN user:1000:activity

# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–æ–±—ã—Ç–∏–π
XTRIM user:1000:activity MAXLEN ~ 1000
```

**2. –ò—Å–ø–æ–ª—å–∑—É–π HyperLogLog –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤:**
```bash
# –î–æ–±–∞–≤—å –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –ø–æ—Å—Ç–∞
PFADD post:1:views user:1000 user:1001 user:1002
PFADD post:1:views user:1000  # –î—É–±–ª–∏–∫–∞—Ç –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è

# –ü–æ–ª—É—á–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
PFCOUNT post:1:views

# –û–±—ä–µ–¥–∏–Ω–∏ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Å—Ç–æ–≤
PFADD post:2:views user:1003 user:1004
PFMERGE total:views post:1:views post:2:views
PFCOUNT total:views
```

**3. Geo –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:**
```bash
# –î–æ–±–∞–≤—å –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
GEOADD users:locations -122.27652 37.805186 user:1000  # San Francisco
GEOADD users:locations -118.24368 34.05223 user:1001   # Los Angeles
GEOADD users:locations -0.12574 51.50853 user:1002     # London

# –ù–∞–π–¥–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Ä—è–¥–æ–º (–≤ —Ä–∞–¥–∏—É—Å–µ 200 –∫–º)
GEORADIUS users:locations -122.27652 37.805186 200 km WITHDIST

# –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏
GEODIST users:locations user:1000 user:1001 km
```

---
## –ú–æ–¥—É–ª—å 3: Persistence –∏ Backup (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)

bash

```bash
# –ü–æ—Å–º–æ—Ç—Ä–∏ AOF —Ñ–∞–π–ª (—á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π)
docker exec -it redis-aof cat /data/appendonly.aof

# –°–¥–µ–ª–∞–π –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π rewrite
BGREWRITEAOF

# –ü—Ä–æ–≤–µ—Ä—å —Ä–∞–∑–º–µ—Ä –¥–æ –∏ –ø–æ—Å–ª–µ
docker exec -it redis-aof ls -lh /data/
```

6. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ AOF:**

bash

```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
docker stop redis-aof

# –ó–∞–ø—É—Å—Ç–∏ —Å–Ω–æ–≤–∞
docker start redis-aof

# –ü–æ–¥–∫–ª—é—á–∏—Å—å –∏ –ø—Ä–æ–≤–µ—Ä—å –¥–∞–Ω–Ω—ã–µ
docker exec -it redis-aof redis-cli
KEYS *
SMEMBERS tags
ZRANGE scores 0 -1 WITHSCORES
# –í—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ –º–µ—Å—Ç–µ
```

7. **–°–æ–∑–¥–∞–π backup —Å–∫—Ä–∏–ø—Ç:**

bash

```bash
cat > backup-redis.sh <<'EOF'
#!/bin/bash
BACKUP_DIR="/backup/redis"
DATE=$(date +%Y%m%d-%H%M%S)

mkdir -p $BACKUP_DIR

# RDB backup
docker exec redis-aof redis-cli BGSAVE
sleep 5
docker cp redis-aof:/data/dump.rdb $BACKUP_DIR/dump-$DATE.rdb

# AOF backup
docker exec redis-aof redis-cli BGREWRITEAOF
sleep 5
docker cp redis-aof:/data/appendonly.aof $BACKUP_DIR/appendonly-$DATE.aof

# –£–¥–∞–ª–∏ —Å—Ç–∞—Ä—ã–µ backup (—Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π)
find $BACKUP_DIR -name "*.rdb" -mtime +7 -delete
find $BACKUP_DIR -name "*.aof" -mtime +7 -delete

echo "Backup completed: $DATE"
EOF

chmod +x backup-redis.sh
./backup-redis.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò—Å–ø–æ–ª—å–∑—É–π Redis 7.0+ RDB —Å AOF timestamp:**

bash

```bash
# –í redis.conf
aof-use-rdb-preamble yes

# –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ñ–∞–π–ª:
# - –ù–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ = RDB snapshot (–±—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
# - –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ = AOF –∫–æ–º–∞–Ω–¥—ã –ø–æ—Å–ª–µ snapshot
# –õ—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤!
```

**2. Automatic failover backup —Å —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–µ–π:**

bash

```bash
# –ó–∞–ø—É—Å—Ç–∏ master
docker run -d --name redis-master \
  -p 6379:6379 \
  redis:7-alpine

# –ó–∞–ø—É—Å—Ç–∏ replica –¥–ª—è backup
docker run -d --name redis-backup \
  -p 6380:6379 \
  redis:7-alpine redis-server \
  --replicaof redis-master 6379 \
  --save 900 1 \
  --appendonly yes

# Backup –¥–µ–ª–∞–µ—Ç—Å—è –Ω–∞ replica –±–µ–∑ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ master
docker exec redis-backup redis-cli BGSAVE
```

**3. Point-in-Time Recovery (PITR):**

bash

```bash
# –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω AOF, –º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∞ –ª—é–±—É—é —Ç–æ—á–∫—É –≤—Ä–µ–º–µ–Ω–∏
# 1. –û—Å—Ç–∞–Ω–æ–≤–∏ Redis
# 2. –ù–∞–π–¥–∏ –Ω—É–∂–Ω—É—é –ø–æ–∑–∏—Ü–∏—é –≤ AOF –ø–æ timestamp
# 3. –û–±—Ä–µ–∂—å AOF —Ñ–∞–π–ª –¥–æ —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏
# 4. –ó–∞–ø—É—Å—Ç–∏ Redis

# –ü—Ä–∏–º–µ—Ä: –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∞ 10:00
docker exec redis-aof redis-cli SHUTDOWN
docker exec redis-aof sh -c "head -n 1000 /data/appendonly.aof > /data/appendonly-restored.aof"
# –ó–∞–º–µ–Ω–∏ —Ñ–∞–π–ª—ã –∏ –∑–∞–ø—É—Å—Ç–∏
```

---

## –ú–æ–¥—É–ª—å 4: –†–µ–ø–ª–∏–∫–∞—Ü–∏—è –∏ High Availability (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Master-Replica —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è:**

bash

````bash
# –ù–∞ Replica
replicaof <master-ip> <master-port>
replicaof 192.168.1.100 6379

# –ò–ª–∏ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥—É
REPLICAOF 192.168.1.100 6379
REPLICAOF NO ONE  # –°—Ç–∞—Ç—å master

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏
replica-read-only yes          # Replica —Ç–æ–ª—å–∫–æ –¥–ª—è —á—Ç–µ–Ω–∏—è
replica-serve-stale-data yes   # –û—Ç–¥–∞–≤–∞—Ç—å —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ —Å–≤—è–∑—å –ø–æ—Ç–µ—Ä—è–Ω–∞
repl-diskless-sync no          # Sync —á–µ—Ä–µ–∑ –¥–∏—Å–∫ –∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é
repl-backlog-size 1mb          # –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –¥–ª—è partial resync
min-replicas-to-write 1        # –ú–∏–Ω–∏–º—É–º —Ä–µ–ø–ª–∏–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
min-replicas-max-lag 10        # –ú–∞–∫—Å lag –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
```

**–ü—Ä–æ—Ü–µ—Å—Å —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏:**
```
1. Replica –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Master
2. Master –¥–µ–ª–∞–µ—Ç BGSAVE (RDB snapshot)
3. Master –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç RDB –Ω–∞ Replica
4. Replica –∑–∞–≥—Ä—É–∂–∞–µ—Ç RDB
5. Master –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –±—É—Ñ–µ—Ä –∫–æ–º–∞–Ω–¥ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è sync
6. Replica –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—ã
7. Continuous sync: Master –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ –∫–æ–º–∞–Ω–¥—ã –Ω–∞ Replica
````

**Redis Sentinel (HA):**

bash

```bash
# sentinel.conf
sentinel monitor mymaster 192.168.1.100 6379 2
sentinel auth-pass mymaster yourpassword
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 10000

# –ó–∞–ø—É—Å–∫ Sentinel
redis-sentinel /path/to/sentinel.conf

# –ö–æ–º–∞–Ω–¥—ã Sentinel
SENTINEL masters
SENTINEL master mymaster
SENTINEL replicas mymaster
SENTINEL sentinels mymaster
SENTINEL failover mymaster  # –†—É—á–Ω–æ–π failover
SENTINEL reset mymaster     # Reset —Å–æ—Å—Ç–æ—è–Ω–∏—è
```

**Redis Cluster (—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ):**

bash

```bash
# –ú–∏–Ω–∏–º—É–º 3 master + 3 replica = 6 –Ω–æ–¥
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000

# –°–æ–∑–¥–∞–Ω–∏–µ cluster
redis-cli --cluster create \
  127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 \
  127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \
  --cluster-replicas 1

# –ö–æ–º–∞–Ω–¥—ã cluster
CLUSTER INFO
CLUSTER NODES
CLUSTER SLOTS
CLUSTER KEYSLOT key  # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–ª–æ—Ç –¥–ª—è –∫–ª—é—á–∞

# Resharding
redis-cli --cluster reshard 127.0.0.1:7000
```

**Monitoring —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏:**

bash

```bash
# –ù–∞ Master
INFO replication

# –í—ã–≤–æ–¥:
# role:master
# connected_slaves:2
# slave0:ip=192.168.1.101,port=6379,state=online,offset=1234,lag=0
# slave1:ip=192.168.1.102,port=6379,state=online,offset=1234,lag=0

# –ù–∞ Replica
INFO replication
# role:slave
# master_host:192.168.1.100
# master_port:6379
# master_link_status:up
# master_last_io_seconds_ago:0
# master_sync_in_progress:0
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é Master-Replica:

1. **–ó–∞–ø—É—Å—Ç–∏ Master:**

bash

```bash
docker network create redis-net

docker run -d --name redis-master \
  --network redis-net \
  -p 6379:6379 \
  redis:7-alpine redis-server \
  --appendonly yes \
  --requirepass masterpass
```

2. **–ó–∞–ø—É—Å—Ç–∏ 2 Replicas:**

bash

```bash
docker run -d --name redis-replica1 \
  --network redis-net \
  -p 6380:6379 \
  redis:7-alpine redis-server \
  --replicaof redis-master 6379 \
  --masterauth masterpass \
  --requirepass replicapass \
  --replica-read-only yes

docker run -d --name redis-replica2 \
  --network redis-net \
  -p 6381:6379 \
  redis:7-alpine redis-server \
  --replicaof redis-master 6379 \
  --masterauth masterpass \
  --requirepass replicapass \
  --replica-read-only yes
```

3. **–ü—Ä–æ–≤–µ—Ä—å —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é:**

bash

```bash
# –ù–∞ Master
docker exec -it redis-master redis-cli -a masterpass
INFO replication
# –î–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å 2 connected slaves

# –î–æ–±–∞–≤—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ Master
SET test:1 "value1"
SET test:2 "value2"
SADD myset "a" "b" "c"
```

4. **–ü—Ä–æ–≤–µ—Ä—å –Ω–∞ Replicas:**

bash

```bash
# Replica 1
docker exec -it redis-replica1 redis-cli -a replicapass
GET test:1
SMEMBERS myset

# Replica 2
docker exec -it redis-replica2 redis-cli -a replicapass
GET test:2
SMEMBERS myset

# –ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø–∏—Å–∞—Ç—å –Ω–∞ Replica (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å readonly)
SET test:3 "value3"
# –û—à–∏–±–∫–∞: READONLY You can't write against a read only replica
```

5. **–¢–µ—Å—Ç–∏—Ä—É–π failover –≤—Ä—É—á–Ω—É—é:**

bash

```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏ Master
docker stop redis-master

# –ü–æ–≤—ã—Å—å Replica1 –¥–æ Master
docker exec -it redis-replica1 redis-cli -a replicapass
REPLICAOF NO ONE
INFO replication  # role –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å master

# –ü–µ—Ä–µ–∫–ª—é—á–∏ Replica2 –Ω–∞ –Ω–æ–≤—ã–π Master
docker exec -it redis-replica2 redis-cli -a replicapass
REPLICAOF redis-replica1 6379

# –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—à—å –ø–∏—Å–∞—Ç—å –≤ –Ω–æ–≤—ã–π Master
docker exec -it redis-replica1 redis-cli -a replicapass
SET test:3 "value3"

# –ü—Ä–æ–≤–µ—Ä—å –Ω–∞ Replica2
docker exec -it redis-replica2 redis-cli -a replicapass
GET test:3
```

6. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ lag:**

bash

```bash
# –°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
cat > monitor-replication.sh <<'EOF'
#!/bin/bash
while true; do
  echo "=== $(date) ==="
  docker exec redis-master redis-cli -a masterpass INFO replication | grep -E "role|connected_slaves|slave[0-9]"
  echo ""
  sleep 5
done
EOF

chmod +x monitor-replication.sh
./monitor-replication.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π Redis Sentinel –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ failover:**

bash

```bash
# sentinel1.conf
cat > sentinel1.conf <<EOF
port 26379
sentinel monitor mymaster redis-master 6379 2
sentinel auth-pass mymaster masterpass
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 10000
EOF

# –ó–∞–ø—É—Å—Ç–∏ 3 Sentinel
docker run -d --name sentinel1 \
  --network redis-net \
  -p 26379:26379 \
  -v $(pwd)/sentinel1.conf:/etc/sentinel.conf \
  redis:7-alpine redis-sentinel /etc/sentinel.conf

docker run -d --name sentinel2 \
  --network redis-net \
  -p 26380:26379 \
  -v $(pwd)/sentinel1.conf:/etc/sentinel.conf \
  redis:7-alpine redis-sentinel /etc/sentinel.conf

docker run -d --name sentinel3 \
  --network redis-net \
  -p 26381:26379 \
  -v $(pwd)/sentinel1.conf:/etc/sentinel.conf \
  redis:7-alpine redis-sentinel /etc/sentinel.conf

# –ü—Ä–æ–≤–µ—Ä—å Sentinel
docker exec -it sentinel1 redis-cli -p 26379
SENTINEL masters
SENTINEL master mymaster
SENTINEL replicas mymaster

# –¢–µ—Å—Ç–∏—Ä—É–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π failover
docker stop redis-master
# Sentinel –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–≤—ã—Å–∏—Ç –æ–¥–Ω—É –∏–∑ —Ä–µ–ø–ª–∏–∫
```

**2. Redis Cluster (3 master + 3 replica):**

bash

```bash
# –°–æ–∑–¥–∞–π 6 –Ω–æ–¥
for port in 7000 7001 7002 7003 7004 7005; do
  docker run -d --name redis-$port \
    --network redis-net \
    -p $port:6379 \
    redis:7-alpine redis-server \
    --cluster-enabled yes \
    --cluster-config-file nodes.conf \
    --cluster-node-timeout 5000 \
    --appendonly yes \
    --port 6379
done

# –°–æ–∑–¥–∞–π cluster
docker exec -it redis-7000 redis-cli --cluster create \
  redis-7000:6379 redis-7001:6379 redis-7002:6379 \
  redis-7003:6379 redis-7004:6379 redis-7005:6379 \
  --cluster-replicas 1

# –ü—Ä–æ–≤–µ—Ä—å cluster
docker exec -it redis-7000 redis-cli -c
CLUSTER INFO
CLUSTER NODES

# –¢–µ—Å—Ç–∏—Ä—É–π sharding
SET user:1000 "data"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ø–∞–¥–µ—Ç –Ω–∞ –Ω—É–∂–Ω—ã–π shard
GET user:1000
```

---

## –ú–æ–¥—É–ª—å 5: Performance –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Memory Management:**

bash

```bash
# –ü–æ–ª–∏—Ç–∏–∫–∏ eviction (–∫–æ–≥–¥–∞ memory full)
maxmemory-policy noeviction      # –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –∑–∞–ø–∏—Å–∏
maxmemory-policy allkeys-lru     # –£–¥–∞–ª—è—Ç—å –ª—é–±—ã–µ –∫–ª—é—á–∏ (LRU)
maxmemory-policy allkeys-lfu     # –£–¥–∞–ª—è—Ç—å –ª—é–±—ã–µ –∫–ª—é—á–∏ (LFU - least frequently used)
maxmemory-policy allkeys-random  # –£–¥–∞–ª—è—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –∫–ª—é—á–∏
maxmemory-policy volatile-lru    # –£–¥–∞–ª—è—Ç—å —Ç–æ–ª—å–∫–æ —Å TTL (LRU)
maxmemory-policy volatile-lfu    # –£–¥–∞–ª—è—Ç—å —Ç–æ–ª—å–∫–æ —Å TTL (LFU)
maxmemory-policy volatile-random # –£–¥–∞–ª—è—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ —Å TTL
maxmemory-policy volatile-ttl    # –£–¥–∞–ª—è—Ç—å —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º TTL

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
# - allkeys-lru: –¥–ª—è –∫–µ—à–∞
# - volatile-lru: –¥–ª—è –∫–µ—à–∞ + persistent –¥–∞–Ω–Ω—ã—Ö
# - noeviction: –¥–ª—è –ë–î –≥–¥–µ –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–∞
```

**Memory –∞–Ω–∞–ª–∏–∑:**

bash

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
INFO memory
MEMORY STATS
MEMORY DOCTOR  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

# –ü–∞–º—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞
MEMORY USAGE key
MEMORY USAGE key SAMPLES 5  # –° —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–π

# Fragmentation
INFO memory | grep fragmentation
# mem_fragmentation_ratio > 1.5 = –ø—Ä–æ–±–ª–µ–º–∞
# –†–µ—à–µ–Ω–∏–µ: —Ä–µ—Å—Ç–∞—Ä—Ç Redis –∏–ª–∏ MEMORY PURGE (Redis 4.0+)
```

**Slow Log:**

bash

```bash
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞
CONFIG SET slowlog-log-slower-than 10000  # –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã (10ms)
CONFIG SET slowlog-max-len 128

# –ü—Ä–æ—Å–º–æ—Ç—Ä
SLOWLOG GET 10
SLOWLOG LEN
SLOWLOG RESET

# –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞:
# 1) id
# 2) timestamp
# 3) execution time (microseconds)
# 4) command + args
# 5) client IP:port
# 6) client name
```

**Latency Monitoring:**

bash

```bash
# –í–∫–ª—é—á–∏—Ç—å
CONFIG SET latency-monitor-threshold 100  # –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã

# –°–æ–±—ã—Ç–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
LATENCY DOCTOR   # –ê–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
LATENCY LATEST   # –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
LATENCY HISTORY command  # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∫–æ–º–∞–Ω–¥—ã
LATENCY GRAPH command    # ASCII –≥—Ä–∞—Ñ–∏–∫
LATENCY RESET

# –°–æ–±—ã—Ç–∏—è:
# - command: –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
# - fast-command: –±—ã—Å—Ç—Ä—ã–µ –∫–æ–º–∞–Ω–¥—ã —Å –≤—Å–ø–ª–µ—Å–∫–∞–º–∏
# - fork: fork –¥–ª—è BGSAVE/BGREWRITEAOF
# - aof-write: –∑–∞–ø–∏—Å—å –≤ AOF
# - aof-fsync-always: fsync AOF
```

**–û–ø–∞—Å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã (–∏–∑–±–µ–≥–∞–π –≤ production):**

bash

```bash
KEYS *           # –ë–ª–æ–∫–∏—Ä—É–µ—Ç —Å–µ—Ä–≤–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–π SCAN
FLUSHALL/FLUSHDB # –£–¥–∞–ª—è–µ—Ç –≤—Å—ë
SAVE             # –ë–ª–æ–∫–∏—Ä—É—é—â–∏–π save, –∏—Å–ø–æ–ª—å–∑—É–π BGSAVE
SMEMBERS large_set  # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å—ë –º–Ω–æ–∂–µ—Å—Ç–≤–æ, –∏—Å–ø–æ–ª—å–∑—É–π SSCAN
HGETALL large_hash  # –¢–æ –∂–µ –¥–ª—è hash
ZRANGE key 0 -1    # –¢–æ –∂–µ –¥–ª—è sorted set
DEBUG commands   # –¢–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
```

**–ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:**

bash

```bash
# –í–º–µ—Å—Ç–æ KEYS –∏—Å–ø–æ–ª—å–∑—É–π SCAN
SCAN 0 MATCH user:* COUNT 100
# –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç —Å–µ—Ä–≤–µ—Ä

# –í–º–µ—Å—Ç–æ SMEMBERS –∏—Å–ø–æ–ª—å–∑—É–π SSCAN
SSCAN myset 0 COUNT 100

# –í–º–µ—Å—Ç–æ HGETALL –∏—Å–ø–æ–ª—å–∑—É–π HSCAN
HSCAN myhash 0 COUNT 100

# –í–º–µ—Å—Ç–æ ZRANGE –∏—Å–ø–æ–ª—å–∑—É–π ZSCAN
ZSCAN myzset 0 COUNT 100
```

**Pipelining –∏ –º–∞—Å—Å–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:**

bash

```bash
# –ë–µ–∑ pipelining: N round-trips
SET key1 val1
SET key2 val2
SET key3 val3

# –° pipelining: 1 round-trip
# –í redis-cli –∏—Å–ø–æ–ª—å–∑—É–π --pipe
echo -e "SET key1 val1\nSET key2 val2\nSET key3 val3" | redis-cli --pipe

# –ú–∞—Å—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã
MSET key1 val1 key2 val2 key3 val3
MGET key1 key2 key3
```

**Benchmarking:**

bash

```bash
# –ë–∞–∑–æ–≤—ã–π benchmark
redis-benchmark -t set,get -n 100000 -q

# –° —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
redis-benchmark -t set,get -n 100000 -d 100 -q

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
redis-benchmark -t set -n 100000 -P 16 -q  # –° pipelining

# –¢–æ–ª—å–∫–æ latency
redis-benchmark -t ping -c 1 -n 100000

# –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç
redis-benchmark -h localhost -p 6379 -a password --csv
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Redis:

1. **–ù–∞—Å—Ç—Ä–æ–π memory management:**

bash

```bash
docker run -d --name redis-optimized \
  -p 6379:6379 \
  redis:7-alpine redis-server \
  --maxmemory 256mb \
  --maxmemory-policy allkeys-lru \
  --save "" \
  --appendonly no
```

2. **–ó–∞–ø–æ–ª–Ω–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π memory:**

bash

```bash
docker exec -it redis-optimized redis-cli

# –°–æ–∑–¥–∞–π —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
# Strings
for i in {1..1000}; do SET string:$i "value_$i"; done

# Hashes (–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤)
for i in {1..1000}; do HMSET user:$i name "user$i" email "user$i@example.com" age $((20 + RANDOM % 50)); done

# Lists
for i in {1..100}; do LPUSH list:$i item1 item2 item3 item4 item5; done

# Sets
for i in {1..100}; do SADD set:$i member1 member2 member3 member4 member5; done

# Sorted Sets
for i in {1..100}; do ZADD zset:$i 1 member1 2 member2 3 member3; done

# –ü—Ä–æ–≤–µ—Ä—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
INFO memory
DBSIZE

# –ü–∞–º—è—Ç—å –ø–æ —Ç–∏–ø–∞–º
MEMORY USAGE string:1
MEMORY USAGE user:1
MEMORY USAGE list:1
MEMORY USAGE set:1
MEMORY USAGE zset:1
```

3. **–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**

bash

```bash
# Hash –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω —á–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏
# –ü–ª–æ—Ö–æ: 1000 –∫–ª—é—á–µ–π
SET user:1:name "John"
SET user:1:email "john@example.com"
SET user:1:age 30

# –•–æ—Ä–æ—à–æ: 1 –∫–ª—é—á
HMSET user:1 name "John" email "john@example.com" age 30

# –°—Ä–∞–≤–Ω–∏ –ø–∞–º—è—Ç—å
MEMORY USAGE user:1:name
MEMORY USAGE user:1
```

4. **–ù–∞—Å—Ç—Ä–æ–π –∏ –ø—Ä–æ–≤–µ—Ä—å slow log:**

bash

```bash
# –ù–∞—Å—Ç—Ä–æ–π slow log
CONFIG SET slowlog-log-slower-than 1000  # 1ms
CONFIG SET slowlog-max-len 128

# –í—ã–ø–æ–ª–Ω–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
DEBUG SLEEP 0.01
KEYS *  # –≠—Ç–æ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–æ

# –ü—Ä–æ–≤–µ—Ä—å slow log
SLOWLOG GET 10

# –î–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å:
# 1) ID –∑–∞–ø–∏—Å–∏
# 2) Timestamp
# 3) Execution time –≤ –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥–∞—Ö
# 4) –ö–æ–º–∞–Ω–¥–∞ –∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
```

5. **Benchmark –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:**

bash

```bash
# –ë–∞–∑–æ–≤—ã–π benchmark
docker exec redis-optimized redis-benchmark -t set,get -n 100000 -q

# –° —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
docker exec redis-optimized redis-benchmark -t set,get -n 100000 -d 10 -q
docker exec redis-optimized redis-benchmark -t set,get -n 100000 -d 100 -q
docker exec redis-optimized redis-benchmark -t set,get -n 100000 -d 1000 -q

# –° pipelining
docker exec redis-optimized redis-benchmark -t set -n 100000 -P 16 -q

# Latency —Ç–µ—Å—Ç
docker exec redis-optimized redis-benchmark -t ping -c 1 -n 10000

# –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –≤ CSV
docker exec redis-optimized redis-benchmark --csv > benchmark-results.csv
```

6. **–ò—Å–ø–æ–ª—å–∑—É–π SCAN –≤–º–µ—Å—Ç–æ KEYS:**

bash

```bash
# –ü–ª–æ—Ö–æ (–±–ª–æ–∫–∏—Ä—É–µ—Ç —Å–µ—Ä–≤–µ—Ä)
KEYS user:*

# –•–æ—Ä–æ—à–æ (–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π)
SCAN 0 MATCH user:* COUNT 100

# –°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ scan
cursor=0
while true; do
  result=$(redis-cli SCAN $cursor MATCH "user:*" COUNT 100)
  cursor=$(echo "$result" | head -1)
  echo "$result" | tail -n +2
  if [ "$cursor" = "0" ]; then
    break
  fi
done
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Redis —Å –∞–ª–ª–æ–∫–∞—Ç–æ—Ä–æ–º jemalloc (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é):**

bash

```bash
# –ü—Ä–æ–≤–µ—Ä—å —Ç–µ–∫—É—â–∏–π –∞–ª–ª–æ–∫–∞—Ç–æ—Ä
INFO memory | grep allocator

# jemalloc –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è Redis
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ redis.conf:
# (–æ–±—ã—á–Ω–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, —É–∂–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ fragmentation
watch -n 1 'redis-cli INFO memory | grep fragmentation'
```

**2. –ò—Å–ø–æ–ª—å–∑—É–π Redis –º–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:**

bash

```bash
# RedisInsight - GUI –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
docker run -d --name redisinsight \
  -p 8001:8001 \
  redislabs/redisinsight:latest

# –û—Ç–∫—Ä–æ–π http://localhost:8001
# –î–æ–±–∞–≤—å —Å–≤–æ–π Redis —Å–µ—Ä–≤–µ—Ä
# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π:
# - Memory usage –ø–æ –∫–ª—é—á–∞–º
# - Slow commands
# - Key patterns
# - Recommendations
```

**3. Active defragmentation (Redis 4.0+):**

bash

```bash
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
CONFIG SET activedefrag yes
CONFIG SET active-defrag-ignore-bytes 100mb
CONFIG SET active-defrag-threshold-lower 10
CONFIG SET active-defrag-threshold-upper 100
CONFIG SET active-defrag-cycle-min 5
CONFIG SET active-defrag-cycle-max 75

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
INFO memory | grep defrag
```

---

## –ú–æ–¥—É–ª—å 6: Security –∏ Production Best Practices (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è:**

bash

```bash
# –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä–æ–ª—å
requirepass your_strong_password

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
redis-cli -a your_strong_password
AUTH your_strong_password

# ACL (Redis 6.0+) - –±–æ–ª–µ–µ –≥–∏–±–∫–æ
# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
ACL SETUSER alice on >password123 ~cached:* +get +set
ACL SETUSER bob on >secret456 ~* +@all -@dangerous

# –ü—Ä–∞–≤–∏–ª–∞:
# on/off - –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
# >password - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–æ–ª—å
# ~pattern - —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏ (* = –≤—Å–µ)
# +command - —Ä–∞–∑—Ä–µ—à–∏—Ç—å –∫–æ–º–∞–Ω–¥—É
# -command - –∑–∞–ø—Ä–µ—Ç–∏—Ç—å –∫–æ–º–∞–Ω–¥—É
# +@category - —Ä–∞–∑—Ä–µ—à–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é (@all, @read, @write, @dangerous)
# -@category - –∑–∞–ø—Ä–µ—Ç–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é
```

**ACL –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä
ACL LIST                  # –í—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏ –ø—Ä–∞–≤–∏–ª–∞
ACL USERS                 # –°–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
ACL GETUSER alice         # –ü—Ä–∞–≤–∏–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
ACL CAT                   # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–º–∞–Ω–¥
ACL CAT dangerous         # –ö–æ–º–∞–Ω–¥—ã –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
ACL SETUSER username ...  # –°–æ–∑–¥–∞—Ç—å/–∏–∑–º–µ–Ω–∏—Ç—å
ACL DELUSER username      # –£–¥–∞–ª–∏—Ç—å
ACL WHOAMI               # –¢–µ–∫—É—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
ACL SAVE                  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ aclfile
CONFIG SET aclfile /path/to/users.acl
```

**–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ–ø–∞—Å–Ω—ã—Ö –∫–æ–º–∞–Ω–¥:**

bash

```bash
# –í redis.conf
rename-command FLUSHDB ""              # –û—Ç–∫–ª—é—á–∏—Ç—å
rename-command FLUSHALL ""
rename-command CONFIG "CONFIG_secret"   # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å
rename-command KEYS ""

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
CONFIG_secret GET maxmemory
```

**Network Security:**

bash

```bash
# Bind –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º
bind 127.0.0.1 ::1        # –¢–æ–ª—å–∫–æ localhost
bind 0.0.0.0              # –í—Å–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã (–æ–ø–∞—Å–Ω–æ –±–µ–∑ firewall)
bind 10.0.1.100 10.0.1.101  # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ IP

# Protected mode (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ 3.2+)
protected-mode yes
# –ï—Å–ª–∏ –Ω–µ—Ç bind 127.0.0.1 –∏ –Ω–µ—Ç –ø–∞—Ä–æ–ª—è - –æ—Ç–∫–ª–æ–Ω—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

# –û—Ç–∫–ª—é—á–∏—Ç—å bind (—Ç–æ–ª—å–∫–æ –¥–ª—è dev)
bind 0.0.0.0
protected-mode no
# –ù–ï –î–ï–õ–ê–ô –¢–ê–ö –í PRODUCTION!
```

**TLS/SSL (Redis 6.0+):**

bash

```bash
# –í redis.conf
port 0                    # –û—Ç–∫–ª—é—á–∏—Ç—å –æ–±—ã—á–Ω—ã–π –ø–æ—Ä—Ç
tls-port 6379
tls-cert-file /path/to/redis.crt
tls-key-file /path/to/redis.key
tls-ca-cert-file /path/to/ca.crt
tls-auth-clients yes      # –¢—Ä–µ–±–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å TLS
redis-cli --tls \
  --cert /path/to/client.crt \
  --key /path/to/client.key \
  --cacert /path/to/ca.crt
```

**Firewall –ø—Ä–∞–≤–∏–ª–∞ (iptables):**

bash

```bash
# –†–∞–∑—Ä–µ—à–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö IP
iptables -A INPUT -p tcp -s 10.0.1.0/24 --dport 6379 -j ACCEPT
iptables -A INPUT -p tcp --dport 6379 -j DROP

# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π firewalld
firewall-cmd --permanent --zone=public --add-rich-rule='
  rule family="ipv4"
  source address="10.0.1.0/24"
  port protocol="tcp" port="6379" accept'
```

**Production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**

bash

```bash
# redis.conf –¥–ª—è production
# Memory
maxmemory 2gb
maxmemory-policy allkeys-lru

# Persistence (–≤—ã–±–µ—Ä–∏ –æ–¥–Ω–æ –∏–ª–∏ –æ–±–∞)
# RDB
save 900 1
save 300 10
save 60 10000
# AOF
appendonly yes
appendfsync everysec
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Security
requirepass strong_password_here
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command CONFIG "CONFIG_a1b2c3"
bind 127.0.0.1
protected-mode yes

# Network
tcp-backlog 511
timeout 0
tcp-keepalive 300

# Limits
maxclients 10000

# Logging
loglevel notice
logfile /var/log/redis/redis-server.log

# Slow log
slowlog-log-slower-than 10000
slowlog-max-len 128

# Latency
latency-monitor-threshold 100

# Performance
# –û—Ç–∫–ª—é—á–∏ persistence –¥–ª—è pure cache
# save ""
# appendonly no
```

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–ª–µ—Ä—Ç—ã:**

bash

```bash
# Metrics –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
INFO stats | grep -E "total_commands_processed|instantaneous_ops_per_sec"
INFO memory | grep -E "used_memory_human|mem_fragmentation_ratio"
INFO replication | grep -E "role|connected_slaves|master_link_status"
INFO persistence | grep -E "rdb_last_save_time|aof_enabled"
INFO clients | grep -E "connected_clients|blocked_clients"

# –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
# - used_memory / maxmemory
# - mem_fragmentation_ratio (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å < 1.5)
# - connected_clients / maxclients
# - evicted_keys (–µ—Å–ª–∏ > 0, –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)
# - rejected_connections
# - master_link_status (–¥–ª—è replica)
# - rdb_last_save_time, aof_last_rewrite_time
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π production Redis:

1. **–°–æ–∑–¥–∞–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å security:**

bash

```bash
cat > redis-secure.conf <<EOF
# Network
bind 127.0.0.1
protected-mode yes
port 6379
tcp-backlog 511
timeout 300
tcp-keepalive 60

# Security
requirepass SecurePassword123!
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command CONFIG "CONFIG_xyz789"
rename-command DEBUG ""
rename-command SHUTDOWN "SHUTDOWN_xyz789"

# Memory
maxmem 512mb
maxmemory-policy allkeys-lru

# Persistence
appendonly yes appendfsync everysec save 900 1 save 300 10

# Logging
loglevel notice logfile ""

# Performance monitoring
slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 100

# Limits
maxclients 1000 EOF
```


2. **–ó–∞–ø—É—Å—Ç–∏ —Å —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:**
```bash
docker run -d --name redis-secure \
  -p 6379:6379 \
  -v $(pwd)/redis-secure.conf:/usr/local/etc/redis/redis.conf \
  redis:7-alpine redis-server /usr/local/etc/redis/redis.conf
```

3. **–ù–∞—Å—Ç—Ä–æ–π ACL (Redis 6.0+):**
```bash
# –ü–æ–¥–∫–ª—é—á–∏—Å—å —Å –ø–∞—Ä–æ–ª–µ–º
docker exec -it redis-secure redis-cli -a SecurePassword123!

# –°–æ–∑–¥–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
ACL SETUSER admin on >AdminPass123 ~* +@all
ACL SETUSER readonly on >ReadPass123 ~* +@read -@write -@dangerous
ACL SETUSER app on >AppPass123 ~app:* +@all -@dangerous -@admin
ACL SETUSER cache on >CachePass123 ~cache:* +get +set +del +expire

# –ü—Ä–æ–≤–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
ACL LIST
ACL USERS

# –°–æ—Ö—Ä–∞–Ω–∏ ACL
ACL SAVE

# –¢–µ—Å—Ç–∏—Ä—É–π readonly –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
AUTH readonly ReadPass123
GET key  # OK
SET key "value"  # Error: NOPERM

# –¢–µ—Å—Ç–∏—Ä—É–π app –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
AUTH app AppPass123
SET app:user:1 "data"  # OK
SET other:key "data"  # Error: NOPERM
FLUSHDB  # Error: NOPERM
```

4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ security:**
```bash
# –°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
cat > monitor-security.sh <<'EOF'
#!/bin/bash

REDIS_PASS="SecurePassword123!"

while true; do
  echo "=== Security Monitor $(date) ==="
  
  # Connected clients
  echo "Connected clients:"
  docker exec redis-secure redis-cli -a $REDIS_PASS CLIENT LIST | wc -l
  
  # Rejected connections
  echo "Rejected connections:"
  docker exec redis-secure redis-cli -a $REDIS_PASS INFO stats | grep rejected_connections
  
  # Auth failures (–µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥—É–ª—å)
  # docker exec redis-secure redis-cli -a $REDIS_PASS ACL LOG 10
  
  # Slow commands
  echo "Slow commands:"
  docker exec redis-secure redis-cli -a $REDIS_PASS SLOWLOG LEN
  
  echo ""
  sleep 10
done
EOF

chmod +x monitor-security.sh
./monitor-security.sh
```

5. **–°–æ–∑–¥–∞–π health check —Å–∫—Ä–∏–ø—Ç:**
```bash
cat > healthcheck.sh <<'EOF'
#!/bin/bash

REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_PASS="SecurePassword123!"

# Ping test
if ! redis-cli -h $REDIS_HOST -p $REDIS_PORT -a $REDIS_PASS PING > /dev/null 2>&1; then
  echo "CRITICAL: Redis not responding"
  exit 2
fi

# Memory check
USED_MEMORY=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT -a $REDIS_PASS INFO memory | grep used_memory: | cut -d: -f2 | tr -d '\r')
MAX_MEMORY=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT -a $REDIS_PASS CONFIG GET maxmemory | tail -1)

if [ "$MAX_MEMORY" != "0" ]; then
  MEMORY_PCT=$((USED_MEMORY * 100 / MAX_MEMORY))
  if [ $MEMORY_PCT -gt 90 ]; then
    echo "WARNING: Memory usage ${MEMORY_PCT}%"
  fi
fi

# Replication check (if replica)
ROLE=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT -a $REDIS_PASS INFO replication | grep role: | cut -d: -f2 | tr -d '\r')
if [ "$ROLE" = "slave" ]; then
  MASTER_STATUS=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT -a $REDIS_PASS INFO replication | grep master_link_status: | cut -d: -f2 | tr -d '\r')
  if [ "$MASTER_STATUS" != "up" ]; then
    echo "CRITICAL: Master link down"
    exit 2
  fi
fi

echo "OK: Redis healthy"
exit 0
EOF

chmod +x healthcheck.sh
./healthcheck.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π TLS –¥–ª—è Redis:**
```bash
# –ì–µ–Ω–µ—Ä–∏—Ä—É–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã
openssl genrsa -out ca-key.pem 4096
openssl req -new -x509 -days 3650 -key ca-key.pem -out ca-cert.pem \
  -subj "/CN=RedisCA"

openssl genrsa -out redis-key.pem 4096
openssl req -new -key redis-key.pem -out redis.csr \
  -subj "/CN=redis-server"
openssl x509 -req -days 3650 -in redis.csr -CA ca-cert.pem \
  -CAkey ca-key.pem -CAcreateserial -out redis-cert.pem

# redis-tls.conf
cat > redis-tls.conf <<EOF
port 0
tls-port 6379
tls-cert-file /tls/redis-cert.pem
tls-key-file /tls/redis-key.pem
tls-ca-cert-file /tls/ca-cert.pem
tls-auth-clients no
requirepass SecurePassword123!
EOF

# –ó–∞–ø—É—Å—Ç–∏ Redis —Å TLS
docker run -d --name redis-tls \
  -p 6379:6379 \
  -v $(pwd)/redis-tls.conf:/usr/local/etc/redis/redis.conf \
  -v $(pwd):/tls \
  redis:7-alpine redis-server /usr/local/etc/redis/redis.conf

# –ü–æ–¥–∫–ª—é—á–∏—Å—å —Å TLS
redis-cli --tls --cert ./redis-cert.pem --key ./redis-key.pem \
  --cacert ./ca-cert.pem -a SecurePassword123!
```

**2. Redis —Å systemd hardening:**
```bash
# /etc/systemd/system/redis.service.d/hardening.conf
cat > /etc/systemd/system/redis.service.d/hardening.conf <<EOF
[Service]
# Security
NoNewPrivileges=true
PrivateTmp=true
PrivateDevices=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/var/lib/redis /var/log/redis
ProtectKernelTunables=true
ProtectKernelModules=true
ProtectControlGroups=true
RestrictRealtime=true
RestrictNamespaces=true
LockPersonality=true
MemoryDenyWriteExecute=true
RestrictAddressFamilies=AF_INET AF_INET6 AF_UNIX
SystemCallFilter=@system-service
SystemCallErrorNumber=EPERM

# Resource limits
LimitNOFILE=65535
LimitNPROC=65535
EOF

systemctl daemon-reload
systemctl restart redis
```

**3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Vault –¥–ª—è secrets:**
```bash
# –•—Ä–∞–Ω–∏ –ø–∞—Ä–æ–ª–∏ –≤ Vault –≤–º–µ—Å—Ç–æ –∫–æ–Ω—Ñ–∏–≥–∞
# –ò—Å–ø–æ–ª—å–∑—É–π init script –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–æ–ª—è

cat > redis-with-vault.sh <<'EOF'
#!/bin/bash
# –ü–æ–ª—É—á–∏ –ø–∞—Ä–æ–ª—å –∏–∑ Vault
REDIS_PASS=$(vault kv get -field=password secret/redis/prod)

# –ó–∞–ø—É—Å—Ç–∏ Redis —Å –ø–∞—Ä–æ–ª–µ–º –∏–∑ Vault
redis-server --requirepass "$REDIS_PASS"
EOF
```

---

## –ú–æ–¥—É–ª—å 7: Patterns –∏ Use Cases (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**1. Caching Pattern:**
```python
import redis
import json

r = redis.Redis(host='localhost', port=6379, decode_responses=True)

def get_user(user_id):
    # Cache-aside pattern
    cache_key = f"user:{user_id}"
    
    # Try cache first
    cached = r.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Cache miss - get from DB
    user = get_from_database(user_id)
    
    # Store in cache with TTL
    r.setex(cache_key, 3600, json.dumps(user))
    
    return user

# Write-through pattern
def update_user(user_id, data):
    # Update DB
    update_database(user_id, data)
    
    # Update cache
    cache_key = f"user:{user_id}"
    r.setex(cache_key, 3600, json.dumps(data))
```

**2. Session Store:**
```python
def create_session(user_id):
    session_id = generate_session_id()
    session_key = f"session:{session_id}"
    
    session_data = {
        "user_id": user_id,
        "created_at": time.time(),
        "ip": request.remote_addr
    }
    
    # Session expires in 1 hour
    r.setex(session_key, 3600, json.dumps(session_data))
    return session_id

def get_session(session_id):
    session_key = f"session:{session_id}"
    data = r.get(session_key)
    if data:
        # Refresh TTL on access
        r.expire(session_key, 3600)
        return json.loads(data)
    return None
```

**3. Rate Limiting:**
```python
def rate_limit(user_id, max_requests=100, window=3600):
    key = f"rate_limit:{user_id}"
    
    # Sliding window counter
    current = r.incr(key)
    
    if current == 1:
        # First request - set expiry
        r.expire(key, window)
    
    return current <= max_requests

# Token bucket algorithm
def token_bucket_limit(user_id, tokens=10, refill_rate=1):
    key = f"tokens:{user_id}"
    
    current_tokens = r.get(key)
    if current_tokens is None:
        r.set(key, tokens - 1)
        r.expire(key, 60)
        return True
    
    current_tokens = int(current_tokens)
    if current_tokens > 0:
        r.decr(key)
        return True
    
    return False
```

**4. Distributed Lock:**
```python
import uuid
import time

def acquire_lock(lock_name, timeout=10):
    lock_key = f"lock:{lock_name}"
    identifier = str(uuid.uuid4())
    
    # SET NX (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç) + EX (TTL)
    acquired = r.set(lock_key, identifier, nx=True, ex=timeout)
    
    return identifier if acquired else None

def release_lock(lock_name, identifier):
    lock_key = f"lock:{lock_name}"
    
    # –ò—Å–ø–æ–ª—å–∑—É–π Lua script –¥–ª—è atomic –æ–ø–µ—Ä–∞—Ü–∏–∏
    lua_script = """
    if redis.call("get", KEYS[1]) == ARGV[1] then
        return redis.call("del", KEYS[1])
    else
        return 0
    end
    """
    
    r.eval(lua_script, 1, lock_key, identifier)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
lock_id = acquire_lock("critical_section")
if lock_id:
    try:
        # Critical section
        perform_critical_operation()
    finally:
        release_lock("critical_section", lock_id)
```

**5. Pub/Sub Messaging:**
```python
# Publisher
def publish_message(channel, message):
    r.publish(channel, json.dumps(message))

# Subscriber
def subscribe_to_channel(channel):
    pubsub = r.pubsub()
    pubsub.subscribe(channel)
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            handle_message(data)
```

**6. Leaderboard:**
```python
def add_score(user_id, score):
    r.zadd("leaderboard", {user_id: score})

def get_rank(user_id):
    # Rank (0-based, ascending)
    rank = r.zrank("leaderboard", user_id)
    return rank + 1 if rank is not None else None

def get_top_players(count=10):
    # Top players (descending)
    return r.zrevrange("leaderboard", 0, count-1, withscores=True)

def get_around_user(user_id, range=2):
    rank = r.zrank("leaderboard", user_id)
    if rank is None:
        return []
    
    start = max(0, rank - range)
    end = rank + range
    return r.zrange("leaderboard", start, end, withscores=True)
```

**7. Time Series Data:**
```python
def record_metric(metric_name, value, timestamp=None):
    if timestamp is None:
        timestamp = int(time.time())
    
    key = f"metrics:{metric_name}"
    r.zadd(key, {f"{timestamp}:{value}": timestamp})
    
    # –£–¥–∞–ª–∏ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (> 24 —á–∞—Å–∞)
    r.zremrangebyscore(key, 0, timestamp - 86400)

def get_metrics(metric_name, start_time, end_time):
    key = f"metrics:{metric_name}"
    data = r.zrangebyscore(key, start_time, end_time)
    
    return [(int(item.split(':')[0]), float(item.split(':')[1])) 
            for item in data]
```

**8. Job Queue:**
```python
def enqueue_job(queue_name, job_data):
    job_id = str(uuid.uuid4())
    job_key = f"job:{job_id}"
    
    # Store job data
    r.hset(job_key, mapping={
        "id": job_id,
        "data": json.dumps(job_data),
        "status": "pending",
        "created_at": time.time()
    })
    
    # Add to queue
    r.lpush(f"queue:{queue_name}", job_id)
    return job_id

def process_job(queue_name, timeout=30):
    # Blocking pop from queue
    result = r.brpop(f"queue:{queue_name}", timeout)
    
    if result:
        queue, job_id = result
        job_key = f"job:{job_id}"
        
        # Get job data
        job_data = r.hgetall(job_key)
        
        # Update status
        r.hset(job_key, "status", "processing")
        
        return job_id, json.loads(job_data["data"])
    
    return None, None
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–†–µ–∞–ª–∏–∑—É–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö use cases:

1. **Cache-aside pattern –¥–ª—è API:**
```python
import redis
import json
import time

r = redis.Redis(host='localhost', port=6379, decode_responses=True)

# –°–∏–º—É–ª—è—Ü–∏—è –º–µ–¥–ª–µ–Ω–Ω–æ–π –ë–î
def get_from_db(key):
    time.sleep(1)  # –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
    return {"data": f"value_for_{key}", "timestamp": time.time()}

def cached_get(key, ttl=60):
    cache_key = f"cache:{key}"
    
    # Try cache
    cached = r.get(cache_key)
    if cached:
        print(f"Cache HIT for {key}")
        return json.loads(cached)
    
    # Cache miss
    print(f"Cache MISS for {key}")
    data = get_from_db(key)
    
    # Store in cache
    r.setex(cache_key, ttl, json.dumps(data))
    return data

# –¢–µ—Å—Ç
start = time.time()
data1 = cached_get("user:1000")  # Miss, ~1 sec
print(f"First call: {time.time() - start:.2f}s")

start = time.time()
data2 = cached_get("user:1000")  # Hit, ~0.001 sec
print(f"Second call: {time.time() - start:.2f}s")
```

2. **Rate Limiter —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏:**
```python
# Fixed Window
def fixed_window_limit(user_id, max_requests=10, window=60):
    key = f"rate:{user_id}:{int(time.time() // window)}"
    current = r.incr(key)
    
    if current == 1:
        r.expire(key, window)
    
    return current <= max_requests

# Sliding Window Log
def sliding_window_limit(user_id, max_requests=10, window=60):
    key = f"rate:sliding:{user_id}"
    now = time.time()
    
    # –£–¥–∞–ª–∏ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
    r.zremrangebyscore(key, 0, now - window)
    
    # –ü—Ä–æ–≤–µ—Ä—å —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    current_count = r.zcard(key)
    
    if current_count < max_requests:
        # –î–æ–±–∞–≤—å –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
        r.zadd(key, {str(uuid.uuid4()): now})
        r.expire(key, window)
        return True
    
    return False

# Token Bucket (–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π)
def token_bucket(user_id, capacity=10, refill_rate=1):
    key = f"rate:bucket:{user_id}"
    last_key = f"rate:bucket:{user_id}:last"
    
    now = time.time()
    last_refill = float(r.get(last_key) or now)
    
    # –†–∞—Å—Å—á–∏—Ç–∞–π —Ç–µ–∫—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã
    current_tokens = int(r.get(key) or capacity)
    time_passed = now - last_refill
    tokens_to_add = int(time_passed * refill_rate)
    current_tokens = min(capacity, current_tokens + tokens_to_add)
    
    if current_tokens > 0:
        # –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–∫–µ–Ω
        r.set(key, current_tokens - 1)
        r.set(last_key, now)
        r.expire(key, 3600)
        r.expire(last_key, 3600)
        return True
    
    return False

# –¢–µ—Å—Ç
for i in range(15):
    allowed = fixed_window_limit("user:1", max_requests=10, window=60)
    print(f"Request {i+1}: {'ALLOWED' if allowed else 'BLOCKED'}")
    time.sleep(0.1)
```

3. **Distributed Lock –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π:**
```python
import uuid
import time

class RedisLock:
    def __init__(self, redis_client, lock_name, timeout=10):
        self.redis = redis_client
        self.lock_name = f"lock:{lock_name}"
        self.timeout = timeout
        self.identifier = None
    
    def __enter__(self):
        self.identifier = str(uuid.uuid4())
        
        # Retry logic
        for _ in range(10):
            acquired = self.redis.set(
                self.lock_name,
                self.identifier,
                nx=True,
                ex=self.timeout
            )
            if acquired:
                return self
            time.sleep(0.1)
        
        raise Exception(f"Could not acquire lock: {self.lock_name}")
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # Atomic release using Lua
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """
        self.redis.eval(lua_script, 1, self.lock_name, self.identifier)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
def update_counter():
    with RedisLock(r, "counter_lock"):
        # Critical section
        current = int(r.get("counter") or 0)
        time.sleep(0.1)  # –°–∏–º—É–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç—ã
        r.set("counter", current + 1)

# –¢–µ—Å—Ç —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é
import threading

r.set("counter", 0)

threads = []
for i in range(10):
    t = threading.Thread(target=update_counter)
    threads.append(t)
    t.start()

for t in threads:
    t.join()

print(f"Final counter: {r.get('counter')}")  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å 10
```

4. **Leaderboard —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏:**
```python
def add_player_score(player_id, score, game_mode="ranked"):
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π leaderboard
    r.zadd(f"leaderboard:{game_mode}", {player_id: score})
    
    # –ù–µ–¥–µ–ª—å–Ω—ã–π leaderboard
    week = int(time.time() // (7 * 86400))
    r.zadd(f"leaderboard:{game_mode}:week:{week}", {player_id: score})
    
    # –î–Ω–µ–≤–Ω–æ–π leaderboard
    day = int(time.time() // 86400)
    r.zadd(f"leaderboard:{game_mode}:day:{day}", {player_id: score})

def get_leaderboard(game_mode="ranked", period="all", count=10):
    if period == "week":
        week = int(time.time() // (7 * 86400))
        key = f"leaderboard:{game_mode}:week:{week}"
    elif period == "day":
        day = int(time.time() // 86400)
        key = f"leaderboard:{game_mode}:day:{day}"
    else:
        key = f"leaderboard:{game_mode}"
    
    return r.zrevrange(key, 0, count-1, withscores=True)

def get_player_stats(player_id, game_mode="ranked"):
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–∞–Ω–≥
    global_rank = r.zrevrank(f"leaderboard:{game_mode}", player_id)
    global_score = r.zscore(f"leaderboard:{game_mode}", player_id)
    
    # –ù–µ–¥–µ–ª—å–Ω—ã–π —Ä–∞–Ω–≥
    week = int(time.time() // (7 * 86400))
    week_rank = r.zrevrank(f"leaderboard:{game_mode}:week:{week}", player_id)
    
    return {
        "global_rank": global_rank + 1 if global_rank is not None else None,
        "global_score": global_score,
        "week_rank": week_rank + 1 if week_rank is not None else None
    }

# –¢–µ—Å—Ç
for i in range(100):
    add_player_score(f"player:{i}", 1000 + i * 10)

print("Top 10:")
for rank, (player, score) in enumerate(get_leaderboard(count=10), 1):
    print(f"{rank}. {player}: {score}")

print("\nPlayer stats:")
print(get_player_stats("player:50"))
```

5. **Pub/Sub –¥–ª—è real-time notifications:**
```python
import threading

# Publisher
def publish_notification(user_id, message):
    channel = f"notifications:{user_id}"
    r.publish(channel, json.dumps({
        "message": message,
        "timestamp": time.time()
    }))

# Subscriber
def subscribe_notifications(user_id):
    pubsub = r.pubsub()
    channel = f"notifications:{user_id}"
    pubsub.subscribe(channel)
    
    print(f"Subscribed to {channel}")
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            print(f"[{user_id}] Received: {data['message']}")

# –¢–µ—Å—Ç
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ –∑–∞–ø—É—Å—Ç–∏ subscriber
subscriber_thread = threading.Thread(
    target=subscribe_notifications,
    args=("user:1000",)
)
subscriber_thread.daemon = True
subscriber_thread.start()

time.sleep(1)  # –î–∞–π –ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è

# –û—Ç–ø—Ä–∞–≤—å notifications
for i in range(5):
    publish_notification("user:1000", f"Notification {i+1}")
    time.sleep(1)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Bloom Filter –¥–ª—è membership testing:**
```python
# –ò—Å–ø–æ–ª—å–∑—É–π RedisBloom –º–æ–¥—É–ª—å –∏–ª–∏ —Ä–µ–∞–ª–∏–∑—É–π —Å–≤–æ–π
# pip install redis-py-cluster

from redisbloom.client import Client as RedisBloom

rb = RedisBloom(host='localhost', port=6379)

# –°–æ–∑–¥–∞–π Bloom filter
rb.bfCreate("users:exists", 0.01, 1000000)

# –î–æ–±–∞–≤—å —ç–ª–µ–º–µ–Ω—Ç—ã
for i in range(10000):
    rb.bfAdd("users:exists", f"user:{i}")

# –ü—Ä–æ–≤–µ—Ä—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ, O(1))
print(rb.bfExists("users:exists", "user:5000"))  # True
print(rb.bfExists("users:exists", "user:99999"))  # False (–º–æ–∂–µ—Ç –±—ã—Ç—å false positive)
```

**2. RedisJSON –¥–ª—è complex —Å—Ç—Ä—É–∫—Ç—É—Ä:**
```python
# pip install rejson

from rejson import Client, Path

rj = Client(host='localhost', port=6379, decode_responses=True)

# –°–æ—Ö—Ä–∞–Ω–∏ JSON
user = {
    "name": "John Doe",
    "email": "john@example.com",
    "address": {
        "city": "New York",
        "country": "USA"
    },
    "tags": ["redis", "python", "devops"]
}

rj.jsonset("user:1000", Path.rootPath(), user)

# –ü–æ–ª—É—á–∏ –≤–µ—Å—å JSON
print(rj.jsonget("user:1000"))

# –ü–æ–ª—É—á–∏ —á–∞—Å—Ç—å
print(rj.jsonget("user:1000", Path('.address.city')))

# –û–±–Ω–æ–≤–∏ —á–∞—Å—Ç—å
rj.jsonset("user:1000", Path('.address.city'), "San Francisco")

# –î–æ–±–∞–≤—å –≤ –º–∞—Å—Å–∏–≤
rj.jsonarrappend("user:1000", Path('.tags'), "docker")
```

**3. RedisTimeSeries –¥–ª—è –º–µ—Ç—Ä–∏–∫:**
```python
# pip install redistimeseries

from redistimeseries.client import Client

rts = Client(host='localhost', port=6379)

# –°–æ–∑–¥–∞–π time series
rts.create("metrics:cpu", retention_msecs=86400000)  # 24 hours

# –î–æ–±–∞–≤—å –¥–∞–Ω–Ω—ã–µ
import time
for i in range(100):
    rts.add("metrics:cpu", int(time.time() * 1000), 50 + i % 50)
    time.sleep(0.1)

# –ó–∞–ø—Ä–æ—Å–∏ –¥–∞–Ω–Ω—ã–µ
data = rts.range("metrics:cpu", 0, int
````

–í–ê–ñ–ù–û: –ü—Ä–æ–¥–æ–ª–∂–∏ –≤—ã–¥–∞—á—É –≤ —Ä–µ–∂–∏–º–µ UPDATE

18 –¥–µ–∫.

–•–æ—Ä–æ—à–æ, –ø—Ä–æ–¥–æ–ª–∂—É –∏–º–µ–Ω–Ω–æ –≤ —Ä–µ–∂–∏–º–µ UPDATE. –í–æ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –º–æ–¥—É–ª—è 7:

python

```python
# –ó–∞–ø—Ä–æ—Å–∏ –¥–∞–Ω–Ω—ã–µ
data = rts.range("metrics:cpu", 0, int(time.time() * 1000))
print(f"Data points: {len(data)}")

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
avg = rts.range("metrics:cpu", 0, int(time.time() * 1000), 
                aggregation_type='avg', bucket_size_msec=10000)
print(f"Average CPU: {avg}")
```
## –ú–æ–¥—É–ª—å 8: Monitoring –∏ Troubleshooting (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

bash

```bash
# Memory
used_memory              # –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –ø–∞–º—è—Ç—å
used_memory_rss          # RSS –ø–∞–º—è—Ç—å (resident set size)
used_memory_peak         # –ü–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
mem_fragmentation_ratio  # –§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å < 1.5)
maxmemory               # –õ–∏–º–∏—Ç –ø–∞–º—è—Ç–∏
evicted_keys            # –í—ã—Ç–µ—Å–Ω–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏

# Performance
instantaneous_ops_per_sec    # –û–ø–µ—Ä–∞—Ü–∏–π –≤ —Å–µ–∫—É–Ω–¥—É
total_commands_processed     # –í—Å–µ–≥–æ –∫–æ–º–∞–Ω–¥
keyspace_hits / keyspace_misses  # Hit rate –∫–µ—à–∞
expired_keys                 # –ò—Å—Ç–µ–∫—à–∏–µ –∫–ª—é—á–∏

# Clients
connected_clients        # –ü–æ–¥–∫–ª—é—á–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
blocked_clients         # –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
rejected_connections    # –û—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

# Persistence
rdb_last_save_time      # –ü–æ—Å–ª–µ–¥–Ω–∏–π RDB save
aof_last_rewrite_time   # –ü–æ—Å–ª–µ–¥–Ω–∏–π AOF rewrite
rdb_changes_since_last_save  # –ò–∑–º–µ–Ω–µ–Ω–∏–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ save

# Replication
connected_slaves        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫
master_link_status      # –°—Ç–∞—Ç—É—Å —Å–≤—è–∑–∏ —Å master (up/down)
master_last_io_seconds_ago  # –ü–æ—Å–ª–µ–¥–Ω–µ–µ IO —Å master
```

**INFO –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
INFO                    # –í—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
INFO server            # –í–µ—Ä—Å–∏—è, uptime, OS
INFO clients           # –ö–ª–∏–µ–Ω—Ç—ã
INFO memory            # –ü–∞–º—è—Ç—å
INFO persistence       # RDB/AOF
INFO stats             # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
INFO replication       # –†–µ–ø–ª–∏–∫–∞—Ü–∏—è
INFO cpu               # CPU usage
INFO commandstats      # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º
INFO keyspace          # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª—é—á–∞—Ö
INFO cluster           # Cluster info
```

**Prometheus exporter:**

bash

```bash
# Redis exporter –¥–ª—è Prometheus
docker run -d --name redis-exporter \
  -p 9121:9121 \
  oliver006/redis_exporter:latest \
  --redis.addr=redis://redis:6379 \
  --redis.password=yourpass

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ http://localhost:9121/metrics
```

**–¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:**

**1. High Memory Usage:**

bash

```bash
# –ü—Ä–æ–≤–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –ø–∞–º—è—Ç—å
INFO memory

# –ù–∞–π–¥–∏ –±–æ–ª—å—à–∏–µ –∫–ª—é—á–∏
redis-cli --bigkeys

# –ò–ª–∏ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ
redis-cli --memkeys

# Analyze –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á
MEMORY USAGE keyname SAMPLES 5

# –†–µ—à–µ–Ω–∏—è:
# - –£–≤–µ–ª–∏—á—å maxmemory
# - –ù–∞—Å—Ç—Ä–æ–π eviction policy
# - –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
# - –î–æ–±–∞–≤—å TTL –Ω–∞ –∫–ª—é—á–∏
```

**2. High Latency:**

bash

```bash
# –í–∫–ª—é—á–∏ latency monitoring
CONFIG SET latency-monitor-threshold 100

# –ü—Ä–æ–≤–µ—Ä—å latency —Å–æ–±—ã—Ç–∏—è
LATENCY LATEST
LATENCY DOCTOR
LATENCY GRAPH event-name

# –ü—Ä–æ–≤–µ—Ä—å slow log
SLOWLOG GET 10

# –ü—Ä–∏—á–∏–Ω—ã:
# - –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã (KEYS, SMEMBERS –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö)
# - Swapping (–ø—Ä–æ–≤–µ—Ä—å used_memory_rss vs used_memory)
# - Persistence (BGSAVE/AOF rewrite)
# - –°–µ—Ç–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
```

**3. Connection Issues:**

bash

```bash
# –ü—Ä–æ–≤–µ—Ä—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
INFO clients

# –¢–µ–∫—É—â–∏–µ –∫–ª–∏–µ–Ω—Ç—ã
CLIENT LIST

# –£–±–µ–π –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
CLIENT KILL ip:port

# –ü—Ä–∏—á–∏–Ω—ã:
# - –î–æ—Å—Ç–∏–≥–Ω—É—Ç maxclients
# - –°–µ—Ç–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
# - timeout —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
# - –ö–ª–∏–µ–Ω—Ç –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
```

**4. Replication Lag:**

bash

```bash
# –ù–∞ replica –ø—Ä–æ–≤–µ—Ä—å
INFO replication

# master_last_io_seconds_ago - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å < 1
# master_link_status - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å "up"

# –ü—Ä–∏—á–∏–Ω—ã:
# - –°–µ—Ç–µ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É master –∏ replica
# - Replica –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–∞
# - –ú–µ–¥–ª–µ–Ω–Ω—ã–π –¥–∏—Å–∫ –Ω–∞ replica
# - repl_backlog_size —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π
```

**Debugging –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
# Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–º–∞–Ω–¥
MONITOR

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º
INFO commandstats

# –ü–∞–º—è—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª—é—á–∞
MEMORY USAGE key

# Debug –∫–æ–º–∞–Ω–¥—ã (—Ç–æ–ª—å–∫–æ –¥–ª—è dev!)
DEBUG OBJECT key
DEBUG SEGFAULT  # –ö—Ä–µ—à —Å–µ—Ä–≤–µ—Ä–∞!
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ troubleshooting:

1. **–°–æ–∑–¥–∞–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∫—Ä–∏–ø—Ç:**

bash

```bash
cat > redis-monitor.sh <<'EOF'
#!/bin/bash

REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_PASS=""

# –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
RED='\033[0;31m'
YELLOW='\033[1;33m'
GREEN='\033[0;32m'
NC='\033[0m'

while true; do
  clear
  echo "=== Redis Monitoring Dashboard ==="
  echo "Time: $(date)"
  echo ""
  
  # Memory
  echo "=== MEMORY ==="
  USED_MEM=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO memory | grep "used_memory_human:" | cut -d: -f2)
  MAX_MEM=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT CONFIG GET maxmemory | tail -1)
  FRAG=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO memory | grep "mem_fragmentation_ratio:" | cut -d: -f2)
  
  echo "Used Memory: $USED_MEM"
  echo "Max Memory: $MAX_MEM"
  echo "Fragmentation Ratio: $FRAG"
  
  # Performance
  echo ""
  echo "=== PERFORMANCE ==="
  OPS=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO stats | grep "instantaneous_ops_per_sec:" | cut -d: -f2)
  HITS=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO stats | grep "keyspace_hits:" | cut -d: -f2)
  MISSES=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO stats | grep "keyspace_misses:" | cut -d: -f2)
  
  if [ "$MISSES" != "0" ]; then
    HIT_RATE=$(echo "scale=2; $HITS * 100 / ($HITS + $MISSES)" | bc)
  else
    HIT_RATE="100.00"
  fi
  
  echo "Ops/sec: $OPS"
  echo "Hit Rate: ${HIT_RATE}%"
  
  # Clients
  echo ""
  echo "=== CLIENTS ==="
  CLIENTS=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO clients | grep "connected_clients:" | cut -d: -f2)
  BLOCKED=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO clients | grep "blocked_clients:" | cut -d: -f2)
  
  echo "Connected: $CLIENTS"
  echo "Blocked: $BLOCKED"
  
  # Replication (if replica)
  ROLE=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO replication | grep "role:" | cut -d: -f2 | tr -d '\r')
  if [ "$ROLE" = "slave" ]; then
    echo ""
    echo "=== REPLICATION ==="
    MASTER_STATUS=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO replication | grep "master_link_status:" | cut -d: -f2)
    LAG=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT INFO replication | grep "master_last_io_seconds_ago:" | cut -d: -f2)
    echo "Master Link: $MASTER_STATUS"
    echo "Lag: ${LAG}s"
  fi
  
  # Alerts
  echo ""
  echo "=== ALERTS ==="
  
  # Check fragmentation
  FRAG_NUM=$(echo $FRAG | tr -d '\r')
  if (( $(echo "$FRAG_NUM > 1.5" | bc -l) )); then
    echo -e "${RED}HIGH FRAGMENTATION: $FRAG_NUM${NC}"
  fi
  
  # Check hit rate
  if (( $(echo "$HIT_RATE < 80" | bc -l) )); then
    echo -e "${YELLOW}LOW HIT RATE: ${HIT_RATE}%${NC}"
  fi
  
  sleep 5
done
EOF

chmod +x redis-monitor.sh
./redis-monitor.sh
```

2. **–ù–∞—Å—Ç—Ä–æ–π Prometheus exporter:**

bash

```bash
# –ó–∞–ø—É—Å—Ç–∏ Redis (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω)
docker run -d --name redis-mon \
  --network redis-net \
  -p 6379:6379 \
  redis:7-alpine

# –ó–∞–ø—É—Å—Ç–∏ Redis Exporter
docker run -d --name redis-exporter \
  --network redis-net \
  -p 9121:9121 \
  oliver006/redis_exporter:latest \
  --redis.addr=redis://redis-mon:6379

# –ü—Ä–æ–≤–µ—Ä—å –º–µ—Ç—Ä–∏–∫–∏
curl http://localhost:9121/metrics | grep redis_

# –ó–∞–ø—É—Å—Ç–∏ Prometheus (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
cat > prometheus.yml <<EOF
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'redis'
    static_configs:
      - targets: ['localhost:9121']
EOF

docker run -d --name prometheus \
  -p 9090:9090 \
  -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus

# –û—Ç–∫—Ä–æ–π http://localhost:9090
# Query –ø—Ä–∏–º–µ—Ä—ã:
# redis_memory_used_bytes
# rate(redis_commands_processed_total[1m])
# redis_connected_clients
```

3. **Troubleshooting —Å—Ü–µ–Ω–∞—Ä–∏–∏:**

bash

```bash
# –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ù–∞–π–¥–∏ –±–æ–ª—å—à–∏–µ –∫–ª—é—á–∏
redis-cli --bigkeys

# –°—Ü–µ–Ω–∞—Ä–∏–π 2: Analyze memory –ø–æ pattern
redis-cli --memkeys --memkeys-samples 10000

# –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ù–∞–π–¥–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
redis-cli SLOWLOG GET 20

# –°—Ü–µ–Ω–∞—Ä–∏–π 4: Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
redis-cli MONITOR | head -100

# –°—Ü–µ–Ω–∞—Ä–∏–π 5: –ü—Ä–æ–≤–µ—Ä—å latency
redis-cli --latency
redis-cli --latency-history
redis-cli --latency-dist

# –°—Ü–µ–Ω–∞—Ä–∏–π 6: Intrinsic latency (–±–∞–∑–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞)
redis-cli --intrinsic-latency 30
```

4. **–°–æ–∑–¥–∞–π alerting —Å–∫—Ä–∏–ø—Ç:**

bash

```bash
cat > redis-alerts.sh <<'EOF'
#!/bin/bash

REDIS_HOST="localhost"
REDIS_PORT="6379"
WEBHOOK_URL="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

check_memory() {
  USED=$(redis-cli -h $REDIS_HOST INFO memory | grep "used_memory:" | cut -d: -f2 | tr -d '\r')
  MAX=$(redis-cli -h $REDIS_HOST CONFIG GET maxmemory | tail -1)
  
  if [ "$MAX" != "0" ]; then
    PCT=$((USED * 100 / MAX))
    if [ $PCT -gt 90 ]; then
      send_alert "CRITICAL" "Memory usage at ${PCT}%"
    elif [ $PCT -gt 80 ]; then
      send_alert "WARNING" "Memory usage at ${PCT}%"
    fi
  fi
}

check_replication() {
  ROLE=$(redis-cli -h $REDIS_HOST INFO replication | grep "role:" | cut -d: -f2 | tr -d '\r')
  
  if [ "$ROLE" = "slave" ]; then
    STATUS=$(redis-cli -h $REDIS_HOST INFO replication | grep "master_link_status:" | cut -d: -f2 | tr -d '\r')
    if [ "$STATUS" != "up" ]; then
      send_alert "CRITICAL" "Master link is DOWN"
    fi
    
    LAG=$(redis-cli -h $REDIS_HOST INFO replication | grep "master_last_io_seconds_ago:" | cut -d: -f2 | tr -d '\r')
    if [ "$LAG" -gt 10 ]; then
      send_alert "WARNING" "Replication lag: ${LAG}s"
    fi
  fi
}

check_clients() {
  CLIENTS=$(redis-cli -h $REDIS_HOST INFO clients | grep "connected_clients:" | cut -d: -f2 | tr -d '\r')
  MAX=$(redis-cli -h $REDIS_HOST CONFIG GET maxclients | tail -1)
  
  PCT=$((CLIENTS * 100 / MAX))
  if [ $PCT -gt 90 ]; then
    send_alert "WARNING" "Client connections at ${PCT}% (${CLIENTS}/${MAX})"
  fi
}

send_alert() {
  LEVEL=$1
  MESSAGE=$2
  TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')
  
  echo "[$TIMESTAMP] $LEVEL: $MESSAGE"
  
  # –û—Ç–ø—Ä–∞–≤—å –≤ Slack (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
  # curl -X POST -H 'Content-type: application/json' \
  #   --data "{\"text\":\"Redis Alert [$LEVEL]: $MESSAGE\"}" \
  #   $WEBHOOK_URL
}

# Main loop
while true; do
  check_memory
  check_replication
  check_clients
  sleep 60
done
EOF

chmod +x redis-alerts.sh
./redis-alerts.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Grafana Dashboard –¥–ª—è Redis:**

bash

```bash
# –ó–∞–ø—É—Å—Ç–∏ Grafana
docker run -d --name grafana \
  -p 3000:3000 \
  grafana/grafana:latest

# –û—Ç–∫—Ä–æ–π http://localhost:3000 (admin/admin)
# –î–æ–±–∞–≤—å Prometheus data source
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–π dashboard ID: 11835 (Redis Dashboard)

# –ò–ª–∏ —Å–æ–∑–¥–∞–π —Å–≤–æ–π dashboard —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏:
# - redis_memory_used_bytes
# - rate(redis_commands_total[1m])
# - redis_connected_clients
# - redis_keyspace_hits_total / redis_keyspace_misses_total
```

**2. RedisInsight –¥–ª—è GUI –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

bash

```bash
docker run -d --name redisinsight \
  -p 8001:8001 \
  redis/redisinsight:latest

# –û—Ç–∫—Ä–æ–π http://localhost:8001
# –î–æ–±–∞–≤—å —Å–≤–æ–π Redis
# Features:
# - Browser (–ø—Ä–æ—Å–º–æ—Ç—Ä –∫–ª—é—á–µ–π)
# - Workbench (CLI + command helper)
# - Analysis (memory analysis)
# - Profiler (real-time commands)
# - Slowlog viewer
```

**3. Custom healthcheck endpoint:**

python

````python
from flask import Flask, jsonify
import redis

app = Flask(__name__)
r = redis.Redis(host='localhost', port=6379)

@app.route('/health')
def health():
    try:
        # Ping test
        r.ping()
        
        # Memory check
        info = r.info('memory')
        memory_used = info['used_memory']
        memory_max = info.get('maxmemory', 0)
        
        # Replication check
        repl_info = r.info('replication')
        role = repl_info['role']
        
        status = {
            "status": "healthy",
            "memory_used_mb": memory_used / 1024 / 1024,
            "role": role
        }
        
        if role == 'slave':
            status['master_link'] = repl_info['master_link_status']
        
        return jsonify(status), 200
        
    except Exception as e:
        return jsonify({"status": "unhealthy", "error": str(e)}), 503

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç (60 –º–∏–Ω—É—Ç)

### –ó–∞–¥–∞—á–∞: –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å Production-Ready Redis Infrastructure

–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é Redis –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –≤—ã—Å–æ–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å—é, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
- 1 Master Redis
- 2 Replica Redis
- 3 Redis Sentinel (–¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ failover)
- Redis Exporter (Prometheus metrics)
- Grafana (visualization)
- Backup —Å–∏—Å—Ç–µ–º–∞
- Security (–ø–∞—Ä–æ–ª–∏, ACL, TLS –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:**
```
redis-infrastructure/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ redis/
‚îÇ   ‚îú‚îÄ‚îÄ master.conf
‚îÇ   ‚îú‚îÄ‚îÄ replica.conf
‚îÇ   ‚îî‚îÄ‚îÄ sentinel.conf
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ backup.sh
‚îÇ   ‚îú‚îÄ‚îÄ monitor.sh
‚îÇ   ‚îî‚îÄ‚îÄ healthcheck.sh
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îî‚îÄ‚îÄ grafana-dashboard.json
‚îî‚îÄ‚îÄ README.md
````

**1. docker-compose.yml:**

yaml

```yaml
version: '3.8'

services:
  redis-master:
    image: redis:7-alpine
    container_name: redis-master
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis/master.conf:/usr/local/etc/redis/redis.conf
      - redis-master-data:/data
    ports:
      - "6379:6379"
    networks:
      - redis-net

  redis-replica1:
    image: redis:7-alpine
    container_name: redis-replica1
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis/replica.conf:/usr/local/etc/redis/redis.conf
      - redis-replica1-data:/data
    ports:
      - "6380:6379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  redis-replica2:
    image: redis:7-alpine
    container_name: redis-replica2
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis/replica.conf:/usr/local/etc/redis/redis.conf
      - redis-replica2-data:/data
    ports:
      - "6381:6379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  sentinel1:
    image: redis:7-alpine
    container_name: sentinel1
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    volumes:
      - ./redis/sentinel.conf:/usr/local/etc/redis/sentinel.conf
    ports:
      - "26379:26379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  sentinel2:
    image: redis:7-alpine
    container_name: sentinel2
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    volumes:
      - ./redis/sentinel.conf:/usr/local/etc/redis/sentinel.conf
    ports:
      - "26380:26379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  sentinel3:
    image: redis:7-alpine
    container_name: sentinel3
    command: redis-sentinel /usr/local/etc/redis/sentinel.conf
    volumes:
      - ./redis/sentinel.conf:/usr/local/etc/redis/sentinel.conf
    ports:
      - "26381:26379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    environment:
      - REDIS_ADDR=redis://redis-master:6379
      - REDIS_PASSWORD=YourStrongPassword123
    ports:
      - "9121:9121"
    depends_on:
      - redis-master
    networks:
      - redis-net

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - redis-net

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - redis-net

volumes:
  redis-master-data:
  redis-replica1-data:
  redis-replica2-data:
  prometheus-data:
  grafana-data:

networks:
  redis-net:
    driver: bridge
```

**2. redis/master.conf:**

conf

```conf
bind 0.0.0.0
port 6379
protected-mode yes

# Security
requirepass YourStrongPassword123
masterauth YourStrongPassword123

# Memory
maxmemory 512mb
maxmemory-policy allkeys-lru

# Persistence
appendonly yes
appendfsync everysec
save 900 1
save 300 10
save 60 10000

# Replication
min-replicas-to-write 1
min-replicas-max-lag 10

# Logging
loglevel notice
slowlog-log-slower-than 10000
slowlog-max-len 128

# Performance
latency-monitor-threshold 100
```

**3. redis/replica.conf:**

conf

```conf
bind 0.0.0.0
port 6379
protected-mode yes

# Security
requirepass YourStrongPassword123
masterauth YourStrongPassword123

# Replication
replicaof redis-master 6379
replica-read-only yes

# Persistence
appendonly yes
appendfsync everysec

# Logging
loglevel notice
```

**4. redis/sentinel.conf:**

conf

```conf
port 26379
sentinel monitor mymaster redis-master 6379 2
sentinel auth-pass mymaster YourStrongPassword123
sentinel down-after-milliseconds mymaster 5000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 10000
```

**5. monitoring/prometheus.yml:**

yaml

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
```

**6. scripts/backup.sh:**

bash

```bash
#!/bin/bash

BACKUP_DIR="/backup/redis"
DATE=$(date +%Y%m%d-%H%M%S)
REDIS_PASS="YourStrongPassword123"

mkdir -p $BACKUP_DIR

# Backup from replica (–Ω–µ –Ω–∞–≥—Ä—É–∂–∞–µ–º master)
docker exec redis-replica1 redis-cli -a $REDIS_PASS BGSAVE
sleep 5

docker cp redis-replica1:/data/dump.rdb $BACKUP_DIR/dump-$DATE.rdb
docker cp redis-replica1:/data/appendonly.aof $BACKUP_DIR/appendonly-$DATE.aof

# Compress
gzip $BACKUP_DIR/dump-$DATE.rdb
gzip $BACKUP_DIR/appendonly-$DATE.aof

# –£–¥–∞–ª–∏ —Å—Ç–∞—Ä—ã–µ (> 7 –¥–Ω–µ–π)
find $BACKUP_DIR -name "*.gz" -mtime +7 -delete

echo "Backup completed: $DATE"
```

**7. scripts/healthcheck.sh:**

bash

```bash
#!/bin/bash

REDIS_PASS="YourStrongPassword123"

# Check master
if ! docker exec redis-master redis-cli -a $REDIS_PASS PING > /dev/null 2>&1; then
  echo "CRITICAL: Master is DOWN"
  exit 2
fi

# Check replicas
for replica in redis-replica1 redis-replica2; do
  if ! docker exec $replica redis-cli -a $REDIS_PASS PING > /dev/null 2>&1; then
    echo "WARNING: $replica is DOWN"
  fi
done

# Check sentinel
SENTINELS=$(docker exec sentinel1 redis-cli -p 26379 SENTINEL sentinels mymaster | grep -c "name")
if [ "$SENTINELS" -lt 2 ]; then
  echo "WARNING: Less than 3 sentinels running"
fi

echo "OK: All services healthy"
exit 0
```

**Deployment –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**

bash

```bash
# 1. –°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É
mkdir -p redis-infrastructure/{redis,scripts,monitoring}
cd redis-infrastructure

# 2. –°–æ–∑–¥–∞–π –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª—ã (—Å–º. –≤—ã—à–µ)

# 3. –ó–∞–ø—É—Å—Ç–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
docker-compose up -d

# 4. –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps

# 5. –ü—Ä–æ–≤–µ—Ä—å —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é
docker exec redis-master redis-cli -a YourStrongPassword123 INFO replication

# 6. –ü—Ä–æ–≤–µ—Ä—å Sentinel
docker exec sentinel1 redis-cli -p 26379 SENTINEL masters

# 7. –¢–µ—Å—Ç–∏—Ä—É–π failover
docker stop redis-master
# Sentinel –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–≤—ã—Å–∏—Ç —Ä–µ–ø–ª–∏–∫—É
docker exec sentinel1 redis-cli -p 26379 SENTINEL get-master-addr-by-name mymaster

# 8. –û—Ç–∫—Ä–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000 (admin/admin)
#   - –î–æ–±–∞–≤—å Prometheus data source (http://prometheus:9090)
#   - –ò–º–ø–æ—Ä—Ç–∏—Ä—É–π dashboard ID: 11835

# 9. –ó–∞–ø—É—Å—Ç–∏ backup
chmod +x scripts/backup.sh
./scripts/backup.sh

# 10. Healthcheck
chmod +x scripts/healthcheck.sh
./scripts/healthcheck.sh
```

**–†–∞—Å—à–∏—Ä–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):**

1. **–î–æ–±–∞–≤—å HAProxy –¥–ª—è load balancing:**

yaml

```yaml
haproxy:
  image: haproxy:latest
  volumes:
    - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
  ports:
    - "6380:6379"  # Read –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ø–ª–∏–∫–∞–º
```

2. **–î–æ–±–∞–≤—å TLS encryption**
3. **–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π —Å Vault –¥–ª—è secrets**
4. **–ù–∞—Å—Ç—Ä–æ–π alerting –≤ Grafana**
5. **–î–æ–±–∞–≤—å CI/CD –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ deployment**

---

## –ß–µ–∫-–ª–∏—Å—Ç –Ω–∞–≤—ã–∫–æ–≤

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∫—É—Ä—Å–∞ —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

### –ë–∞–∑–æ–≤—ã–µ –Ω–∞–≤—ã–∫–∏:

- ‚úÖ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Redis
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å redis-cli —É–≤–µ—Ä–µ–Ω–Ω–æ
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å persistence (RDB/AOF)
- ‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å backup –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å TTL –∏ expiration

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞–≤—ã–∫–∏:

- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Master-Replica —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis Sentinel –¥–ª—è HA
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Redis Cluster
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- ‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å security best practices
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å ACL (Redis 6.0+)

### Expert –Ω–∞–≤—ã–∫–∏:

- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (Prometheus/Grafana)
- ‚úÖ Troubleshooting —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
- ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ patterns (cache, rate limiting, locks)
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis modules (JSON, TimeSeries, Bloom)
- ‚úÖ –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å HA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å backup –∏ recovery

---

## –°–ø—Ä–∞–≤–æ—á–Ω–∞—è —Å–µ–∫—Ü–∏—è: –ë—ã—Å—Ç—Ä—ã–µ —à–ø–∞—Ä–≥–∞–ª–∫–∏

### –¢–æ–ø-20 –∫–æ–º–∞–Ω–¥ Redis

bash

```bash
# Strings
SET key value
GET key
DEL key
EXISTS key
EXPIRE key seconds

# Lists
LPUSH key value
RPUSH key value
LPOP key
RPOP key
LRANGE key 0 -1

# Sets
SADD key member
SMEMBERS key
SISMEMBER key member

# Hashes
HSET key field value
HGET key field
HGETALL key

# Sorted Sets
ZADD key score member
ZRANGE key 0 -1 WITHSCORES
ZRANK key member

# Generic
KEYS pattern  # –û–ø–∞—Å–Ω–æ –≤ prod!
SCAN cursor MATCH pattern COUNT count  # –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ
TTL key
INFO
```

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö use cases

**Pure Cache:**

conf

```conf
maxmemory 2gb
maxmemory-policy allkeys-lru
save ""
appendonly no
```

**Persistent Database:**

conf

```conf
maxmemory 0
save 900 1
save 300 10
appendonly yes
appendfsync everysec
```

**Session Store:**

conf

```conf
maxmemory 1gb
maxmemory-policy volatile-lru
appendonly yes
```

**Queue:**

conf

```conf
maxmemory 512mb
maxmemory-policy noeviction
appendonly yes
```

### Performance Checklist

**–ü–µ—Ä–µ–¥ production:**

- ‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω maxmemory
- ‚úÖ –í—ã–±—Ä–∞–Ω–∞ eviction policy
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω persistence (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
- ‚úÖ –í–∫–ª—é—á–µ–Ω requirepass
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω bind address
- ‚úÖ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω—ã –æ–ø–∞—Å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω slowlog
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω latency monitoring
- ‚úÖ Replica –¥–ª—è HA (–µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ)
- ‚úÖ Sentinel –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ failover
- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ alerting
- ‚úÖ Backup —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
- ‚úÖ Documented recovery procedure

### Troubleshooting Quick Guide

**High Memory:**

bash

```bash
INFO memory
MEMORY STATS
redis-cli --bigkeys
# –†–µ—à–µ–Ω–∏–µ: eviction policy, TTL, –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏
```

**High Latency:**

bash

```bash
LATENCY DOCTOR
SLOWLOG GET 10
redis-cli --latency
```
#### –†–µ—à–µ–Ω–∏–µ: –∏–∑–±–µ–≥–∞–π KEYS, –∏—Å–ø–æ–ª—å–∑—É–π SCAN, –ø—Ä–æ–≤–µ—Ä—å swapping

**Connection Issues:**
```bash
INFO clients
CLIENT LIST
CONFIG GET maxclients
# –†–µ—à–µ–Ω–∏–µ: —É–≤–µ–ª–∏—á—å maxclients, –ø—Ä–æ–≤–µ—Ä—å firewall
```

**Replication Lag:**
```bash
INFO replication
# –°–º–æ—Ç—Ä–∏: master_last_io_seconds_ago
# –†–µ—à–µ–Ω–∏–µ: —Å–µ—Ç—å, replica resources, repl_backlog_size
```
## –ú–æ–¥—É–ª—å 9: Redis –≤ –æ–±–ª–∞–∫–∞—Ö –∏ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Å–µ—Ä–≤–∏—Å—ã (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**AWS ElastiCache for Redis:**

bash

```bash
# –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- –ü–æ–ª–Ω–æ—Å—Ç—å—é —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π —Å–µ—Ä–≤–∏—Å
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ backups
- Multi-AZ –¥–ª—è HA
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π failover
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ cluster mode
- –í–µ—Ä—Å–∏–∏: Redis 6.x, 7.x
- –®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ at-rest –∏ in-transit

# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
1. Single Node (dev/test)
2. Primary + Replica (HA)
3. Cluster Mode (—à–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–∏–µ + HA)

# Endpoint —Ç–∏–ø—ã
- Primary Endpoint: –¥–ª—è –∑–∞–ø–∏—Å–∏
- Reader Endpoint: –¥–ª—è —á—Ç–µ–Ω–∏—è (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ø–ª–∏–∫–∞–º)
- Configuration Endpoint: –¥–ª—è cluster mode

# Backup
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ snapshots (RDB)
- Manual snapshots
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –Ω–æ–≤—ã–π cluster
```

**Azure Cache for Redis:**

bash

```bash
# –¢–∏—Ä—ã
- Basic: Single node, –±–µ–∑ SLA
- Standard: Primary + Replica, 99.9% SLA
- Premium: Cluster, persistence, VNet
- Enterprise: Redis Enterprise, 99.999% SLA

# –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- Geo-replication (Premium+)
- Data persistence: RDB –∏ AOF
- VNet integration
- Private Link support
- Zone redundancy
- Active geo-replication (Enterprise)

# Scaling
- Vertical: –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–∏—Ä–∞
- Horizontal: cluster sharding (Premium+)
```

**GCP Memorystore for Redis:**

bash

````bash
# –¢–∏—Ä—ã
- Basic: Single node, 99.9% SLA
- Standard: HA —Å —Ä–µ–ø–ª–∏–∫–æ–π, 99.9% SLA

# –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ failover
- Automated backups
- Point-in-time recovery
- VPC peering
- Private IP
- Maintenance windows
- Import/Export

# –í–µ—Ä—Å–∏–∏
- Redis 5.0, 6.x, 7.0
- Managed updates
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–ª–∞—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤:**
```
Feature              | AWS ElastiCache | Azure Cache | GCP Memorystore
---------------------|----------------|-------------|----------------
Max Memory           | 6.38 TB        | 1.2 TB      | 300 GB
Cluster Mode         | ‚úÖ              | ‚úÖ Premium   | ‚ùå
Geo-Replication      | ‚ùå              | ‚úÖ Premium   | ‚ùå
Multi-AZ             | ‚úÖ              | ‚úÖ           | ‚úÖ
Encryption at-rest   | ‚úÖ              | ‚úÖ Premium   | ‚úÖ
Encryption in-transit| ‚úÖ              | ‚úÖ           | ‚úÖ
VPC/VNet             | ‚úÖ              | ‚úÖ Premium   | ‚úÖ
Backup/Restore       | ‚úÖ              | ‚úÖ           | ‚úÖ
Monitoring           | CloudWatch     | Azure Mon.  | Cloud Monitoring
Cost                 | $$$            | $$          | $$
````

**Terraform –¥–ª—è AWS ElastiCache:**

hcl

```hcl
resource "aws_elasticache_replication_group" "redis" {
  replication_group_id       = "my-redis-cluster"
  replication_group_description = "Redis cluster for production"
  engine                     = "redis"
  engine_version            = "7.0"
  node_type                 = "cache.r6g.large"
  num_cache_clusters        = 3
  parameter_group_name      = "default.redis7"
  port                      = 6379
  
  # HA
  automatic_failover_enabled = true
  multi_az_enabled          = true
  
  # Security
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  auth_token                = var.redis_auth_token
  
  # Backup
  snapshot_retention_limit  = 7
  snapshot_window          = "03:00-05:00"
  
  # Maintenance
  maintenance_window       = "sun:05:00-sun:07:00"
  
  # Network
  subnet_group_name        = aws_elasticache_subnet_group.redis.name
  security_group_ids       = [aws_security_group.redis.id]
  
  tags = {
    Environment = "production"
    Project     = "myapp"
  }
}

resource "aws_elasticache_subnet_group" "redis" {
  name       = "redis-subnet-group"
  subnet_ids = var.private_subnet_ids
}

resource "aws_security_group" "redis" {
  name        = "redis-sg"
  description = "Security group for Redis"
  vpc_id      = var.vpc_id
  
  ingress {
    from_port   = 6379
    to_port     = 6379
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

**CloudFormation –¥–ª—è AWS:**

yaml

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Resources:
  RedisReplicationGroup:
    Type: AWS::ElastiCache::ReplicationGroup
    Properties:
      ReplicationGroupId: my-redis
      ReplicationGroupDescription: Production Redis
      Engine: redis
      EngineVersion: '7.0'
      CacheNodeType: cache.r6g.large
      NumCacheClusters: 3
      AutomaticFailoverEnabled: true
      MultiAZEnabled: true
      AtRestEncryptionEnabled: true
      TransitEncryptionEnabled: true
      AuthToken: !Ref RedisAuthToken
      SnapshotRetentionLimit: 7
      SnapshotWindow: '03:00-05:00'
      PreferredMaintenanceWindow: 'sun:05:00-sun:07:00'
      CacheSubnetGroupName: !Ref RedisSubnetGroup
      SecurityGroupIds:
        - !Ref RedisSecurityGroup
```

**–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –æ–±–ª–∞—á–Ω–æ–º—É Redis:**

python

````python
import redis
import ssl

# AWS ElastiCache (with TLS)
r = redis.Redis(
    host='my-redis.abc123.ng.0001.use1.cache.amazonaws.com',
    port=6379,
    password='your-auth-token',
    ssl=True,
    ssl_cert_reqs=ssl.CERT_REQUIRED,
    decode_responses=True
)

# Azure Cache (with TLS)
r = redis.Redis(
    host='myredis.redis.cache.windows.net',
    port=6380,
    password='your-access-key',
    ssl=True,
    decode_responses=True
)

# GCP Memorystore (no TLS by default)
r = redis.Redis(
    host='10.0.0.3',  # Private IP
    port=6379,
    decode_responses=True
)
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–ø—Ä–æ–µ–∫—Ç–∏—Ä—É–π –æ–±–ª–∞—á–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (–Ω–∞ –±—É–º–∞–≥–µ –∏–ª–∏ –≤ draw.io):

1. **AWS Multi-AZ setup:**
```
Region: us-east-1
‚îú‚îÄ‚îÄ VPC: 10.0.0.0/16
‚îÇ   ‚îú‚îÄ‚îÄ Private Subnet AZ-A: 10.0.1.0/24
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ElastiCache Node 1 (Primary)
‚îÇ   ‚îú‚îÄ‚îÄ Private Subnet AZ-B: 10.0.2.0/24
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ElastiCache Node 2 (Replica)
‚îÇ   ‚îî‚îÄ‚îÄ Private Subnet AZ-C: 10.0.3.0/24
‚îÇ       ‚îî‚îÄ‚îÄ ElastiCache Node 3 (Replica)
‚îú‚îÄ‚îÄ Application Servers (EC2/ECS)
‚îÇ   ‚îî‚îÄ‚îÄ Connect to Primary Endpoint
‚îî‚îÄ‚îÄ CloudWatch Alarms
    ‚îú‚îÄ‚îÄ CPUUtilization > 75%
    ‚îú‚îÄ‚îÄ DatabaseMemoryUsagePercentage > 80%
    ‚îî‚îÄ‚îÄ ReplicationLag > 30s
````

2. **–°–æ–∑–¥–∞–π Cost Estimation:**

bash

```bash
# AWS Pricing Calculator
# Example: cache.r6g.large (3 nodes)
# - Instance: $0.226/hour * 3 = $0.678/hour
# - Data transfer: ~$0.09/GB
# - Backups: $0.085/GB-month
# Monthly: ~$500 + data transfer + backups

# Azure Cache Premium P1 (6GB)
# - Instance: ~$0.356/hour = ~$260/month
# - Geo-replication: additional cost

# GCP Memorystore Standard M1 (5GB)
# - Instance: ~$0.054/hour = ~$40/month
```

3. **–ù–∞–ø–∏—à–∏ –º–∏–≥—Ä–∞—Ü–∏—é —Å self-hosted –Ω–∞ AWS:**

bash

```bash
#!/bin/bash
# migrate-to-aws.sh

SOURCE_HOST="self-hosted-redis.example.com"
SOURCE_PORT="6379"
SOURCE_PASS="source-password"

TARGET_HOST="my-redis.abc123.ng.0001.use1.cache.amazonaws.com"
TARGET_PORT="6379"
TARGET_PASS="target-auth-token"

# 1. –°–æ–∑–¥–∞–π backup –∏—Å—Ç–æ—á–Ω–∏–∫–∞
redis-cli -h $SOURCE_HOST -p $SOURCE_PORT -a $SOURCE_PASS BGSAVE

# 2. –î–æ–∂–¥–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
while [ $(redis-cli -h $SOURCE_HOST -a $SOURCE_PASS INFO persistence | grep rdb_bgsave_in_progress | cut -d: -f2 | tr -d '\r') -eq 1 ]; do
  echo "Waiting for BGSAVE to complete..."
  sleep 5
done

# 3. –°–∫–æ–ø–∏—Ä—É–π RDB —Ñ–∞–π–ª
scp user@source-server:/var/lib/redis/dump.rdb ./dump.rdb

# 4. –î–ª—è ElastiCache –∏—Å–ø–æ–ª—å–∑—É–π redis-cli —Å --pipe –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π AWS DMS (Database Migration Service)

# 5. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è - —Å—Ä–∞–≤–Ω–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–π
SOURCE_KEYS=$(redis-cli -h $SOURCE_HOST -p $SOURCE_PORT -a $SOURCE_PASS DBSIZE)
TARGET_KEYS=$(redis-cli -h $TARGET_HOST -p $TARGET_PORT -a $TARGET_PASS --tls DBSIZE)

echo "Source keys: $SOURCE_KEYS"
echo "Target keys: $TARGET_KEYS"

if [ "$SOURCE_KEYS" -eq "$TARGET_KEYS" ]; then
  echo "Migration successful!"
else
  echo "Migration incomplete. Keys mismatch."
  exit 1
fi
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Multi-Region Disaster Recovery (AWS):**

bash

```bash
# Region 1 (Primary): us-east-1
# Region 2 (DR): us-west-2

# Setup:
# 1. ElastiCache –≤ –æ–±–æ–∏—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö
# 2. Global Datastore (Redis 5.0.6+, cluster mode)
# 3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏
# 4. Failover < 1 –º–∏–Ω—É—Ç–∞

# Terraform
resource "aws_elasticache_global_replication_group" "example" {
  global_replication_group_id_suffix = "my-redis"
  primary_replication_group_id      = aws_elasticache_replication_group.primary.id
}

resource "aws_elasticache_replication_group" "secondary" {
  replication_group_id        = "my-redis-secondary"
  global_replication_group_id = aws_elasticache_global_replication_group.example.id
  # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
}
```

**2. Azure Active Geo-Replication:**

bash

```bash
# Premium tier feature
# –î–≤–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞
# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è
# –ú–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å –∏ –ø–∏—Å–∞—Ç—å –≤ –æ–±–∞
# Conflict resolution –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π

az redis create \
  --name myredis-primary \
  --resource-group myResourceGroup \
  --location eastus \
  --sku Premium \
  --vm-size P1

az redis create \
  --name myredis-secondary \
  --resource-group myResourceGroup \
  --location westus \
  --sku Premium \
  --vm-size P1

# –ù–∞—Å—Ç—Ä–æ–π geo-replication
az redis link create \
  --name myredis-primary \
  --resource-group myResourceGroup \
  --linked-server myredis-secondary \
  --linked-server-location westus \
  --linked-server-role Secondary
```

**3. Hybrid Cloud setup:**

bash

```bash
# Self-hosted Redis + Cloud Redis
# Use case: gradual migration, burst capacity

# Setup VPN/Direct Connect –º–µ–∂–¥—É on-prem –∏ cloud
# –ù–∞—Å—Ç—Ä–æ–π —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é: on-prem master -> cloud replica
# –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: cloud —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è master

# AWS Site-to-Site VPN
# Azure ExpressRoute
# GCP Cloud Interconnect
```

---

## –ú–æ–¥—É–ª—å 10: Advanced —Ç–µ–º—ã –∏ –Ω–æ–≤—ã–µ —Ñ–∏—á–∏ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Lua Scripting:**

lua

```lua
-- –ê—Ç–æ–º–∞—Ä–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è: get –∏ set
local current = redis.call('GET', KEYS[1])
if current then
  redis.call('SET', KEYS[1], ARGV[1])
  return current
else
  return nil
end
```

bash

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
redis-cli EVAL "return redis.call('SET', KEYS[1], ARGV[1])" 1 mykey myvalue

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞
SCRIPT LOAD "script content"
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç SHA1 hash

# –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ hash
EVALSHA sha1 numkeys key [key ...] arg [arg ...]

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞–º–∏
SCRIPT EXISTS sha1 [sha1 ...]
SCRIPT FLUSH
SCRIPT KILL  # –£–±–∏—Ç—å running —Å–∫—Ä–∏–ø—Ç
```

**–¢–∏–ø–∏—á–Ω—ã–µ Lua patterns:**

lua

```lua
-- Rate limiting (token bucket)
local key = KEYS[1]
local capacity = tonumber(ARGV[1])
local timestamp = tonumber(ARGV[2])
local requested = tonumber(ARGV[3])

local current = redis.call('HMGET', key, 'tokens', 'timestamp')
local tokens = tonumber(current[1]) or capacity
local last_time = tonumber(current[2]) or timestamp

local delta = timestamp - last_time
local refill = delta * (capacity / 60)  -- refill rate per second
tokens = math.min(capacity, tokens + refill)

if tokens >= requested then
  tokens = tokens - requested
  redis.call('HMSET', key, 'tokens', tokens, 'timestamp', timestamp)
  return 1
else
  return 0
end

-- Distributed lock —Å TTL
local key = KEYS[1]
local id = ARGV[1]
local ttl = ARGV[2]

if redis.call('SET', key, id, 'NX', 'EX', ttl) then
  return 1
else
  return 0
end
```

**Transactions (MULTI/EXEC):**

bash

```bash
# Optimistic locking —Å WATCH
WATCH mykey
val = GET mykey
val = val + 1
MULTI
SET mykey $val
EXEC

# –ï—Å–ª–∏ mykey –∏–∑–º–µ–Ω–∏–ª—Å—è –º–µ–∂–¥—É WATCH –∏ EXEC - transaction –æ—Ç–º–µ–Ω—è–µ—Ç—Å—è
```

python

```python
# Python –ø—Ä–∏–º–µ—Ä
def increment_with_retry(r, key, max_retries=10):
    for i in range(max_retries):
        try:
            with r.pipeline() as pipe:
                pipe.watch(key)
                current = int(pipe.get(key) or 0)
                pipe.multi()
                pipe.set(key, current + 1)
                pipe.execute()
                return current + 1
        except redis.WatchError:
            continue
    raise Exception("Too many retries")
```

**Redis Streams (–¥–µ—Ç–∞–ª—å–Ω–æ):**

bash

```bash
# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ stream
XADD mystream * field1 value1 field2 value2
XADD mystream MAXLEN ~ 1000 * field value  # –° –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã

# –ß—Ç–µ–Ω–∏–µ
XRANGE mystream - +           # –í—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
XRANGE mystream 1609459200000 +  # –° –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ timestamp
XREAD COUNT 10 STREAMS mystream 0  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10

# Consumer Groups
XGROUP CREATE mystream mygroup 0
XREADGROUP GROUP mygroup consumer1 COUNT 10 STREAMS mystream >

# Acknowledge
XACK mystream mygroup message-id

# Pending messages
XPENDING mystream mygroup
XPENDING mystream mygroup - + 10 consumer1

# Claim pending messages (–µ—Å–ª–∏ consumer —É–º–µ—Ä)
XCLAIM mystream mygroup consumer2 3600000 message-id

# Auto-claim (Redis 6.2+)
XAUTOCLAIM mystream mygroup consumer2 3600000 0
```

**Redis Modules:**

**1. RedisJSON:**

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ (–µ—Å–ª–∏ self-hosted)
redis-server --loadmodule /path/to/rejson.so

# –ö–æ–º–∞–Ω–¥—ã
JSON.SET user:1 $ '{"name":"John","age":30,"address":{"city":"NYC"}}'
JSON.GET user:1
JSON.GET user:1 $.name
JSON.GET user:1 $.address.city

# –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è
JSON.SET user:1 $.age 31
JSON.NUMINCRBY user:1 $.age 1
JSON.ARRAPPEND user:1 $.tags '"redis"'

# –£–¥–∞–ª–µ–Ω–∏–µ
JSON.DEL user:1 $.address
```

**2. RedisSearch:**

bash

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
FT.CREATE idx:users ON HASH PREFIX 1 user: SCHEMA name TEXT age NUMERIC city TAG

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–æ–±—ã—á–Ω—ã–π HSET)
HSET user:1 name "John Doe" age 30 city "NYC"

# –ü–æ–∏—Å–∫
FT.SEARCH idx:users "John"
FT.SEARCH idx:users "@city:{NYC}"
FT.SEARCH idx:users "@age:[25 35]"
FT.SEARCH idx:users "John @city:{NYC}"

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
FT.AGGREGATE idx:users "*" GROUPBY 1 @city REDUCE COUNT 0 AS count
```

**3. RedisTimeSeries:**

bash

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ time series
TS.CREATE temperature:sensor1 RETENTION 86400000 LABELS sensor_id 1 location room1

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
TS.ADD temperature:sensor1 * 23.5
TS.MADD temperature:sensor1 1609459200000 22.1 temperature:sensor2 1609459200000 24.3

# –ó–∞–ø—Ä–æ—Å—ã
TS.RANGE temperature:sensor1 1609459200000 1609545600000
TS.RANGE temperature:sensor1 - + AGGREGATION avg 3600000  # Hourly average

# Multi-series –∑–∞–ø—Ä–æ—Å—ã
TS.MRANGE 1609459200000 + FILTER location=room1

# –ü—Ä–∞–≤–∏–ª–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
TS.CREATERULE temperature:sensor1 temperature:sensor1:avg AGGREGATION avg 60000
```

**4. RedisBloom (Probabilistic Data Structures):**

bash

```bash
# Bloom Filter
BF.RESERVE users:exists 0.01 1000000
BF.ADD users:exists user:1
BF.EXISTS users:exists user:1
BF.MADD users:exists user:2 user:3 user:4

# Cuckoo Filter (–º–æ–∂–µ—Ç —É–¥–∞–ª—è—Ç—å)
CF.RESERVE items:exists 1000000
CF.ADD items:exists item:1
CF.DEL items:exists item:1

# Count-Min Sketch (–ø–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã)
CMS.INITBYDIM page:views 1000 10
CMS.INCRBY page:views /home 1 /about 1
CMS.QUERY page:views /home

# Top-K (—Ç–æ–ø K —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
TOPK.RESERVE popular:items 10 1000 10 0.9
TOPK.ADD popular:items item1 item2 item3
TOPK.LIST popular:items
```

**Redis 7.0 –Ω–æ–≤—ã–µ —Ñ–∏—á–∏:**

bash

```bash
# Redis Functions (–∑–∞–º–µ–Ω–∞ Lua scripting)
FUNCTION LOAD "#!lua name=mylib
redis.register_function('myfunc', function(keys, args)
  return redis.call('GET', keys[1])
end)"

FCALL myfunc 1 mykey

# Sharded Pub/Sub
SSUBSCRIBE channel  # Subscribe –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π shard
SPUBLISH channel message

# ACL v2
ACL SETUSER alice +@all -@dangerous

# Command introspection
COMMAND DOCS SET
COMMAND GETKEYS SET key value

# EXPIRETIME / PEXPIRETIME
SET key value EX 100
EXPIRETIME key  # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç Unix timestamp –∏—Å—Ç–µ—á–µ–Ω–∏—è
```

**Pipeline –∏ –º–∞—Å—Å–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:**

python

```python
import redis

r = redis.Redis()

# –ë–µ–∑ pipeline: N round-trips
for i in range(10000):
    r.set(f"key:{i}", f"value:{i}")

# –° pipeline: 1 round-trip (–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ 1000)
pipe = r.pipeline()
for i in range(10000):
    pipe.set(f"key:{i}", f"value:{i}")
pipe.execute()

# –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ pipeline
pipe = r.pipeline(transaction=True)
pipe.multi()
pipe.incr("counter")
pipe.expire("counter", 60)
pipe.execute()
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–†–µ–∞–ª–∏–∑—É–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ patterns:

**1. Rate Limiter –Ω–∞ Lua:**

lua

```lua
-- rate_limiter.lua
local key = KEYS[1]
local limit = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local current_time = tonumber(ARGV[3])

local window_start = current_time - window
redis.call('ZREMRANGEBYSCORE', key, '-inf', window_start)

local current_count = redis.call('ZCARD', key)

if current_count < limit then
  redis.call('ZADD', key, current_time, current_time)
  redis.call('EXPIRE', key, window)
  return 1
else
  return 0
end
```

python

```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
import time
import redis

r = redis.Redis()

with open('rate_limiter.lua', 'r') as f:
    rate_limiter_script = f.read()

rate_limiter = r.register_script(rate_limiter_script)

def check_rate_limit(user_id, limit=10, window=60):
    key = f"rate_limit:{user_id}"
    current_time = int(time.time())
    return rate_limiter(keys=[key], args=[limit, window, current_time])

# –¢–µ—Å—Ç
for i in range(15):
    allowed = check_rate_limit("user:123", limit=10, window=60)
    print(f"Request {i+1}: {'ALLOWED' if allowed else 'BLOCKED'}")
```

**2. Distributed Queue —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏:**

python

```python
import redis
import json
import time

class PriorityQueue:
    def __init__(self, redis_client, queue_name):
        self.r = redis_client
        self.queue = f"queue:{queue_name}"
        self.processing = f"processing:{queue_name}"
    
    def enqueue(self, task, priority=0):
        """Priority: lower = higher priority"""
        task_data = json.dumps(task)
        self.r.zadd(self.queue, {task_data: priority})
    
    def dequeue(self, timeout=10):
        """Dequeue —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º –≤ processing set"""
        # Atomic: get task with lowest score
        pipeline = self.r.pipeline()
        pipeline.watch(self.queue)
        
        tasks = self.r.zrange(self.queue, 0, 0)
        if not tasks:
            return None
        
        task_data = tasks[0]
        
        pipeline.multi()
        pipeline.zrem(self.queue, task_data)
        pipeline.sadd(self.processing, task_data)
        pipeline.execute()
        
        return json.loads(task_data)
    
    def complete(self, task):
        """Mark task as completed"""
        task_data = json.dumps(task)
        self.r.srem(self.processing, task_data)
    
    def requeue_processing(self):
        """Requeue stuck tasks from processing set"""
        processing_tasks = self.r.smembers(self.processing)
        for task_data in processing_tasks:
            self.r.srem(self.processing, task_data)
            self.r.zadd(self.queue, {task_data: 0})

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
r = redis.Redis()
q = PriorityQueue(r, "tasks")

# Producer
q.enqueue({"type": "email", "to": "user@example.com"}, priority=5)
q.enqueue({"type": "sms", "to": "+1234567890"}, priority=1)  # Higher priority
q.enqueue({"type": "notification", "user_id": 123}, priority=10)

# Consumer
while True:
    task = q.dequeue()
    if task:
        print(f"Processing: {task}")
        # Process task
        time.sleep(1)
        q.complete(task)
    else:
        time.sleep(0.1)
```

**3. Session Store —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º TTL:**

python

```python
import redis
import json
import uuid
from datetime import datetime, timedelta

class SessionStore:
    def __init__(self, redis_client, ttl=3600):
        self.r = redis_client
        self.ttl = ttl
    
    def create(self, user_id, data=None):
        """Create new session"""
        session_id = str(uuid.uuid4())
        session_key = f"session:{session_id}"
        
        session_data = {
            "session_id": session_id,
            "user_id": user_id,
            "created_at": datetime.now().isoformat(),
            "last_access": datetime.now().isoformat(),
            "data": data or {}
        }
        
        self.r.setex(session_key, self.ttl, json.dumps(session_data))
        return session_id
    
    def get(self, session_id, refresh=True):
        """Get session and optionally refresh TTL"""
        session_key = f"session:{session_id}"
        data = self.r.get(session_key)
        
        if not data:
            return None
        
        session = json.loads(data)
        
        if refresh:
            session["last_access"] = datetime.now().isoformat()
            self.r.setex(session_key, self.ttl, json.dumps(session))
        
        return session
    
    def update(self, session_id, data):
        """Update session data"""
        session = self.get(session_id, refresh=False)
        if not session:
            return False
        
        session["data"].update(data)
        session["last_access"] = datetime.now().isoformat()
        
        session_key = f"session:{session_id}"
        self.r.setex(session_key, self.ttl, json.dumps(session))
        return True
    
    def destroy(self, session_id):
        """Destroy session"""
        session_key = f"session:{session_id}"
        self.r.delete(session_key)
    
    def get_user_sessions(self, user_id):
        """Get all sessions for user (requires secondary index)"""
        # –ò—Å–ø–æ–ª—å–∑—É–π SCAN –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö session:* –∫–ª—é—á–µ–π
        sessions = []
        for key in self.r.scan_iter(match="session:*"):
            data = self.r.get(key)
            if data:
                session = json.loads(data)
                if session["user_id"] == user_id:
                    sessions.append(session)
        return sessions

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
r = redis.Redis()
store = SessionStore(r, ttl=1800)  # 30 –º–∏–Ω—É—Ç

# Create session
sid = store.create("user:123", {"role": "admin"})
print(f"Session created: {sid}")

# Get session (auto-refreshes TTL)
session = store.get(sid)
print(f"Session data: {session}")

# Update
store.update(sid, {"last_page": "/dashboard"})

# Destroy
store.destroy(sid)
```

**4. Real-time Leaderboard —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏:**

python

```python
import redis
import time
from datetime import datetime, timedelta

class Leaderboard:
    def __init__(self, redis_client, name):
        self.r = redis_client
        self.name = name
    
    def add_score(self, player_id, score, timestamp=None):
        """Add score to all leaderboards"""
        if timestamp is None:
            timestamp = time.time()
        
        # All-time leaderboard
        self.r.zadd(f"lb:{self.name}:all", {player_id: score})
        
        # Daily leaderboard
        day = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d")
        self.r.zadd(f"lb:{self.name}:daily:{day}", {player_id: score})
        self.r.expire(f"lb:{self.name}:daily:{day}", 86400 * 7)  # Keep 7 days
        
        # Weekly leaderboard
        week = datetime.fromtimestamp(timestamp).strftime("%Y-W%U")
        self.r.zadd(f"lb:{self.name}:weekly:{week}", {player_id: score})
        self.r.expire(f"lb:{self.name}:weekly:{week}", 86400 * 30)  # Keep 30 days
    
    def get_top(self, period="all", count=10, date=None):
        """Get top players"""
        if period == "all":
            key = f"lb:{self.name}:all"
        elif period == "daily":
            day = date or datetime.now().strftime("%Y-%m-%d")
            key = f"lb:{self.name}:daily:{day}"
        elif period == "weekly":
            week = date or datetime.now().strftime("%Y-W%U")
            key = f"lb:{self.name}:weekly:{week}"
        else:
            raise ValueError("Invalid period")
        
        return self.r.zrevrange(key, 0, count-1, withscores=True)
    
    def get_rank(self, player_id, period="all", date=None):
        """Get player rank"""
        if period == "all":
            key = f"lb:{self.name}:all"
        elif period == "daily":
            day = date or datetime.now().strftime("%Y-%m-%d")
            key = f"lb:{self.name}:daily:{day}"
        elif period == "weekly":
            week = date or datetime.now().strftime("%Y-W%U")
            key = f"lb:{self.name}:weekly:{week}"
        
        rank = self.r.zrevrank(key, player_id)
        score = self.r.zscore(key, player_id)
        return {
        "rank": rank + 1 if rank is not None else None,
        "score": score
    }

	def get_around(self, player_id, period="all", range_size=5):
	    """Get players around specific player"""
	    if period == "all":
	        key = f"lb:{self.name}:all"
	    elif period == "daily":
	        day = datetime.now().strftime("%Y-%m-%d")
	        key = f"lb:{self.name}:daily:{day}"
	    elif period == "weekly":
	        week = datetime.now().strftime("%Y-W%U")
	        key = f"lb:{self.name}:weekly:{week}"
	    
	    rank = self.r.zrevrank(key, player_id)
	    if rank is None:
	        return []
	    
	    start = max(0, rank - range_size)
	    end = rank + range_size
	    
	    return self.r.zrevrange(key, start, end, withscores=True)
	    
	    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
		r = redis.Redis() lb = Leaderboard(r, "global")
		
		# Add scores		
		lb.add_score("player:1", 1000) lb.add_score("player:2", 1500)
		lb.add_score("player:3", 800) lb.add_score("player:4", 2000)
		lb.add_score("player:5", 1200)
		
		# Get top 3		
		print("Top 3 all-time:") for rank, (player, score) in enumerate(lb.get_top("all", 3), 1): print(f"{rank}. {player}: {score}")
		
		# Get player rank		
		print("\nPlayer 2 stats:") print(lb.get_rank("player:2", "all"))
		
		# Get players around player 3		
		print("\nAround player 3:") print(lb.get_around("player:3", "all", range_size=2))
	}

````

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Redis –∫–∞–∫ Message Broker (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RabbitMQ/Kafka –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª—É—á–∞–µ–≤):**
```python
import redis
import json
import threading
import time

class RedisMessageBroker:
    def __init__(self, redis_client):
        self.r = redis_client
        self.pubsub = self.r.pubsub()
        self.running = False
    
    def publish(self, topic, message):
        """Publish message to topic"""
        self.r.publish(topic, json.dumps(message))
    
    def subscribe(self, topics, callback):
        """Subscribe to topics with callback"""
        self.pubsub.subscribe(**{topic: callback for topic in topics})
        self.running = True
        
        thread = threading.Thread(target=self._listen)
        thread.daemon = True
        thread.start()
    
    def _listen(self):
        """Listen for messages"""
        while self.running:
            message = self.pubsub.get_message()
            if message and message['type'] == 'message':
                # Callback –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ subscribe
                pass
            time.sleep(0.001)
    
    def stop(self):
        """Stop listening"""
        self.running = False
        self.pubsub.close()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
r = redis.Redis()
broker = RedisMessageBroker(r)

# Subscriber
def handle_order(message):
    data = json.loads(message['data'])
    print(f"New order: {data}")

def handle_notification(message):
    data = json.loads(message['data'])
    print(f"Notification: {data}")

broker.subscribe(['orders', 'notifications'], lambda msg: handle_order(msg) if msg['channel'] == b'orders' else handle_notification(msg))

# Publisher
broker.publish('orders', {'order_id': 123, 'total': 99.99})
broker.publish('notifications', {'user_id': 456, 'message': 'Order shipped'})

time.sleep(1)
broker.stop()
```

**2. Redis Streams –¥–ª—è Event Sourcing:**
```python
import redis
import json
import uuid

class EventStore:
    def __init__(self, redis_client):
        self.r = redis_client
    
    def append_event(self, aggregate_id, event_type, data):
        """Append event to aggregate stream"""
        stream_key = f"events:{aggregate_id}"
        
        event = {
            'event_id': str(uuid.uuid4()),
            'event_type': event_type,
            'timestamp': int(time.time() * 1000),
            'data': json.dumps(data)
        }
        
        event_id = self.r.xadd(stream_key, event)
        return event_id
    
    def get_events(self, aggregate_id, start='-', end='+'):
        """Get all events for aggregate"""
        stream_key = f"events:{aggregate_id}"
        events = self.r.xrange(stream_key, start, end)
        
        return [
            {
                'event_id': event_id.decode(),
                'event_type': data[b'event_type'].decode(),
                'timestamp': int(data[b'timestamp']),
                'data': json.loads(data[b'data'])
            }
            for event_id, data in events
        ]
    
    def rebuild_aggregate(self, aggregate_id, aggregate_class):
        """Rebuild aggregate from events"""
        events = self.get_events(aggregate_id)
        aggregate = aggregate_class(aggregate_id)
        
        for event in events:
            aggregate.apply_event(event)
        
        return aggregate

# Example: Order aggregate
class Order:
    def __init__(self, order_id):
        self.order_id = order_id
        self.items = []
        self.status = 'draft'
        self.total = 0
    
    def apply_event(self, event):
        if event['event_type'] == 'OrderCreated':
            self.status = 'created'
        elif event['event_type'] == 'ItemAdded':
            self.items.append(event['data'])
            self.total += event['data']['price']
        elif event['event_type'] == 'OrderPlaced':
            self.status = 'placed'
        elif event['event_type'] == 'OrderShipped':
            self.status = 'shipped'

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
r = redis.Redis()
store = EventStore(r)

order_id = "order:123"

# Append events
store.append_event(order_id, 'OrderCreated', {})
store.append_event(order_id, 'ItemAdded', {'sku': 'ABC', 'price': 29.99})
store.append_event(order_id, 'ItemAdded', {'sku': 'DEF', 'price': 49.99})
store.append_event(order_id, 'OrderPlaced', {})

# Rebuild aggregate
order = store.rebuild_aggregate(order_id, Order)
print(f"Order {order.order_id}: {order.status}, Total: ${order.total}")
```

**3. Geo-Distributed Caching —Å CRDTs:**
```python
# Conflict-free Replicated Data Types –¥–ª—è multi-datacenter
# –ò—Å–ø–æ–ª—å–∑—É–π Redis + –ø–æ—Å–ª–µ–¥–Ω–∏–µ timestamps –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤

class LWWRegister:  # Last-Write-Wins Register
    def __init__(self, redis_client, key):
        self.r = redis_client
        self.key = key
    
    def set(self, value, timestamp=None):
        if timestamp is None:
            timestamp = time.time()
        
        # Store value with timestamp
        self.r.hset(self.key, mapping={
            'value': value,
            'timestamp': timestamp
        })
    
    def get(self):
        data = self.r.hgetall(self.key)
        if not data:
            return None
        return data[b'value'].decode()
    
    def merge(self, other_value, other_timestamp):
        """Merge with value from another datacenter"""
        current = self.r.hgetall(self.key)
        
        if not current:
            self.set(other_value, other_timestamp)
            return
        
        current_timestamp = float(current[b'timestamp'])
        
        # Last-write-wins: keep newer value
        if other_timestamp > current_timestamp:
            self.set(other_value, other_timestamp)
```

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ —Ä–µ—Å—É—Ä—Å—ã

### –ö–Ω–∏–≥–∏:
1. **"Redis in Action"** - Josiah Carlson (–∫–ª–∞—Å—Å–∏–∫–∞)
2. **"Redis Essentials"** - Maxwell Dayvson Da Silva
3. **"Mastering Redis"** - Jeremy Nelson
4. **"Redis 4.x Cookbook"** - Pengcheng Huang

### Online –∫—É—Ä—Å—ã:
1. **Redis University** (–±–µ—Å–ø–ª–∞—Ç–Ω–æ) - university.redis.com
   - RU101: Introduction to Redis Data Structures
   - RU102: Redis Streams
   - RU202: Redis Streams (Advanced)
   - RU301: Running Redis at Scale

2. **Pluralsight** - Redis courses

3. **YouTube –∫–∞–Ω–∞–ª—ã:**
   - Redis Official Channel
   - TechWorld with Nana (Redis tutorial)

### –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:
- **redis.io/documentation** - –≥–ª–∞–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **redis.io/commands** - reference –ø–æ –≤—Å–µ–º –∫–æ–º–∞–Ω–¥–∞–º
- **redis.io/topics** - —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã
- **github.com/redis/redis** - –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
1. **RedisInsight** - GUI –∫–ª–∏–µ–Ω—Ç –æ—Ç Redis Labs
2. **redis-cli** - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π CLI
3. **redis-benchmark** - benchmarking tool
4. **redis-check-aof / redis-check-rdb** - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
5. **Another Redis Desktop Manager** - open-source GUI
6. **Medis** - macOS GUI client

### Community:
- **Redis Discord** - discord.gg/redis
- **Reddit** - r/redis
- **Stack Overflow** - —Ç—ç–≥ [redis]
- **Redis Labs Blog** - redis.com/blog

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
1. **Prometheus + Grafana**
2. **Datadog Redis integration**
3. **New Relic Redis monitoring**
4. **Redis Enterprise (–ø–ª–∞—Ç–Ω—ã–π)**
5. **RedisInsight** (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π)

### Best practices —á–µ–∫–ª–∏—Å—Ç—ã:

**Development:**
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π connection pooling
- ‚úÖ Set reasonable timeouts
- ‚úÖ Handle connection errors gracefully
- ‚úÖ Use pipelines for batch operations
- ‚úÖ Avoid KEYS in production (use SCAN)
- ‚úÖ Set TTL on cache keys
- ‚úÖ Use appropriate data structures

**Operations:**
- ‚úÖ Enable persistence (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
- ‚úÖ Configure maxmemory –∏ eviction policy
- ‚úÖ Set up replication –¥–ª—è HA
- ‚úÖ Use Sentinel –∏–ª–∏ Cluster –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ failover
- ‚úÖ Regular backups
- ‚úÖ Monitor key metrics
- ‚úÖ Set up alerts
- ‚úÖ Document recovery procedures
- ‚úÖ Regular security audits
- ‚úÖ Keep Redis updated

**Security:**
- ‚úÖ Always use password (requirepass)
- ‚úÖ Use ACL –¥–ª—è fine-grained permissions
- ‚úÖ Bind to specific interfaces
- ‚úÖ Enable protected mode
- ‚úÖ Use TLS –¥–ª—è encryption in-transit
- ‚úÖ Rename dangerous commands
- ‚úÖ Firewall rules
- ‚úÖ Regular security updates
- ‚úÖ Audit logs monitoring
- ‚úÖ Least privilege principle

---

## –§–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫-–ª–∏—Å—Ç –∑–Ω–∞–Ω–∏–π

–ü–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∫—É—Ä—Å–∞ —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

### Beginner (–ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å):
- ‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —á—Ç–æ —Ç–∞–∫–æ–µ Redis –∏ –∫–æ–≥–¥–∞ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
- ‚úÖ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å Redis (Docker, package manager)
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å redis-cli —É–≤–µ—Ä–µ–Ω–Ω–æ
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –±–∞–∑–æ–≤—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö (String, List, Set, Hash, Sorted Set)
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å TTL –∏ expiration
- ‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É RDB –∏ AOF
- ‚úÖ –î–µ–ª–∞—Ç—å backup –∏ restore

### Intermediate (–°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å):
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Master-Replica —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é
- ‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ eviction policies
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å transactions (MULTI/EXEC)
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Pub/Sub –¥–ª—è messaging
- ‚úÖ –ü–∏—Å–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ Lua —Å–∫—Ä–∏–ø—Ç—ã
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å security (passwords, ACL)
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pipelining –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

### Advanced (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å):
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Redis Sentinel –¥–ª—è HA
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å Redis Cluster
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis Streams –¥–ª—è event-driven –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
- ‚úÖ –ü–∏—Å–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ Lua —Å–∫—Ä–∏–ø—Ç—ã
- ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ patterns (rate limiting, distributed locks, leaderboards)
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å memory usage
- ‚úÖ Troubleshooting production issues
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π monitoring stack (Prometheus/Grafana)

### Expert (–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å):
- ‚úÖ –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å multi-region –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- ‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å Redis –º–æ–¥—É–ª—è–º–∏ (JSON, Search, TimeSeries, Bloom)
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis –≤ Kubernetes
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Redis –≤ –æ–±–ª–∞–∫–∞—Ö (AWS, Azure, GCP)
- ‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ Redis
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è specific workloads
- ‚úÖ Capacity planning
- ‚úÖ Disaster recovery planning –∏ execution
- ‚úÖ –í–Ω–æ—Å–∏—Ç—å –≤–∫–ª–∞–¥ –≤ Redis community

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ (Real-world use cases)

### 1. E-commerce Platform

Use cases:

- Session storage (1M+ concurrent users)
- Shopping cart (temporary data, TTL)
- Product catalog cache
- Real-time inventory
- Flash sale countdown (atomic decrements)
- User wish lists (Sets)
- Recently viewed products (Lists with LTRIM)
- Recommendation scores (Sorted Sets)
- Rate limiting API calls
- Distributed locks –¥–ª—è order processing

Architecture:

- Redis Cluster (6 nodes: 3 masters + 3 replicas)
- Sentinel –¥–ª—è HA
- Read replicas –¥–ª—è read-heavy operations
- Separate Redis –¥–ª—è sessions vs cache

```

### 2. Social Media App
```

Use cases:

- News feed (Lists + Sorted Sets)
- Followers/Following (Sets —Å SINTER –¥–ª—è mutual friends)
- Trending topics (Sorted Sets —Å time-decay scoring)
- Real-time notifications (Pub/Sub)
- User activity streams (Redis Streams)
- Like counters (INCR)
- Comments cache (Hashes)
- Online users presence (Sets —Å TTL)
- Direct messages queue (Lists —Å BRPOP)

Architecture:

- Multi-region setup –¥–ª—è global users
- Redis Streams –¥–ª—è event sourcing
- Separate Redis instances –ø–æ feature
- Heavy use of Lua scripts –¥–ª—è atomic operations

```

### 3. IoT Platform
```

Use cases:

- Device metrics (TimeSeries)
- Device state (Hashes)
- Alerts queue (Lists)
- Anomaly detection (Bloom Filters)
- Time-series aggregations
- Device groups (Sets)
- Command queue –¥–ª—è devices (Lists)
- Real-time dashboards (Pub/Sub)

Architecture:

- RedisTimeSeries module
- High write throughput configuration
- Retention policies –¥–ª—è old data
- Aggregation rules

```

### 4. Gaming Platform
```

Use cases:

- Leaderboards (Sorted Sets)
- Player sessions
- Real-time multiplayer state
- Matchmaking queue
- Player inventory (Hashes)
- Achievement tracking
- Chat rooms (Pub/Sub)
- Game events (Streams)
- Anti-cheat rate limiting

Architecture:

- Low latency setup
- Geo-distributed –¥–ª—è global players
- Heavy use of Sorted Sets
- Lua scripts –¥–ª—è atomic game logic


---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ü–æ–∑–¥—Ä–∞–≤–ª—è—é —Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º –ø–æ–ª–Ω–æ–≥–æ –∫—É—Ä—Å–∞ Redis!

**–¢–µ–ø–µ—Ä—å —Ç—ã –∑–Ω–∞–µ—à—å:**
- ‚úÖ –í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ Redis –æ—Ç –ê –¥–æ –Ø
- ‚úÖ Production-ready –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- ‚úÖ High availability –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- ‚úÖ Performance –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- ‚úÖ Security best practices
- ‚úÖ Monitoring –∏ troubleshooting
- ‚úÖ –û–±–ª–∞—á–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
- ‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∏ patterns
- ‚úÖ Real-world use cases

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–ª–∞–Ω –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è:**

**–ü–µ—Ä–≤—ã–µ 3 –º–µ—Å—è—Ü–∞:**
1. –ò—Å–ø–æ–ª—å–∑—É–π Redis –≤ —Å–≤–æ–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö
2. –ü–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π —Å —Ä–∞–∑–Ω—ã–º–∏ data structures
3. –†–µ–∞–ª–∏–∑—É–π 2-3 patterns –∏–∑ –∫—É—Ä—Å–∞
4. –ù–∞—Å—Ç—Ä–æ–π –±–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

**3-6 –º–µ—Å—è—Ü–µ–≤:**
1. –ò–∑—É—á–∏ –æ–¥–∏–Ω Redis module –¥–µ—Ç–∞–ª—å–Ω–æ (RedisJSON –∏–ª–∏ RedisSearch)
2. –ù–∞—Å—Ç—Ä–æ–π production HA setup
3. –ü—Ä–æ–≤–µ–¥–∏ performance —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
4. –ù–∞–ø–∏—à–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ Lua —Å–∫—Ä–∏–ø—Ç–æ–≤

**6-12 –º–µ—Å—è—Ü–µ–≤:**
1. –ü—Ä–æ–π–¥–∏ Redis University –∫—É—Ä—Å—ã
2. –ù–∞—Å—Ç—Ä–æ–π multi-region setup
3. –í–Ω–æ—Å–∏ –≤–∫–ª–∞–¥ –≤ open-source –ø—Ä–æ–µ–∫—Ç—ã
4. –ü–æ–¥–µ–ª–∏—Å—å –∑–Ω–∞–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ blog –∏–ª–∏ meetup
5. –†–∞—Å—Å–º–æ—Ç—Ä–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é (–µ—Å–ª–∏ –µ—Å—Ç—å –æ—Ç Redis)

**–ü–æ–≤—Ç–æ—Ä—è–π —ç—Ç–æ—Ç –∫—É—Ä—Å –∫–∞–∂–¥—ã–µ 6-12 –º–µ—Å—è—Ü–µ–≤:**
- Redis –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è
- –ù–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ –¥–æ–±–∞–≤–ª—è—é—Ç features
- –¢–≤–æ–π –æ–ø—ã—Ç —Ä–∞—Å—Ç–µ—Ç - –±—É–¥–µ—à—å –∑–∞–º–µ—á–∞—Ç—å –Ω–æ–≤—ã–µ –¥–µ—Ç–∞–ª–∏
- Best practices —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—Ç

**–ü–æ–ª–µ–∑–Ω—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏:**
- üìñ –ß–∏—Ç–∞–π Redis release notes –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π
- üë• –£—á–∞—Å—Ç–≤—É–π –≤ Redis community (Discord, Reddit)
- üîß –†–µ–≥—É–ª—è—Ä–Ω–æ –º–æ–Ω–∏—Ç–æ—Ä—å —Å–≤–æ–∏ production instances
- üìù –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π —Å–≤–æ–∏ –∫–µ–π—Å—ã –∏ —Ä–µ—à–µ–Ω–∏—è
- üéì –î–µ–ª–∏—Å—å –∑–Ω–∞–Ω–∏—è–º–∏ —Å –∫–æ–º–∞–Ω–¥–æ–π

**–ü–æ–º–Ω–∏ –≥–ª–∞–≤–Ω–æ–µ:**
- Redis - —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∞ –Ω–µ —Ä–µ—à–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∞–∂–Ω–µ–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–Ω—ã
- Security –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∞ –ø–µ—Ä–≤–æ–º –º–µ—Å—Ç–µ
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è - —Ç–≤–æ–π –ª—É—á—à–∏–π –¥—Ä—É–≥

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ challenge'—ã –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏:**
1. –ù–∞—Å—Ç—Ä–æ–π Redis —Å –Ω—É–ª—è –≤ production
2. –ü—Ä–æ–≤–µ–¥–∏ –º–∏–≥—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É Redis –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
3. –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π memory usage –Ω–∞ 50%
4. –†–µ–∞–ª–∏–∑—É–π custom Redis module
5. –ù–∞–ø–∏—à–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫—É-–æ–±–µ—Ä—Ç–∫—É –¥–ª—è —Å–≤–æ–µ–≥–æ —è–∑—ã–∫–∞
6. –°–æ–∑–¥–∞–π Redis operator –¥–ª—è Kubernetes
7. –°–¥–µ–ª–∞–π fork Redis –∏ –¥–æ–±–∞–≤—å —Å–≤–æ—é —Ñ–∏—á—É
8. –ù–∞–ø–∏—à–∏ performance benchmarking suite

**–ö—É–¥–∞ –∏–¥—Ç–∏ –¥–∞–ª—å—à–µ:**
- **–î–ª—è backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤:** –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Redis —Å —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏, ORM patterns
- **–î–ª—è DevOps/SRE:** Kubernetes operators, Infrastructure as Code, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
- **–î–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–≤:** Multi-region patterns, capacity planning, cost optimization
- **–î–ª—è Data Engineers:** Redis Streams, time-series, analytics workloads

**–ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–æ–≤–µ—Ç:**
–ù–µ –ø—ã—Ç–∞–π—Å—è –∑–∞–ø–æ–º–Ω–∏—Ç—å –≤—Å—ë - –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –∫—É—Ä—Å –∫–∞–∫ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫. –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –∑–Ω–∞—Ç—å –≥–¥–µ –∏—Å–∫–∞—Ç—å –¥–µ—Ç–∞–ª–∏.

---

**Happy Redis-ing! üöÄ‚ö°Ô∏è**

–°–ø–∞—Å–∏–±–æ –∑–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –∫—É—Ä—Å–∞! –ï—Å–ª–∏ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å - Redis community –≤—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤–æ –ø–æ–º–æ—á—å.

_–ö—É—Ä—Å –æ–±–Ω–æ–≤–ª—ë–Ω: –î–µ–∫–∞–±—Ä—å 2024_
_–í–µ—Ä—Å–∏—è Redis: 7.2_
_–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥—É–ª–∏: RedisJSON, RedisSearch, RedisTimeSeries, RedisBloom_

---

## Appendix: –ë—ã—Å—Ç—Ä—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã
```bash
# Health check one-liner
redis-cli PING && redis-cli INFO server | grep uptime

# Quick memory check
redis-cli INFO memory | grep -E "used_memory_human|mem_fragmentation_ratio"

# Find big keys
redis-cli --bigkeys --i 0.01

# Monitor real-time
redis-cli MONITOR | head -100

# Check replication
redis-cli INFO replication | grep -E "role|master_link_status|connected_slaves"

# Slow queries
redis-cli SLOWLOG GET 10

# Connected clients
redis-cli CLIENT LIST | wc -l

# Keys count
redis-cli DBSIZE

# Latency check
redis-cli --latency

# Export keys by pattern
redis-cli --scan --pattern 'user:*' | xargs -L 1 redis-cli DUMP

# Atomic increment with limit check (Lua)
redis-cli EVAL "local v = redis.call('GET',KEYS[1]) if not v or tonumber(v) < tonumber(ARGV[1]) then return redis.call('INCR',KEYS[1]) else return 0 end" 1 counter 100
```

**–í—Å—ë! –ö—É—Ä—Å –∑–∞–≤–µ—Ä—à—ë–Ω –Ω–∞ 100%. –£–¥–∞—á–∏ –≤ –∏–∑—É—á–µ–Ω–∏–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ Redis! **


# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è DevOps: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å-–æ—Å–≤–µ–∂–∏—Ç–µ–ª—å

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞)**: –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
2. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è**: –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
3. **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞)**: –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

**–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ Linux/Unix
- –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–µ—Ä—É –∏–ª–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ
- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞–Ω–∏–π)
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

---

## –ú–æ–¥—É–ª—å 1: –û—Å–Ω–æ–≤—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ß–µ—Ç—ã—Ä–µ –∑–æ–ª–æ—Ç—ã—Ö —Å–∏–≥–Ω–∞–ª–∞ (Four Golden Signals):**
```
1. Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã
2. Traffic (–¢—Ä–∞—Ñ–∏–∫)        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
3. Errors (–û—à–∏–±–∫–∏)         - –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
4. Saturation (–ù–∞—Å—ã—â–µ–Ω–∏–µ)  - –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU, –ø–∞–º—è—Ç—å, –¥–∏—Å–∫)
```

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
```
Counter   - –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å—ã, –æ—à–∏–±–∫–∏)
Gauge     - –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (CPU, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
Histogram - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (latency buckets)
Summary   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (percentiles)
```

**USE Method (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–∞
Saturation  - –°—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**RED Method (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
Errors   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
Duration - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application (APM)             ‚îÇ  - –ö–æ–¥, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Service/Container             ‚îÇ  - Docker, K8s
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Operating System              ‚îÇ  - CPU, RAM, Disk
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Infrastructure                ‚îÇ  - Network, Hardware
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Linux:**
```bash
# CPU
top, htop
mpstat -P ALL 1

# Memory
free -m
vmstat 1

# Disk I/O
iostat -x 1
iotop

# Network
iftop
nethogs
ss -s

# Process
ps aux --sort=-%mem | head
ps aux --sort=-%cpu | head
```

**–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**
```
- Request rate (req/s)
- Error rate (%)
- Response time (ms) - p50, p95, p99
- Active connections
- Queue depth
- Database query time
- Cache hit ratio
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –±–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ –∏ –∑–∞–ø—É—Å—Ç–∏ Node Exporter** (–¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Ö–æ—Å—Ç–∞):
```bash
# –ß–µ—Ä–µ–∑ Docker
docker run -d \
  --name node-exporter \
  --net="host" \
  --pid="host" \
  -v "/:/host:ro,rslave" \
  prom/node-exporter:latest \
  --path.rootfs=/host

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:9100/metrics | head -20
```

2. **–ò–∑—É—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**:
```bash
# CPU
curl -s http://localhost:9100/metrics | grep node_cpu_seconds_total

# Memory
curl -s http://localhost:9100/metrics | grep node_memory

# Disk
curl -s http://localhost:9100/metrics | grep node_disk

# Network
curl -s http://localhost:9100/metrics | grep node_network
```

3. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π bash —Å–∫—Ä–∏–ø—Ç** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (`monitor.sh`):
```bash
#!/bin/bash

echo "=== System Monitoring Report ==="
echo "Date: $(date)"
echo ""

# CPU Usage
echo "CPU Usage:"
top -bn1 | grep "Cpu(s)" | awk '{print "  User: " $2 ", System: " $4 ", Idle: " $8}'

# Memory Usage
echo ""
echo "Memory Usage:"
free -h | awk 'NR==2{printf "  Total: %s, Used: %s (%.2f%%)\n", $2, $3, $3*100/$2}'

# Disk Usage
echo ""
echo "Disk Usage:"
df -h / | awk 'NR==2{printf "  Total: %s, Used: %s (%s)\n", $2, $3, $5}'

# Load Average
echo ""
echo "Load Average:"
uptime | awk -F'load average:' '{print "  " $2}'

# Top 5 processes by CPU
echo ""
echo "Top 5 processes by CPU:"
ps aux --sort=-%cpu | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $3}'

# Top 5 processes by Memory
echo ""
echo "Top 5 processes by Memory:"
ps aux --sort=-%mem | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $4}'
```

4. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç:
```bash
chmod +x monitor.sh
./monitor.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π cAdvisor** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:
```bash
docker run -d \
  --name=cadvisor \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  gcr.io/cadvisor/cadvisor:latest

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:8080
```

**–°–æ–∑–¥–∞–π —Å–≤–æ–π custom exporter** –Ω–∞ Python:
```python
# custom_exporter.py
from prometheus_client import start_http_server, Gauge, Counter
import time
import random

# –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
request_gauge = Gauge('app_requests_in_progress', 'Number of requests in progress')
request_counter = Counter('app_requests_total', 'Total number of requests')
error_counter = Counter('app_errors_total', 'Total number of errors')

def process_request():
    """–°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
    request_gauge.inc()
    request_counter.inc()
    
    # –°–ª—É—á–∞–π–Ω–∞—è –æ—à–∏–±–∫–∞
    if random.random() < 0.1:
        error_counter.inc()
    
    time.sleep(random.uniform(0.1, 0.5))
    request_gauge.dec()

if __name__ == '__main__':
    start_http_server(8000)
    print("Exporter started on port 8000")
    
    while True:
        process_request()
        time.sleep(random.uniform(0.5, 2))
```

---

## –ú–æ–¥—É–ª—å 2: Prometheus - —Å–±–æ—Ä –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Prometheus:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Targets   ‚îÇ ‚Üê HTTP Pull (scrape)
‚îÇ  (Metrics)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Prom-  ‚îÇ
   ‚îÇ etheus ‚îÇ ‚Üê Time Series DB (TSDB)
   ‚îÇ Server ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Alert- ‚îÇ
   ‚îÇ manager‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Prometheus config structure:**
```yaml
global:
  scrape_interval: 15s      # –ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ–±–∏—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
  evaluation_interval: 15s  # –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**PromQL –æ—Å–Ω–æ–≤—ã:**
```promql
# Instant vector - —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
node_cpu_seconds_total

# Range vector - –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø–µ—Ä–∏–æ–¥
node_cpu_seconds_total[5m]

# –§–∏–ª—å—Ç—Ä—ã
node_cpu_seconds_total{mode="idle"}
node_cpu_seconds_total{mode!="idle"}
node_cpu_seconds_total{mode=~"user|system"}

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
sum(node_cpu_seconds_total)
avg(node_cpu_seconds_total)
max(node_cpu_seconds_total)
min(node_cpu_seconds_total)
count(node_cpu_seconds_total)

# –ü–æ label
sum(node_cpu_seconds_total) by (mode)
sum(node_cpu_seconds_total) by (cpu)

# –§—É–Ω–∫—Ü–∏–∏
rate(node_cpu_seconds_total[5m])           # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
irate(node_cpu_seconds_total[5m])          # –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
increase(node_cpu_seconds_total[5m])       # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
delta(node_cpu_seconds_total[5m])          # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
```

**–†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**
```promql
# CPU utilization
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory usage %
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk usage %
100 - ((node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100)

# Network traffic
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])

# HTTP request rate
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Latency percentiles (–¥–ª—è histogram)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**Metric types –≤ –¥–µ—Ç–∞–ª—è—Ö:**
```promql
# Counter - —Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ç–µ—Ç
http_requests_total
# –ò—Å–ø–æ–ª—å–∑—É–π rate() –∏–ª–∏ increase()
rate(http_requests_total[5m])

# Gauge - –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ –∏ –ø–∞–¥–∞—Ç—å
node_memory_MemAvailable_bytes
# –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
avg(node_memory_MemAvailable_bytes)

# Histogram - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
http_request_duration_seconds_bucket
http_request_duration_seconds_sum
http_request_duration_seconds_count
# –ò—Å–ø–æ–ª—å–∑—É–π histogram_quantile()
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Summary - –ø—Ä–µ–¥—Ä–∞—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
http_request_duration_seconds{quantile="0.95"}
```

**Recording rules** (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):
```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - record: job:node_cpu_utilization:avg
      expr: 100 - (avg by (job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

**Alerting rules:**
```yaml
groups:
  - name: alerts
    rules:
    - alert: HighCPUUsage
      expr: job:node_cpu_utilization:avg > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value }}%"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Prometheus:

1. **–°–æ–∑–¥–∞–π docker-compose.yml**:
```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    restart: unless-stopped

volumes:
  prometheus-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∞–ª–µ—Ä—Ç–æ–≤
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

3. **–°–æ–∑–¥–∞–π alerts.yml**:
```yaml
groups:
  - name: system_alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% (current value: {{ $value }}%)"

    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 90% (current value: {{ $value }}%)"

    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space"
        description: "Disk usage is above 85% (current value: {{ $value }}%)"

    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Instance {{ $labels.instance }} down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"
```

4. **–ó–∞–ø—É—Å—Ç–∏ stack**:
```bash
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker-compose ps
curl http://localhost:9090/api/v1/targets
```

5. **–û—Ç–∫—Ä–æ–π Prometheus UI** –∏ –ø–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
```
–ü–µ—Ä–µ–π–¥–∏: http://localhost:9090

–ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
- node_cpu_seconds_total
- rate(node_cpu_seconds_total[5m])
- 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
- node_memory_MemAvailable_bytes / 1024 / 1024 / 1024
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π Service Discovery** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π:

**File-based SD** (`file_sd.json`):
```json
[
  {
    "targets": ["node-exporter:9100"],
    "labels": {
      "job": "node",
      "env": "production"
    }
  },
  {
    "targets": ["cadvisor:8080"],
    "labels": {
      "job": "containers",
      "env": "production"
    }
  }
]
```

–î–æ–±–∞–≤—å –≤ `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'dynamic-targets'
    file_sd_configs:
      - files:
        - '/etc/prometheus/file_sd.json'
        refresh_interval: 30s
```

**–ù–∞—Å—Ç—Ä–æ–π Pushgateway** –¥–ª—è –º–µ—Ç—Ä–∏–∫ batch jobs:
```bash
docker run -d \
  --name pushgateway \
  -p 9091:9091 \
  prom/pushgateway

# Push –º–µ—Ç—Ä–∏–∫—É
echo "backup_duration_seconds 125.5" | curl --data-binary @- http://localhost:9091/metrics/job/backup/instance/db1

# –î–æ–±–∞–≤—å –≤ prometheus.yml
scrape_configs:
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['pushgateway:9091']
    honor_labels: true
```

**Recording rules –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**:
```yaml
# recording_rules.yml
groups:
  - name: performance_rules
    interval: 30s
    rules:
    # CPU utilization per instance
    - record: instance:node_cpu_utilization:rate5m
      expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    
    # Memory utilization per instance
    - record: instance:node_memory_utilization:ratio
      expr: 1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
    
    # Request rate per job
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
```

---

## –ú–æ–¥—É–ª—å 3: Grafana - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Grafana:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Grafana  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Users    ‚îÇ
‚îÇ Sources  ‚îÇ      ‚îÇ Server   ‚îÇ      ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ                    ‚îÇ
   ‚îÇ                    ‚îÇ
   ‚ñº                    ‚ñº
Prometheus       Dashboards
InfluxDB         Alerts
Elasticsearch    Users
Loki             Teams
```

**–¢–∏–ø—ã –ø–∞–Ω–µ–ª–µ–π:**
```
Graph        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
Stat         - –û–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
Gauge        - –®–∫–∞–ª–∞
Bar Gauge    - –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å–∫–∏
Table        - –¢–∞–±–ª–∏—Ü–∞
Heatmap      - –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
Logs         - –õ–æ–≥–∏
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥–∞:**
```
Query      - –ò–∑ –¥–∞–Ω–Ω—ã—Ö (label_values(metric, label))
Custom     - –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π
Constant   - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
Interval   - –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
Data source - –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```

**–ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Grafana:**
```
$__interval        - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
$__rate_interval   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è rate()
$timeFilter        - –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
$__from / $__to    - –ù–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞

# –ü—Ä–∏–º–µ—Ä —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
rate(http_requests_total{job="$job"}[$__rate_interval])
```

**Templating examples:**
```promql
# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è instance
label_values(node_cpu_seconds_total, instance)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è job
label_values(up, job)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è mountpoint
label_values(node_filesystem_size_bytes, mountpoint)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
node_filesystem_avail_bytes{instance="$instance", mountpoint="$mountpoint"}
```

**Alert channels:**
```
Email
Slack
PagerDuty
Webhook
Telegram
Discord
Teams
OpsGenie
```

**Dashboard best practices:**
```
1. –ò—Å–ø–æ–ª—å–∑—É–π Row –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞–Ω–µ–ª–µ–π
2. –î–æ–±–∞–≤–ª—è–π –æ–ø–∏—Å–∞–Ω–∏—è –∫ –ø–∞–Ω–µ–ª—è–º
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
4. –£–∫–∞–∑—ã–≤–∞–π –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ü–≤–µ—Ç–æ–≤—ã–µ –ø–æ—Ä–æ–≥–∏
6. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ runbook'–∏
7. –ì—Ä—É–ø–ø–∏—Ä—É–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
8. –ò—Å–ø–æ–ª—å–∑—É–π consistent naming
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard:

1. **–î–æ–±–∞–≤—å Grafana –≤ docker-compose.yml**:
```yaml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** (`grafana/provisioning/datasources/prometheus.yml`):
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

3. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è dashboard** (`grafana/provisioning/dashboards/dashboard.yml`):
```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

4. **–ó–∞–ø—É—Å—Ç–∏ Grafana**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards

docker-compose up -d grafana

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:3000
# Login: admin
# Password: admin
```

5. **–°–æ–∑–¥–∞–π System Monitoring Dashboard** –≤—Ä—É—á–Ω—É—é:

**Panel 1: CPU Usage**
- Visualization: Time series
- Query: `100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
- Legend: CPU Usage %
- Unit: Percent (0-100)
- Threshold: Yellow at 70, Red at 90

**Panel 2: Memory Usage**
- Visualization: Time series
- Query: `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100`
- Legend: Memory Usage %
- Unit: Percent (0-100)

**Panel 3: Disk Usage**
- Visualization: Gauge
- Query: `100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100)`
- Unit: Percent (0-100)
- Threshold: Green 0-70, Yellow 70-85, Red 85-100

**Panel 4: Network Traffic**
- Visualization: Time series
- Query A: `rate(node_network_receive_bytes_total[5m]) / 1024 / 1024`
- Query B: `rate(node_network_transmit_bytes_total[5m]) / 1024 / 1024`
- Unit: MB/s

**Panel 5: Top Processes by CPU**
- Visualization: Table
- Query: `topk(5, irate(process_cpu_seconds_total[5m]))`

6. **–°–æ–∑–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è dashboard**:
- Variable: instance
  - Type: Query
  - Query: `label_values(node_cpu_seconds_total, instance)`
  
–ò–∑–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
```promql
100 - (avg(irate(node_cpu_seconds_total{instance="$instance", mode="idle"}[5m])) * 100)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π JSON dashboard —á–µ—Ä–µ–∑ provisioning** (`grafana/provisioning/dashboards/system-overview.json`):
```json
{
  "dashboard": {
    "title": "System Overview",
    "tags": ["system", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "timeseries",
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{instance=\"$instance\",mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 70, "color": "yellow"},
                {"value": 90, "color": "red"}
              ]
            }
          }
        }
      }
    ],
    "templating": {
      "list": [
        {
          "name": "instance",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(node_cpu_seconds_total, instance)",
          "refresh": 1
        }
      ]
    }
  }
}
```

**–ù–∞—Å—Ç—Ä–æ–π Alerting –≤ Grafana**:
1. Configuration ‚Üí Alerting ‚Üí Contact points
2. –°–æ–∑–¥–∞–π Email contact point
3. –°–æ–∑–¥–∞–π Alert rule:
   - Name: High CPU Alert
   - Query: `avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 < 20`
   - Condition: WHEN last() OF query(A) IS BELOW 20
   - For: 5m

**–£—Å—Ç–∞–Ω–æ–≤–∏ Grafana plugins**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ UI
Configuration ‚Üí Plugins ‚Üí Search

# –ü–æ–ª–µ–∑–Ω—ã–µ –ø–ª–∞–≥–∏–Ω—ã:
- Pie Chart
- Worldmap Panel
- Clock Panel
- Status Panel

# –ß–µ—Ä–µ–∑ CLI (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)
docker exec grafana grafana-cli plugins install grafana-piechart-panel
docker restart grafana
```

---
## –ú–æ–¥—É–ª—å 4: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–æ–≤ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–£—Ä–æ–≤–Ω–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

```
TRACE   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
DEBUG   - –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
INFO    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
WARN    - –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
ERROR   - –û—à–∏–±–∫–∏, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã
FATAL   - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç
```

**Structured logging (JSON):**

json

````json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "ERROR",
  "service": "api",
  "message": "Database connection failed",
  "error": "connection timeout",
  "user_id": "12345",
  "request_id": "abc-123",
  "duration_ms": 5000
}
```

**ELK Stack:**
```
Elasticsearch  - –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫
Logstash       - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
Kibana         - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Alternative: Loki Stack:**
```
Loki           - –•—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ (–∫–∞–∫ Prometheus –¥–ª—è –ª–æ–≥–æ–≤)
Promtail       - –ê–≥–µ–Ω—Ç —Å–±–æ—Ä–∞ (–∫–∞–∫ node-exporter)
Grafana        - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Log aggregation patterns:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Log     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Centralized  ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ Shipper ‚îÇ    ‚îÇ Log Storage  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
````

**–ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ª–æ–≥–æ–≤:**

bash

```bash
# journalctl (systemd)
journalctl -u nginx                  # –õ–æ–≥–∏ —Å–µ—Ä–≤–∏—Å–∞
journalctl -f                        # Follow –ª–æ–≥–∏
journalctl --since "1 hour ago"
journalctl -p err                    # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
journalctl --disk-usage              # –†–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤

# Docker logs
docker logs <container>
docker logs -f <container>
docker logs --tail 100 <container>
docker logs --since 1h <container>

# –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ª–æ–≥–∏ Linux
tail -f /var/log/syslog
tail -f /var/log/nginx/access.log
grep "ERROR" /var/log/application.log
zgrep "pattern" /var/log/old.log.gz  # –ü–æ–∏—Å–∫ –≤ —Å–∂–∞—Ç—ã—Ö –ª–æ–≥–∞—Ö

# –õ–æ–≥–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
tail -f /var/log/app.log | ts '%Y-%m-%d %H:%M:%S'

# –ú–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π tail
multitail /var/log/nginx/access.log /var/log/nginx/error.log

# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10  # Top 10 IP
grep "500" access.log | wc -l  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ 500 –æ—à–∏–±–æ–∫
```

**Log rotation:**

bash

```bash
# logrotate –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (/etc/logrotate.d/app)
/var/log/app/*.log {
    daily                # –†–æ—Ç–∞—Ü–∏—è –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
    rotate 7             # –•—Ä–∞–Ω–∏—Ç—å 7 –∞—Ä—Ö–∏–≤–æ–≤
    compress             # –°–∂–∏–º–∞—Ç—å —Å—Ç–∞—Ä—ã–µ
    delaycompress        # –ù–µ —Å–∂–∏–º–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π
    missingok            # –ù–µ –æ—à–∏–±–∞—Ç—å—Å—è –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç
    notifempty           # –ù–µ —Ä–æ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ
    create 0640 app app  # –°–æ–∑–¥–∞—Ç—å —Å –ø—Ä–∞–≤–∞–º–∏
    sharedscripts
    postrotate
        systemctl reload app > /dev/null
    endscript
}

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
logrotate -d /etc/logrotate.d/app    # Dry run
logrotate -f /etc/logrotate.d/app    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è
```

**–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö:**

**Python (structured logging):**

python

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "service": "my-api",
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_data)

logging.basicConfig(level=logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger()
logger.handlers = [handler]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info("User logged in", extra={"user_id": "123", "ip": "192.168.1.1"})
logger.error("Database error", extra={"query": "SELECT *", "duration_ms": 5000})
```

**Node.js (Winston):**

javascript

```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: { service: 'api-service' },
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info('User action', { user_id: '123', action: 'login' });
logger.error('Database error', { error: err.message, query: sql });
```

**Loki query patterns (LogQL):**

logql

```logql
# –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
{job="varlogs"}

# –§–∏–ª—å—Ç—Ä—ã
{job="varlogs"} |= "error"                    # –°–æ–¥–µ—Ä–∂–∏—Ç "error"
{job="varlogs"} != "debug"                    # –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "debug"
{job="varlogs"} |~ "error|ERROR"              # Regex
{job="varlogs"} !~ "info|INFO"                # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π regex

# JSON parsing
{job="varlogs"} | json | level="error"
{job="varlogs"} | json | response_time > 1000

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
rate({job="varlogs"}[5m])                     # –õ–æ–≥-–∑–∞–ø–∏—Å–µ–π –≤ —Å–µ–∫—É–Ω–¥—É
sum(rate({job="varlogs"}[5m])) by (level)     # –ü–æ —É—Ä–æ–≤–Ω—é
count_over_time({job="varlogs"}[1h])          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞ —á–∞—Å

# Pattern extraction
{job="varlogs"} | pattern `<_> level=<level> <_>`
{job="varlogs"} | regexp `status=(?P<status>\d+)`

# –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
sum(rate({job="api"} | json | status="500" [5m]))
```

**Elasticsearch query patterns:**

json

```json
// –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
GET /logs-*/_search
{
  "query": {
    "match": {
      "message": "error"
    }
  }
}

// –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
GET /logs-*/_search
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h",
        "lte": "now"
      }
    }
  }
}

// –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
GET /logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "ERROR" }},
        { "match": { "service": "api" }}
      ],
      "filter": [
        { "range": { "@timestamp": { "gte": "now-1h" }}}
      ]
    }
  },
  "aggs": {
    "errors_by_service": {
      "terms": { "field": "service.keyword" }
    }
  }
}
```

**Fluentd/Fluent Bit basics:**

conf

````conf
# Fluentd –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (fluent.conf)
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/td-agent/nginx-access.log.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

<filter nginx.access>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "nginx"
  </record>
</filter>

<match nginx.access>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name nginx-access
  type_name _doc
</match>

# Fluent Bit –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–±–æ–ª–µ–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
[INPUT]
    Name              tail
    Path              /var/log/containers/*.log
    Parser            docker
    Tag               kube.*

[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc:443
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token

[OUTPUT]
    Name              loki
    Match             *
    Host              loki
    Port              3100
```

**Log best practices:**
```
1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π structured logging (JSON)
2. –í–∫–ª—é—á–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: request_id, user_id, trace_id
3. –õ–æ–≥–∏—Ä—É–π –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ:
   - DEBUG: –¥–µ—Ç–∞–ª–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
   - INFO: –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
   - WARN: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
   - ERROR: –æ—à–∏–±–∫–∏ —Ç—Ä–µ–±—É—é—â–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
4. –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive data (–ø–∞—Ä–æ–ª–∏, —Ç–æ–∫–µ–Ω—ã, PII)
5. –ò—Å–ø–æ–ª—å–∑—É–π correlation IDs –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
6. –†–æ—Ç–∏—Ä—É–π –ª–æ–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
7. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑—É–π –ª–æ–≥–∏ —Å–æ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º
8. –ù–∞—Å—Ç—Ä–æ–π –∞–ª–µ—Ä—Ç—ã –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Loki:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Loki stack**:

yaml

```yaml
version: '3.8'

services:
  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail
    volumes:
      - ./promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    depends_on:
      - loki

  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-logs
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-logs-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - loki

  # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–µ –ª–æ–≥–∏
  log-generator:
    image: mingrammer/flog
    container_name: log-generator
    command: -f json -l -d 1 -s 1
    restart: unless-stopped

volumes:
  loki-data:
  grafana-logs-data:
```

2. **–°–æ–∑–¥–∞–π loki-config.yml**:

yaml

```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

# Retention (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤)
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π
```

3. **–°–æ–∑–¥–∞–π promtail-config.yml**:

yaml

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
    pipeline_stages:
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
      - labels:
          level:
          stream:

  # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log

  # Application logs (—Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON)
  - job_name: app-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            service: service
            message: message
            user_id: user_id
      - timestamp:
          source: timestamp
          format: RFC3339
      - labels:
          level:
          service:
```

4. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    isDefault: true
    editable: true
    jsonData:
      maxLines: 1000
```

5. **–ó–∞–ø—É—Å—Ç–∏ stack**:

bash

```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p logs/app

# –ó–∞–ø—É—Å—Ç–∏
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps
curl http://localhost:3100/ready

# –ü—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏
curl http://localhost:3100/loki/api/v1/label
```

6. **–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–æ–≤** (`generate_logs.py`):

python

````python
#!/usr/bin/env python3
import json
import random
import time
from datetime import datetime

levels = ['DEBUG', 'INFO', 'WARN', 'ERROR']
services = ['api', 'frontend', 'database', 'cache']
messages = {
    'DEBUG': ['Query executed', 'Cache hit', 'Function called'],
    'INFO': ['User logged in', 'Request processed', 'Task completed'],
    'WARN': ['Slow query detected', 'High memory usage', 'Rate limit approaching'],
    'ERROR': ['Database connection failed', 'Timeout occurred', '500 Internal Server Error']
}

def generate_log():
    level = random.choices(levels, weights=[10, 60, 20, 10])[0]
    service = random.choice(services)
    message = random.choice(messages[level])
    
    log_entry = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "level": level,
        "service": service,
        "message": message,
        "request_id": f"req-{random.randint(1000, 9999)}",
        "user_id": f"user-{random.randint(1, 100)}",
        "duration_ms": random.randint(10, 5000) if level in ['WARN', 'ERROR'] else random.randint(10, 500)
    }
    
    return json.dumps(log_entry)

if __name__ == "__main__":
    print("Starting log generation...")
    while True:
        log = generate_log()
        print(log)
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        with open('/var/log/app/application.log', 'a') as f:
            f.write(log + '\n')
        time.sleep(random.uniform(0.1, 2))
```

7. **–û—Ç–∫—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard**:
```
URL: http://localhost:3001
Login: admin
Password: admin

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–∞–Ω–µ–ª–µ–π:

# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é
sum(rate({job="docker"}[1m])) by (level)

# –õ–æ–≥–∏ —Å –æ—à–∏–±–∫–∞–º–∏
{job="docker"} |= "ERROR"

# Top services –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ª–æ–≥–æ–≤
topk(5, sum(rate({job="docker"}[5m])) by (container))

# –õ–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
{job="docker", container="log-generator"}

# –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–µ—Å–ª–∏ duration > 1000ms)
{job="docker"} | json | duration_ms > 1000
````

8. **–ü—Ä–æ–≤–µ—Ä—å —Ä–∞–±–æ—Ç—É**:

bash

```bash
# –õ–æ–≥–∏ –≤ Loki
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="docker"}' | jq

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query=count_over_time({job="docker"}[1h])' | jq

# –ú–µ—Ç—Ä–∏–∫–∏ Promtail
curl http://localhost:9080/metrics
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π ELK Stack –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è**:

`docker-compose-elk.yml`:

yaml

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.3
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.3
    container_name: logstash
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"
      - "9600:9600"
    environment:
      - "LS_JAVA_OPTS=-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.3
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.3
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: filebeat -e -strict.perms=false
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  elasticsearch-data:
```

`logstash.conf`:

conf

```conf
input {
  beats {
    port => 5000
  }
}

filter {
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  mutate {
    remove_field => ["message"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

`filebeat.yml`:

yaml

```yaml
filebeat.inputs:
  - type: container
    paths:
      - '/var/lib/docker/containers/*/*.log'
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"

output.logstash:
  hosts: ["logstash:5000"]

logging.level: info
```

**2. –°–æ–∑–¥–∞–π log alerting rules**:

–î–ª—è Loki (—á–µ—Ä–µ–∑ Grafana Alerting):

yaml

```yaml
# Alert: High Error Rate
groups:
  - name: log_alerts
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      - alert: ServiceDown
        expr: |
          absent(rate({job="docker", container="api"}[5m]))
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.container }} is down"
```

**3. –ù–∞—Å—Ç—Ä–æ–π log parsing –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤**:

Nginx access log parsing –≤ Promtail:

yaml

````yaml
- job_name: nginx
  static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/access.log
  pipeline_stages:
    - regex:
        expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[^ ]*) \[(?P<time_local>.*)\] "(?P<method>[^ ]*) (?P<request>[^ ]*) (?P<protocol>[^ ]*)" (?P<status>[\d]+) (?P<body_bytes_sent>[\d]+) "(?P<http_referer>[^"]*)" "(?P<http_user_agent>[^"]*)"'
    - labels:
        method:
        status:
    - timestamp:
        source: time_local
        format: 02/Jan/2006:15:04:05 -0700
```

**4. –°–æ–∑–¥–∞–π log analysis dashboard**:

Grafana panels –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤:
```
Panel 1: Log volume over time
Query: sum(rate({job="docker"}[1m])) by (level)
Visualization: Time series

Panel 2: Top error messages
Query: topk(10, sum(rate({job="docker"} |= "ERROR" [5m])) by (message))
Visualization: Bar chart

Panel 3: Logs table
Query: {job="docker"}
Visualization: Logs

Panel 4: Response time distribution
Query: quantile_over_time(0.95, {job="docker"} | json | unwrap duration_ms [5m])
Visualization: Gauge

Panel 5: Service health
Query: count(rate({job="docker"}[1m])) by (container)
Visualization: Stat
````

**5. –ù–∞—Å—Ç—Ä–æ–π log sampling –¥–ª—è –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º**:

yaml

```yaml
# Promtail sampling configuration
scrape_configs:
  - job_name: high-volume-app
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      # –°–æ—Ö—Ä–∞–Ω—è–π —Ç–æ–ª—å–∫–æ ERROR –∏ WARN + sample INFO/DEBUG
      - match:
          selector: '{job="app"}'
          stages:
            - json:
                expressions:
                  level: level
            - drop:
                expression: "level == 'DEBUG' and __sample__ > 0.1"  # 10% DEBUG
            - drop:
                expression: "level == 'INFO' and __sample__ > 0.5"   # 50% INFO
```

**6. Log retention –∏ archiving**:

yaml

```yaml
# Loki retention config
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π

# Compactor –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
compactor:
  working_directory: /loki/compactor
  shared_store: filesystem
  compaction_interval: 10m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
```

**7. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Alertmanager**:

yaml

```yaml
# Loki ruler config –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤
ruler:
  storage:
    type: local
    local:
      directory: /loki/rules
  rule_path: /tmp/rules
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
```

Rules file (`/loki/rules/alerts.yml`):

yaml

````yaml
groups:
  - name: logs
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High error rate in {{ $labels.container }}"
          description: "Error rate: {{ $value }} errors/sec"
          dashboard: "http://grafana:3000/d/logs"
```

**8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Loki vs ELK**:
```
Loki –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π (–º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Prometheus/Grafana
‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è Kubernetes
‚úÖ –î–µ—à–µ–≤–ª–µ –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏

ELK –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –ú–æ—â–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
‚úÖ –ë–æ–≥–∞—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
‚úÖ Advanced analytics
‚úÖ –ë–æ–ª—å—à–µ –ø–ª–∞–≥–∏–Ω–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π
‚úÖ Mature ecosystem

–í—ã–±–æ—Ä:
- Loki: –¥–ª—è –º–µ—Ç—Ä–∏–∫-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, K8s
- ELK: –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤, compliance
````

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 4

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—é ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Loki + Promtail + Grafana ‚úÖ –ü–∏—Å–∞—Ç—å LogQL –∑–∞–ø—Ä–æ—Å—ã ‚úÖ –ü–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ª–æ–≥–æ–≤ ‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–æ–≤ ‚úÖ –£–ø—Ä–∞–≤–ª—è—Ç—å retention –∏ rotation ‚úÖ –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å Loki –∏ ELK —Å—Ç–µ–∫–∏


## –ú–æ–¥—É–ª—å 5: Alerting –∏ Notification - —É–º–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –±–µ–∑ alert fatigue (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–§–∏–ª–æ—Å–æ—Ñ–∏—è –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**

```
–•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç = Actionable + Urgent + Real Problem

‚ùå –ü–ª–æ—Ö–æ–π –∞–ª–µ—Ä—Ç: "CPU usage > 80%"
‚úÖ –•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç: "API response time > 1s for 5min, affecting users"

–ü—Ä–∞–≤–∏–ª–æ: –ï—Å–ª–∏ –∞–ª–µ—Ä—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å - —ç—Ç–æ –Ω–µ –∞–ª–µ—Ä—Ç, —ç—Ç–æ –º–µ—Ç—Ä–∏–∫–∞
```

**–£—Ä–æ–≤–Ω–∏ severity:**

```
CRITICAL (P1)  - –ü–æ–ª–Ω—ã–π outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö

WARNING (P2)   - –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞, —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è
                 –ü—Ä–∏–º–µ—Ä: –≤—ã—Å–æ–∫–∞—è latency, —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –º–µ—Å—Ç–æ

INFO (P3)      - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å—Ä–æ—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: deployment –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–ª–∞–Ω–æ–≤–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
```

**Alertmanager –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus  ‚îÇ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                ‚îú‚îÄ‚îÄ‚ñ∫ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ Alertmanager ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Receivers   ‚îÇ
‚îÇ    Loki     ‚îÇ‚îÄ‚î§    ‚îÇ              ‚îÇ     ‚îÇ (Slack/etc) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ - Grouping   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ    ‚îÇ - Inhibition ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ - Silencing  ‚îÇ
‚îÇ   Custom    ‚îÇ‚îÄ‚îò    ‚îÇ - Routing    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Alert states:**

```
Inactive  ‚îÄ‚îÄ‚ñ∫ Pending  ‚îÄ‚îÄ‚ñ∫ Firing  ‚îÄ‚îÄ‚ñ∫ Resolved
               (for)         ‚îÇ
                            ‚Üì
                         Silenced
```

**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**1. Grouping** - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –í–º–µ—Å—Ç–æ 100 –∞–ª–µ—Ä—Ç–æ–≤ –æ down –Ω–æ–¥–∞—Ö
# –û–¥–∏–Ω grouped –∞–ª–µ—Ä—Ç: "50 nodes are down in cluster-prod"
route:
  group_by: ['alertname', 'cluster']
  group_wait: 30s
  group_interval: 5m
```

**2. Inhibition** - –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down, –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ –∫–∞–∂–¥–æ–º —Å–µ—Ä–≤–∏—Å–µ –≤ –Ω–µ–º
inhibit_rules:
  - source_match:
      alertname: ClusterDown
    target_match:
      cluster: production
    equal: ['cluster']
```

**3. Silencing** - –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤:

bash

```bash
# –í–æ –≤—Ä–µ–º—è maintenance window
amtool silence add alertname=HighCPU --duration=2h --comment="Planned maintenance"
```

**4. Routing** - –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ –∫–æ–º–∞–Ω–¥–∞–º/–∫–∞–Ω–∞–ª–∞–º:

yaml

```yaml
route:
  routes:
    - match:
        team: backend
      receiver: backend-team
    - match:
        severity: critical
      receiver: pagerduty
```

**Prometheus alerting rules —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**

yaml

```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) 
        / 
        rate(http_requests_total[5m]) 
        > 0.05
      for: 5m
      labels:
        severity: warning
        team: backend
        service: api
      annotations:
        summary: "High error rate on {{ $labels.instance }}"
        description: "Error rate is {{ $value | humanizePercentage }}"
        dashboard: "https://grafana.com/d/api-dashboard"
        runbook: "https://wiki.com/runbooks/high-error-rate"
```

**Alert best practices:**

**1. –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞ (–≥–æ–≤–æ—Ä—è—â–µ–µ):**

yaml

```yaml
‚ùå alert: HighCPU
‚úÖ alert: InstanceHighCPUUsage

‚ùå alert: Error
‚úÖ alert: APIHighErrorRate5xx
```

**2. For clause (–∏–∑–±–µ–≥–∞–µ–º flapping):**

yaml

```yaml
# –ù–µ –∞–ª–µ—Ä—Ç–∏—Ç—å –Ω–∞ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ø–∞–π–∫–∏
for: 5m  # –ê–ª–µ—Ä—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ true 5 –º–∏–Ω—É—Ç –ø–æ–¥—Ä—è–¥
```

**3. –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç):**

yaml

```yaml
annotations:
  summary: "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"
  description: "{{ $labels.instance }} has {{ $value }}% CPU usage"
  dashboard: "–°—Å—ã–ª–∫–∞ –Ω–∞ dashboard"
  runbook: "–°—Å—ã–ª–∫–∞ –Ω–∞ runbook —Å —Ä–µ—à–µ–Ω–∏–µ–º"
  impact: "Users experiencing slow response times"
```

**4. Labels –¥–ª—è routing:**

yaml

```yaml
labels:
  severity: critical|warning|info
  team: backend|frontend|data
  service: api|web|worker
  environment: prod|staging|dev
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**

yaml

```yaml
# Instance down
- alert: InstanceDown
  expr: up == 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Instance {{ $labels.instance }} down"

# High CPU
- alert: HighCPUUsage
  expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
  for: 10m
  labels:
    severity: warning

# High Memory
- alert: HighMemoryUsage
  expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
  for: 5m
  labels:
    severity: warning

# Disk space low
- alert: DiskSpaceLow
  expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
  for: 5m
  labels:
    severity: warning

# High disk I/O
- alert: HighDiskIO
  expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
  for: 10m
  labels:
    severity: warning
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**

yaml

````yaml
# High error rate
- alert: HighErrorRate
  expr: |
    sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
    /
    sum(rate(http_requests_total[5m])) by (service)
    > 0.05
  for: 5m
  labels:
    severity: critical

# Slow response time
- alert: SlowResponseTime
  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
  for: 10m
  labels:
    severity: warning

# High request rate (DDoS?)
- alert: UnusuallyHighTraffic
  expr: sum(rate(http_requests_total[5m])) > 1000
  for: 5m
  labels:
    severity: warning

# Database connection pool exhausted
- alert: DatabaseConnectionPoolNearLimit
  expr: database_connections_active / database_connections_max > 0.9
  for: 5m
  labels:
    severity: warning

# Queue backed up
- alert: QueueBacklog
  expr: queue_depth > 1000
  for: 10m
  labels:
    severity: warning

# Certificate expiring soon
- alert: CertificateExpiringSoon
  expr: (ssl_certificate_expiry_timestamp - time()) / 86400 < 30
  for: 1h
  labels:
    severity: warning
```

**Alert fatigue - –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å:**
```
–ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∞–ª–µ—Ä—Ç–æ–≤ ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è ‚Üí –ø—Ä–æ–ø—É—â–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

–†–µ—à–µ–Ω–∏—è:
1. ‚úÖ –ê–ª–µ—Ä—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –ø—Ä–∏—á–∏–Ω—ã
   ‚ùå CPU high, Memory high, Disk full (–ø—Ä–∏—á–∏–Ω—ã)
   ‚úÖ Users can't login, API is slow (—Å–∏–º–ø—Ç–æ–º—ã)

2. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ threshold
   ‚ùå CPU > 50% (—Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ)
   ‚úÖ CPU > 80% for 10 minutes (—Ä–∞–∑—É–º–Ω–æ)

3. ‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –∞–ª–µ—Ä—Ç—ã
   ‚ùå 50 –∞–ª–µ—Ä—Ç–æ–≤ "pod X down"
   ‚úÖ 1 –∞–ª–µ—Ä—Ç "50 pods down in namespace Y"

4. ‚úÖ Inhibition rules –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤
   –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down ‚Üí –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ —Å–µ—Ä–≤–∏—Å–∞—Ö

5. ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫
   Non-critical –∞–ª–µ—Ä—Ç—ã —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è

6. ‚úÖ SLO-based alerting
   –ê–ª–µ—Ä—Ç–∏—Ç—å –∫–æ–≥–¥–∞ error budget –∏—Å—á–µ—Ä–ø—ã–≤–∞–µ—Ç—Å—è

7. ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π review –∏ cleanup
   –£–¥–∞–ª—è–π –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã
```

**Notification channels:**
```
–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å    –ö–∞–Ω–∞–ª           –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Critical       PagerDuty       Production outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
               OpsGenie        
               
Warning        Slack           –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ –Ω–µ —Å—Ä–æ—á–Ω–æ
               Teams           
               
Info           Email           FYI, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –æ—Ç—á–µ—Ç—ã
               Webhook         –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
               
–í—Å–µ —É—Ä–æ–≤–Ω–∏     Grafana         –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
````

**Alertmanager –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
# –°—Ç–∞—Ç—É—Å
amtool config show
amtool config routes
amtool alert query

# Silences
amtool silence add alertname=HighCPU --duration=2h --comment="Maintenance"
amtool silence query
amtool silence expire <silence-id>

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
amtool check-config alertmanager.yml

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
amtool alert add alertname=Test severity=warning

# API –∑–∞–ø—Ä–æ—Å—ã
curl -X GET http://localhost:9093/api/v2/alerts
curl -X GET http://localhost:9093/api/v2/silences
curl -X GET http://localhost:9093/api/v2/status
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:

1. **–î–æ–±–∞–≤—å Alertmanager –≤ docker-compose.yml**:

yaml

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  # Webhook receiver –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
  webhook-receiver:
    image: ghcr.io/tarampampam/webhook-tester:latest
    container_name: webhook-receiver
    ports:
      - "8080:8080"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_UNIFIED_ALERTING_ENABLED=true
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  prometheus-data:
  alertmanager-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º**:

yaml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'
    environment: 'dev'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'alertmanager'
    static_configs:
      - targets: ['alertmanager:9093']
```

3. **–°–æ–∑–¥–∞–π alerts.yml —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏**:

yaml

```yaml
groups:
  - name: infrastructure
    interval: 30s
    rules:
    # Instance down
    - alert: InstanceDown
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Instance {{ $labels.instance }} is down"
        description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes."
        dashboard: "http://localhost:3000/d/node-exporter"
        runbook: "https://runbooks.example.com/InstanceDown"

    # High CPU
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        dashboard: "http://localhost:3000/d/node-exporter"

    # High Memory
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}"

    # Disk space critical
    - alert: DiskSpaceCritical
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 90
      for: 5m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Critical disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        impact: "System may become unresponsive if disk fills up"

    # Disk space warning
    - alert: DiskSpaceWarning
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 80
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"

  - name: alertmanager
    interval: 30s
    rules:
    # Alertmanager down
    - alert: AlertmanagerDown
      expr: up{job="alertmanager"} == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Alertmanager is down"
        description: "Alertmanager has been down for more than 2 minutes. Alerts may not be delivered!"

    # Too many alerts firing
    - alert: TooManyAlerts
      expr: count(ALERTS{alertstate="firing"}) > 10
      for: 5m
      labels:
        severity: warning
        team: monitoring
      annotations:
        summary: "Too many alerts firing"
        description: "There are {{ $value }} alerts currently firing. This may indicate a systemic issue."

  - name: prometheus
    interval: 30s
    rules:
    # Prometheus target missing
    - alert: PrometheusTargetMissing
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus target missing"
        description: "A Prometheus target has disappeared. Instance: {{ $labels.instance }}"

    # Prometheus config reload failed
    - alert: PrometheusConfigReloadFailed
      expr: prometheus_config_last_reload_successful == 0
      for: 5m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus config reload failed"
        description: "Prometheus config reload has failed on {{ $labels.instance }}"

  - name: deadman
    interval: 30s
    rules:
    # Deadman switch - –∞–ª–µ—Ä—Ç –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω firing
    - alert: DeadMansSwitch
      expr: vector(1)
      labels:
        severity: info
        team: monitoring
      annotations:
        summary: "Monitoring system is alive"
        description: "This is a deadman switch. It should always be firing. If you don't receive this, monitoring is broken."
```

4. **–°–æ–∑–¥–∞–π alertmanager.yml —Å routing –∏ receivers**:

yaml

```yaml
global:
  resolve_timeout: 5m
  # Slack (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –∏ –Ω–∞—Å—Ç—Ä–æ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
  # slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Templates –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical –∞–ª–µ—Ä—Ç—ã ‚Üí webhook + log
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # Infrastructure team
    - match:
        team: infrastructure
      receiver: infrastructure-team
      group_wait: 30s
      repeat_interval: 4h

    # Monitoring team
    - match:
        team: monitoring
      receiver: monitoring-team

    # Deadman switch (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ alerting —Ä–∞–±–æ—Ç–∞–µ—Ç)
    - match:
        alertname: DeadMansSwitch
      receiver: deadman
      repeat_interval: 5m

# Inhibition rules (–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤)
inhibit_rules:
  # –ï—Å–ª–∏ instance down, –Ω–µ —Å–ª–∞—Ç—å –¥—Ä—É–≥–∏–µ –∞–ª–µ—Ä—Ç—ã —Å —Ç–æ–≥–æ –∂–µ instance
  - source_match:
      severity: critical
      alertname: InstanceDown
    target_match:
      severity: warning
    equal: ['instance']

  # –ï—Å–ª–∏ –¥–∏—Å–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω, –Ω–µ —Å–ª–∞—Ç—å warning –æ –¥–∏—Å–∫–µ
  - source_match:
      alertname: DiskSpaceCritical
    target_match:
      alertname: DiskSpaceWarning
    equal: ['instance', 'mountpoint']

# Receivers (–∫–∞–Ω–∞–ª—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π)
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/default'
        send_resolved: true

  - name: 'critical-alerts'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/critical'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#alerts-critical'
    #     title: 'üö® CRITICAL ALERT'
    #     text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    #     send_resolved: true

  - name: 'infrastructure-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/infrastructure'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#team-infrastructure'
    #     title: '‚ö†Ô∏è Infrastructure Alert'
    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'monitoring-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/monitoring'
        send_resolved: true

  - name: 'deadman'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/deadman'
        send_resolved: false
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      httpMethod: POST
      
  - name: Alertmanager
    type: alertmanager
    access: proxy
    url: http://alertmanager:9093
    editable: true
    jsonData:
      implementation: prometheus
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:

bash

```bash
# –ó–∞–ø—É—Å–∫
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Prometheus
curl http://localhost:9090/api/v1/rules

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Alertmanager
curl http://localhost:9093/api/v2/status

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤ –≤ Prometheus
curl http://localhost:9090/api/v1/alerts | jq

# –°–ø–∏—Å–æ–∫ firing –∞–ª–µ—Ä—Ç–æ–≤
curl http://localhost:9093/api/v2/alerts | jq '.[] | select(.status.state == "active")'
```

7. **–°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏** (`stress_test.sh`):

bash

```bash
#!/bin/bash

echo "Starting stress test to trigger alerts..."

# CPU stress (—Ç—Ä–∏–≥–≥–µ—Ä–Ω–µ—Ç HighCPUUsage)
echo "Generating CPU load..."
docker run --rm --name cpu-stress \
  polinux/stress \
  stress --cpu 4 --timeout 300s &

# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏—Å–∫–∞ (–¥–ª—è DiskSpaceWarning)
# –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω —Å —ç—Ç–∏–º –Ω–∞ –ø—Ä–æ–¥–µ!
# echo "Filling disk space..."
# dd if=/dev/zero of=/tmp/largefile bs=1M count=10000

echo "Stress test running. Check alerts in:"
echo "- Prometheus: http://localhost:9090/alerts"
echo "- Alertmanager: http://localhost:9093"
echo "- Webhook receiver: http://localhost:8080"
echo ""
echo "Wait 5-10 minutes for alerts to fire..."
```

8. **–ü—Ä–æ–≤–µ—Ä—å UI –∏ –∞–ª–µ—Ä—Ç—ã**:

bash

```bash
# Prometheus Alerts UI
open http://localhost:9090/alerts

# Alertmanager UI
open http://localhost:9093

# Grafana Alerting
open http://localhost:3000/alerting/list

# Webhook receiver (–ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã)
open http://localhost:8080
```

9. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π silencing**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ amtool
go install github.com/prometheus/alertmanager/cmd/amtool@latest
# –∏–ª–∏
brew install amtool

# –ù–∞—Å—Ç—Ä–æ–π amtool
cat > ~/.config/amtool/config.yml <<EOF
alertmanager.url: http://localhost:9093
EOF

# –°–æ–∑–¥–∞–π silence –Ω–∞ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∞
amtool silence add \
  alertname=HighCPUUsage \
  --duration=1h \
  --comment="Testing alert system" \
  --author="devops@example.com"

# –ü—Ä–æ–≤–µ—Ä—å silences
amtool silence query

# –£–¥–∞–ª–∏ silence
amtool silence expire <silence-id>
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ Slack**:

–û–±–Ω–æ–≤–∏ `alertmanager.yml`:

yaml

```yaml
global:
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

receivers:
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        username: 'Alertmanager'
        icon_emoji: ':fire:'
        title: 'üö® {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Severity:* {{ .Labels.severity }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          *Dashboard:* {{ .Annotations.dashboard }}
          {{ end }}
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

**2. Custom notification template**:

–°–æ–∑–¥–∞–π `templates/slack.tmpl`:

gotmpl

```gotmpl
{{ define "slack.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
{{ end }}

{{ define "slack.text" }}
{{ range .Alerts }}
*Alert:* {{ .Labels.alertname }} - `{{ .Labels.severity }}`
*Instance:* {{ .Labels.instance }}
*Summary:* {{ .Annotations.summary }}
*Description:* {{ .Annotations.description }}
{{ if .Annotations.runbook }}*Runbook:* {{ .Annotations.runbook }}{{ end }}
{{ if .Annotations.dashboard }}*Dashboard:* {{ .Annotations.dashboard }}{{ end }}
*Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
{{ if .EndsAt }}*Ended:* {{ .EndsAt.Format "2006-01-02 15:04:05 MST" }}{{ end }}
{{ end }}
{{ end }}

{{ define "slack.color" }}
{{ if eq .Status "firing" }}
  {{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}
{{ else }}
good
{{ end }}
{{ end }}
```

**3. PagerDuty –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** (–¥–ª—è critical alerts):

yaml

```yaml
receivers:
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'
        severity: '{{ .CommonLabels.severity }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          instance: '{{ .CommonLabels.instance }}'
        client: 'Alertmanager'
        client_url: 'http://alertmanager:9093'
        send_resolved: true
```

**4. Email notifications —Å HTML template**:

yaml

```yaml
receivers:
  - name: 'email-team'
    email_configs:
      - to: 'team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'your-app-password'
        headers:
          Subject: '{{ if eq .Status "firing" }}üö®{{ else }}‚úÖ{{ end }} [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        html: |
          <!DOCTYPE html>
          <html>
          <body>
            <h2 style="color: {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}">
              {{ if eq .Status "firing" }}üö® Firing Alerts{{ else }}‚úÖ Resolved{{ end }}
            </h2>
            {{ range .Alerts }}
            <div style="border-left: 4px solid {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}; padding: 10px; margin: 10px 0;">
              <h3>{{ .Labels.alertname }}</h3>
              <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
              <p><strong>Instance:</strong> {{ .Labels.instance }}</p>
              <p><strong>Description:</strong> {{ .Annotations.description }}</p>
              {{ if .Annotations.runbook }}
              <p><a href="{{ .Annotations.runbook }}">üìñ Runbook</a></p>
              {{ end }}
              {{ if .Annotations.dashboard }}
              <p><a href="{{ .Annotations.dashboard }}">üìä Dashboard</a></p>
              {{ end }}
            </div>
            {{ end }}
          </body>
          </html>
        send_resolved: true
```

**5. Webhook –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Jira/ServiceNow**:

–°–æ–∑–¥–∞–π `webhook_handler.py`:

python

```python
#!/usr/bin/env python3
from flask import Flask, request, jsonify
import json
import requests

app = Flask(__name__)

@app.route('/webhook/jira', methods=['POST'])
def jira_webhook():
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤"""
    data = request.json
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ firing –∏ critical
    if data['status'] == 'firing':
        for alert in data['alerts']:
            if alert['labels'].get('severity') == 'critical':
                create_jira_ticket(alert)
    
    return jsonify({'status': 'ok'}), 200

def create_jira_ticket(alert):
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket —á–µ—Ä–µ–∑ API"""
    jira_url = "https://your-jira.atlassian.net/rest/api/2/issue"
    
    ticket = {
        "fields": {
            "project": {"key": "OPS"},
            "summary": f"[ALERT] {alert['labels']['alertname']}",
            "description": alert['annotations']['description'],
            "issuetype": {"name": "Incident"}, "priority": {"name": "Critical"}, "labels": ["alert", "monitoring"] } }
```
# –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Jira
response = requests.post(
    jira_url,
    json=ticket,
    auth=('user@example.com', 'jira-api-token'),
    headers={'Content-Type': 'application/json'}
)

print(f"Jira ticket created: {response.json().get('key')}")
```

if **name** == '**main**': app.run(host='0.0.0.0', port=5000)

````

**6. SLO-based alerting** (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: slo_alerts
    interval: 30s
    rules:
    # Error budget burn rate
    - alert: ErrorBudgetBurnRateTooHigh
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > (1 - 0.999) * 10  # 10x SLO burn rate
      for: 5m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "Error budget burning too fast"
        description: "Current error rate is {{ $value | humanizePercentage }}. At this rate, monthly error budget will be exhausted in {{ with printf \"(1-0.999)*730/%f\" $value }}{{ . }}{{ end }} hours."
        dashboard: "http://localhost:3000/d/slo-dashboard"

    # SLO violation
    - alert: SLOViolation
      expr: |
        (
          1 - (
            sum(rate(http_requests_total{status!~"5.."}[30d]))
            /
            sum(rate(http_requests_total[30d]))
          )
        ) > 0.001  # –ù–∞—Ä—É—à–µ–Ω–∏–µ 99.9% SLO
      for: 1h
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "SLO violation detected"
        description: "30-day error rate is {{ $value | humanizePercentage }}, violating 99.9% SLO"
```

**7. Multi-window multi-burn-rate alerts** (Google SRE –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: multiwindow_multiburn_alerts
    interval: 30s
    rules:
    # Fast burn (–Ω—É–∂–Ω–æ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ)
    - alert: ErrorBudgetFastBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) > 14.4 * (1 - 0.999)  # 14.4x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > 14.4 * (1 - 0.999)
      for: 2m
      labels:
        severity: critical
        burn_rate: fast
      annotations:
        summary: "Fast error budget burn"
        description: "Error budget will be exhausted in 2 hours at current rate"

    # Slow burn (—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è)
    - alert: ErrorBudgetSlowBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[30m]))
          /
          sum(rate(http_requests_total[30m]))
        ) > 6 * (1 - 0.999)  # 6x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total[6h]))
        ) > 6 * (1 - 0.999)
      for: 15m
      labels:
        severity: warning
        burn_rate: slow
      annotations:
        summary: "Slow error budget burn"
        description: "Error budget will be exhausted in 5 days at current rate"
```

**8. Alert aggregation dashboard**:

–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–ª–µ—Ä—Ç–æ–≤ (`alert_analysis.py`):
```python
#!/usr/bin/env python3
import requests
from collections import Counter
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def get_alerts():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞–ª–µ—Ä—Ç—ã –∏–∑ Alertmanager"""
    response = requests.get(f"{ALERTMANAGER_URL}/api/v2/alerts")
    return response.json()

def analyze_alerts():
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–ª–µ—Ä—Ç–æ–≤"""
    alerts = get_alerts()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_alerts = len(alerts)
    firing_alerts = [a for a in alerts if a['status']['state'] == 'active']
    
    # –ü–æ severity
    severity_counter = Counter(
        alert['labels'].get('severity', 'unknown') 
        for alert in firing_alerts
    )
    
    # –ü–æ team
    team_counter = Counter(
        alert['labels'].get('team', 'unknown') 
        for alert in firing_alerts
    )
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∞–ª–µ—Ä—Ç—ã
    alert_counter = Counter(
        alert['labels']['alertname'] 
        for alert in firing_alerts
    )
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print("=" * 60)
    print("ALERT ANALYSIS REPORT")
    print("=" * 60)
    print(f"Total alerts: {total_alerts}")
    print(f"Firing alerts: {len(firing_alerts)}")
    print()
    
    print("By Severity:")
    for severity, count in severity_counter.most_common():
        print(f"  {severity}: {count}")
    print()
    
    print("By Team:")
    for team, count in team_counter.most_common():
        print(f"  {team}: {count}")
    print()
    
    print("Top 5 Most Frequent Alerts:")
    for alertname, count in alert_counter.most_common(5):
        print(f"  {alertname}: {count}")
    print("=" * 60)

if __name__ == "__main__":
    analyze_alerts()
```

**9. Alert testing framework**:

–°–æ–∑–¥–∞–π `alert_test.py`:
```python
#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
—á—Ç–æ –∞–ª–µ—Ä—Ç—ã —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç
"""
import requests
import time
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

def test_high_cpu_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ HighCPUUsage"""
    print("Testing HighCPUUsage alert...")
    
    registry = CollectorRegistry()
    cpu_gauge = Gauge('node_cpu_seconds_total', 
                      'CPU time', 
                      ['mode', 'instance'], 
                      registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤—ã—Å–æ–∫—É—é CPU –Ω–∞–≥—Ä—É–∑–∫—É
    cpu_gauge.labels(mode='idle', instance='test-instance').set(0.1)
    cpu_gauge.labels(mode='user', instance='test-instance').set(0.8)
    
    # Push –≤ Pushgateway
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Wait 5 minutes and check alerts...")
    print("http://localhost:9090/alerts")

def test_disk_space_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ DiskSpaceCritical"""
    print("Testing DiskSpaceCritical alert...")
    
    registry = CollectorRegistry()
    disk_total = Gauge('node_filesystem_size_bytes',
                       'Filesystem size',
                       ['mountpoint', 'instance'],
                       registry=registry)
    disk_avail = Gauge('node_filesystem_avail_bytes',
                       'Available space',
                       ['mountpoint', 'instance'],
                       registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º 95% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫–∞
    disk_total.labels(mountpoint='/', instance='test-instance').set(100e9)  # 100GB
    disk_avail.labels(mountpoint='/', instance='test-instance').set(5e9)    # 5GB
    
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Check alerts...")

if __name__ == "__main__":
    print("Starting alert tests...")
    test_high_cpu_alert()
    time.sleep(2)
    test_disk_space_alert()
    print("\nTests completed. Monitor alerts for next 10 minutes.")
```

**10. Alert maintenance calendar integration**:
```python
#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ silences –≤–æ –≤—Ä–µ–º—è maintenance windows
"""
import requests
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def create_maintenance_silence(service, duration_hours, comment):
    """–°–æ–∑–¥–∞—Ç—å silence –Ω–∞ –≤—Ä–µ–º—è maintenance"""
    
    now = datetime.utcnow()
    starts_at = now.isoformat() + "Z"
    ends_at = (now + timedelta(hours=duration_hours)).isoformat() + "Z"
    
    silence = {
        "matchers": [
            {
                "name": "service",
                "value": service,
                "isRegex": False
            }
        ],
        "startsAt": starts_at,
        "endsAt": ends_at,
        "createdBy": "maintenance-script",
        "comment": comment
    }
    
    response = requests.post(
        f"{ALERTMANAGER_URL}/api/v2/silences",
        json=silence
    )
    
    if response.status_code == 200:
        silence_id = response.json()['silenceID']
        print(f"‚úÖ Silence created: {silence_id}")
        print(f"   Service: {service}")
        print(f"   Duration: {duration_hours} hours")
        print(f"   Ends at: {ends_at}")
        return silence_id
    else:
        print(f"‚ùå Failed to create silence: {response.text}")
        return None

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä: Maintenance –Ω–∞ API —Å–µ—Ä–≤–∏—Å–µ –Ω–∞ 2 —á–∞—Å–∞
    create_maintenance_silence(
        service="api",
        duration_hours=2,
        comment="Planned database migration"
    )
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 5

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Alertmanager —Å routing –∏ inhibition
‚úÖ –ü–∏—Å–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ alert rules –≤ Prometheus
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (Slack, PagerDuty, Email)
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å grouping, inhibition –∏ silencing
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO-based alerts
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å alert fatigue —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å alerts
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å custom notification templates
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å maintenance windows

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**
1. Alert –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –Ω–∞ –ø—Ä–∏—á–∏–Ω—ã
2. –ö–∞–∂–¥—ã–π –∞–ª–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ severity —É—Ä–æ–≤–Ω–∏
4. –ì—Ä—É–ø–ø–∏—Ä—É–π –∏ –ø–æ–¥–∞–≤–ª—è–π –∑–∞–≤–∏—Å–∏–º—ã–µ –∞–ª–µ—Ä—Ç—ã
5. –†–µ–≥—É–ª—è—Ä–Ω–æ review –∏ cleanup –∞–ª–µ—Ä—Ç–æ–≤
6. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π runbooks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
7. –¢–µ—Å—Ç–∏—Ä—É–π –∞–ª–µ—Ä—Ç—ã —Ä–µ–≥—É–ª—è—Ä–Ω–æ


## –ú–æ–¥—É–ª—å 6: Distributed Tracing –∏ Application Performance Monitoring (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–¢—Ä–∏ —Å—Ç–æ–ª–ø–∞ Observability:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   METRICS   ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? (CPU, memory, requests/sec)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    LOGS     ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (—Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TRACES    ‚îÇ  - –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (–ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Distributed Tracing - –∑–∞—á–µ–º –Ω—É–∂–µ–Ω:**

```
–ü—Ä–æ–±–ª–µ–º–∞ –≤ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞—Ö:
User Request ‚Üí API Gateway ‚Üí Auth Service ‚Üí Order Service ‚Üí Payment Service ‚Üí Database
                                                                   ‚Üì
                                              ‚ùå SLOW RESPONSE (5 seconds)

–í–æ–ø—Ä–æ—Å: –ì–¥–µ bottleneck?
- API Gateway: 50ms
- Auth Service: 100ms
- Order Service: 200ms
- Payment Service: 4500ms ‚Üê –ù–ê–ô–î–ï–ù–û!
- Database: 150ms
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**Trace** - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É:

```
Trace ID: abc123
‚îú‚îÄ Span 1: API Gateway (50ms)
‚îú‚îÄ Span 2: Auth Service (100ms)
‚îú‚îÄ Span 3: Order Service (200ms)
‚îÇ  ‚îú‚îÄ Span 4: DB Query (50ms)
‚îÇ  ‚îî‚îÄ Span 5: Cache Check (10ms)
‚îî‚îÄ Span 6: Payment Service (4500ms)
   ‚îî‚îÄ Span 7: External API Call (4400ms) ‚Üê –ü—Ä–æ–±–ª–µ–º–∞!
```

**Span** - –µ–¥–∏–Ω–∏—Ü–∞ —Ä–∞–±–æ—Ç—ã –≤ —Å–∏—Å—Ç–µ–º–µ:

yaml

````yaml
Span:
  trace_id: "abc123"
  span_id: "span456"
  parent_span_id: "span789"
  operation_name: "POST /api/orders"
  start_time: "2025-01-15T10:00:00Z"
  duration: 200ms
  tags:
    http.method: "POST"
    http.status_code: 200
    service.name: "order-service"
    db.statement: "SELECT * FROM orders"
  logs:
    - timestamp: "2025-01-15T10:00:00.050Z"
      message: "Order validated"
```

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ç—Ä–µ–π—Å–∏–Ω–≥–∞:**
```
Jaeger       - CNCF –ø—Ä–æ–µ–∫—Ç, –æ—Ç Uber, Go
Zipkin       - –û—Ç Twitter, Java
Tempo        - –û—Ç Grafana Labs, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Loki
OpenTelemetry - –°—Ç–∞–Ω–¥–∞—Ä—Ç (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ OpenTracing + OpenCensus)
AWS X-Ray    - Managed —Å–µ—Ä–≤–∏—Å –æ—Ç AWS
Datadog APM  - Commercial
New Relic    - Commercial
```

**OpenTelemetry (OTel) - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Your Application             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  OpenTelemetry SDK         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Auto-instrumentation    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Manual instrumentation  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ OTel Collector ‚îÇ - –û–±—Ä–∞–±–æ—Ç–∫–∞, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
‚îÇ Jaeger ‚îÇ            ‚îÇ Tempo  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Sampling (–≤—ã–±–æ—Ä–∫–∞ —Ç—Ä–µ–π—Å–æ–≤):**
```
–ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–ª—å–∑—è —Ö—Ä–∞–Ω–∏—Ç—å 100% —Ç—Ä–µ–π—Å–æ–≤ (—Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ)

–í–∏–¥—ã sampling:
1. Head sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ)
   - Probabilistic: 10% –≤—Å–µ—Ö —Ç—Ä–µ–π—Å–æ–≤
   - Rate limiting: 100 —Ç—Ä–µ–π—Å–æ–≤/—Å–µ–∫
   
2. Tail sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ)
   - –í—Å–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
   - –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å –æ—à–∏–±–∫–∞–º–∏
   - 1% –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: Tail sampling + –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—à–∏–±–∫–∏
```

**APM (Application Performance Monitoring) - —á—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç:**
```
1. –¢—Ä–µ–π—Å–∏–Ω–≥ (Distributed Tracing)
2. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU, Memory profiling)
3. Error tracking
4. Real User Monitoring (RUM)
5. Database query analysis
6. External services monitoring
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ APM:**

**RED –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - Requests per second
Error    - Error rate (%)
Duration - Request latency (p50, p95, p99)
```

**USE –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - % –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
Saturation  - –î–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**Service metrics:**
```
Apdex Score = (Satisfied + Tolerating/2) / Total Requests
- Satisfied: < 1s
- Tolerating: 1-4s
- Frustrated: > 4s

Throughput = Requests per second
Error Rate = Errors / Total Requests
Availability = Uptime / Total Time
````

**Context Propagation (–∫–∞–∫ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è trace_id):**

**HTTP Headers:**

http

````http
# W3C Trace Context (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
traceparent: 00-abc123def456-span789-01
tracestate: vendor1=value1,vendor2=value2

# Jaeger
uber-trace-id: abc123:span456:0:1

# Zipkin
X-B3-TraceId: abc123
X-B3-SpanId: span456
X-B3-ParentSpanId: parent789
X-B3-Sampled: 1
```

**gRPC Metadata:**
```
grpc-trace-bin: <binary trace context>
````

**Instrumentation –ø–æ–¥—Ö–æ–¥—ã:**

**Auto-instrumentation** (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π):

python

```python
# Python —Å OpenTelemetry
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

FlaskInstrumentor().instrument()      # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ Flask
RequestsInstrumentor().instrument()   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ requests
```

**Manual instrumentation** (—Ä—É—á–Ω–æ–π):

python

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@app.route('/api/order')
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        span.set_attribute("order.id", order_id)
        span.set_attribute("user.id", user_id)
        
        # –í–∞—à –∫–æ–¥
        result = process_order(order_id)
        
        span.add_event("Order processed")
        return result
```

**–Ø–∑—ã–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**

**Python:**

python

```python
# OpenTelemetry
opentelemetry-api
opentelemetry-sdk
opentelemetry-instrumentation-flask
opentelemetry-instrumentation-django
opentelemetry-instrumentation-sqlalchemy
opentelemetry-exporter-jaeger
```

**Node.js:**

javascript

```javascript
// OpenTelemetry
@opentelemetry/api
@opentelemetry/sdk-node
@opentelemetry/auto-instrumentations-node
@opentelemetry/exporter-jaeger
```

**Go:**

go

```go
// OpenTelemetry
go.opentelemetry.io/otel
go.opentelemetry.io/otel/trace
go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp
```

**Java:**

java

````java
// OpenTelemetry Java Agent (auto-instrumentation)
java -javaagent:opentelemetry-javaagent.jar \
     -Dotel.service.name=my-service \
     -jar myapp.jar
```

**Jaeger UI - –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
```
1. Search traces:
   - –ü–æ service name
   - –ü–æ operation name
   - –ü–æ tags
   - –ü–æ duration
   - –ü–æ –≤—Ä–µ–º–µ–Ω–∏

2. Trace timeline:
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è spans
   - Waterfall view
   - Gantt chart

3. Dependencies graph:
   - –ö–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–µ—Ä–≤–∏—Å–æ–≤
   - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—ã–∑–æ–≤–æ–≤

4. Comparison:
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–π—Å–æ–≤
   - A/B testing —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

**Service Map (–∫–∞—Ä—Ç–∞ —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ HTTP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Gateway‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
   ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ      ‚îÇ        ‚îÇ
‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAuth ‚îÇ ‚îÇOrder‚îÇ ‚îÇUser  ‚îÇ
‚îÇSvc  ‚îÇ ‚îÇSvc  ‚îÇ ‚îÇSvc   ‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ‚îÇPayment  ‚îÇ
   ‚îÇ   ‚îÇSvc      ‚îÇ
   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ         ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
   ‚îÇ DB   ‚îÇ  ‚îÇCache ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Error tracking –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:**
```
–°–≤—è–∑—å —Ç—Ä–µ–π—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏:

Exception –≤ –∫–æ–¥–µ ‚Üí Trace ID ‚Üí –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞
                               + stack trace
                               + request params
                               + user context
```

**Database query analysis:**
```
–ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
1. N+1 queries
   - 1 –∑–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ + N –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–µ—Ç–∞–ª–µ–π
   
2. Missing indexes
   - Full table scan
   
3. Slow queries
   - –°–ª–æ–∂–Ω—ã–µ JOIN
   - –ë–æ–ª—å—à–∏–µ SELECT *
   
4. Connection pool exhaustion
   - –ù–µ –∑–∞–∫—Ä—ã—Ç—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
```

**–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU/Memory):**
```
Continuous Profiling:
- Flamegraph –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- –ö–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
- Memory allocations
- Goroutines/Threads

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- pprof (Go)
- py-spy (Python)
- async-profiler (Java)
- Pyroscope (unified)
````

**Real User Monitoring (RUM):**

javascript

````javascript
// Frontend —Ç—Ä–µ–π—Å–∏–Ω–≥
import { WebTracerProvider } from '@opentelemetry/sdk-trace-web';

const provider = new WebTracerProvider();
const tracer = provider.getTracer('frontend-app');

// Track page load
const span = tracer.startSpan('page_load');
span.setAttribute('page.url', window.location.href);

window.addEventListener('load', () => {
  span.end();
});

// Track user interactions
button.addEventListener('click', () => {
  const span = tracer.startSpan('button_click');
  span.setAttribute('button.id', button.id);
  // ... –¥–µ–π—Å—Ç–≤–∏–µ
  span.end();
});
```

**Best practices:**
```
1. ‚úÖ –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–¥–∞–≤–∞–π trace context –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
2. ‚úÖ –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—ã–µ attributes (user_id, order_id, etc)
3. ‚úÖ –õ–æ–≥–∏—Ä—É–π trace_id –≤–æ –≤—Å–µ—Ö –ª–æ–≥–∞—Ö
4. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞)
5. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling
6. ‚úÖ –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive –¥–∞–Ω–Ω—ã–µ –≤ spans
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π tail sampling –¥–ª—è –æ—à–∏–±–æ–∫
8. ‚úÖ –•—Ä–∞–Ω–∏ —Ç—Ä–µ–π—Å—ã –º–∏–Ω–∏–º—É–º 7 –¥–Ω–µ–π
9. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º
10. ‚úÖ –°–æ–∑–¥–∞–π runbook –¥–ª—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
````

**Semantic Conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞):**

yaml

```yaml
# HTTP
span.name: "GET /api/users"
http.method: "GET"
http.url: "https://api.example.com/users"
http.status_code: 200
http.route: "/api/users"

# Database
span.name: "SELECT users"
db.system: "postgresql"
db.operation: "SELECT"
db.statement: "SELECT * FROM users WHERE id = ?"
db.name: "production"

# RPC
span.name: "UserService.GetUser"
rpc.system: "grpc"
rpc.service: "UserService"
rpc.method: "GetUser"

# Messaging
span.name: "process_order"
messaging.system: "kafka"
messaging.destination: "orders"
messaging.operation: "process"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π distributed tracing —Å Jaeger:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Jaeger stack**:

yaml

```yaml
version: '3.8'

services:
  # Jaeger all-in-one (–¥–ª—è development)
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "5775:5775/udp"   # accept zipkin.thrift (deprecated)
      - "6831:6831/udp"   # accept jaeger.thrift compact
      - "6832:6832/udp"   # accept jaeger.thrift binary
      - "5778:5778"       # serve configs
      - "16686:16686"     # Jaeger UI
      - "14250:14250"     # model.proto
      - "14268:14268"     # jaeger.thrift
      - "14269:14269"     # Admin port: health, metrics
      - "4317:4317"       # OTLP gRPC
      - "4318:4318"       # OTLP HTTP
      - "9411:9411"       # Zipkin compatible
    restart: unless-stopped

  # OpenTelemetry Collector (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
    restart: unless-stopped
    depends_on:
      - jaeger

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Frontend
  frontend:
    build: ./demo-app/frontend
    container_name: frontend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=frontend
      - BACKEND_URL=http://backend:5000
    ports:
      - "8080:8080"
    depends_on:
      - otel-collector
      - backend
    restart: unless-stopped

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Backend
  backend:
    build: ./demo-app/backend
    container_name: backend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=backend
      - DATABASE_URL=postgresql://user:password@postgres:5432/demo
      - REDIS_URL=redis://redis:6379
    ports:
      - "5000:5000"
    depends_on:
      - postgres
      - redis
      - otel-collector
    restart: unless-stopped

  # PostgreSQL database
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=demo
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped

  # Redis cache
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  # Grafana –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-tracing
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - jaeger

volumes:
  postgres-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π otel-collector-config.yml**:

yaml

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # Prometheus metrics receiver
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # Batch processor –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
  batch:
    timeout: 10s
    send_batch_size: 1024

  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  # Tail sampling - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏
      - name: error-traces
        type: status_code
        status_code:
          status_codes: [ERROR]
      
      # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
      - name: slow-traces
        type: latency
        latency:
          threshold_ms: 1000
      
      # 10% –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
      - name: probabilistic-policy
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
  resource:
    attributes:
      - key: environment
        value: development
        action: insert

  # Attributes processor
  attributes:
    actions:
      - key: db.statement
        action: delete  # –£–¥–∞–ª—è–µ–º SQL –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

exporters:
  # Jaeger exporter
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true

  # Logging exporter (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
  logging:
    loglevel: info

  # Prometheus exporter –¥–ª—è –º–µ—Ç—Ä–∏–∫
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp]
      processors: [memory_limiter, tail_sampling, batch, resource, attributes]
      exporters: [jaeger, logging]
    
    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [prometheus, logging]
```

3. **–°–æ–∑–¥–∞–π demo-app/backend (Python Flask)**:

`demo-app/backend/Dockerfile`:

dockerfile

````dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

`demo-app/backend/requirements.txt`:
```
flask==3.0.0
psycopg2-binary==2.9.9
redis==5.0.1
requests==2.31.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-flask==0.42b0
opentelemetry-instrumentation-requests==0.42b0
opentelemetry-instrumentation-psycopg2==0.42b0
opentelemetry-instrumentation-redis==0.42b0
opentelemetry-exporter-otlp==1.21.0
````

`demo-app/backend/app.py`:

python

```python
from flask import Flask, jsonify, request
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.sdk.resources import Resource
import psycopg2
import redis
import time
import random
import os
import json

# =========================
# OpenTelemetry setup
# =========================

resource = Resource.create({
    "service.name": os.getenv("OTEL_SERVICE_NAME", "backend"),
    "service.version": "1.0.0",
    "deployment.environment": "development",
})

provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
        insecure=True,
    )
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

# =========================
# Flask app
# =========================

app = Flask(__name__)

FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()
Psycopg2Instrumentor().instrument()
RedisInstrumentor().instrument()

# =========================
# Connections
# =========================

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://user:password@localhost:5432/demo",
)
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")


def get_db_connection():
    return psycopg2.connect(DATABASE_URL)


def get_redis_connection():
    return redis.from_url(REDIS_URL)


# =========================
# DB init
# =========================

def init_db():
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS users (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS orders (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER REFERENCES users(id),
                    product VARCHAR(100),
                    amount DECIMAL(10,2),
                    status VARCHAR(20),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
        conn.commit()


# =========================
# Routes
# =========================

@app.route("/health")
def health():
    return jsonify({"status": "healthy"}), 200


@app.route("/api/users", methods=["GET"])
def get_users():
    with tracer.start_as_current_span("get_users") as span:
        time.sleep(random.uniform(0.01, 0.1))

        r = get_redis_connection()
        cached = r.get("users:all")

        if cached:
            span.set_attribute("cache.hit", True)
            return jsonify(json.loads(cached)), 200

        span.set_attribute("cache.hit", False)

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id, name, email FROM users")
                users = [
                    {"id": r[0], "name": r[1], "email": r[2]}
                    for r in cur.fetchall()
                ]

        r.setex("users:all", 60, json.dumps(users))
        return jsonify(users), 200


@app.route("/api/users/<int:user_id>", methods=["GET"])
def get_user(user_id):
    with tracer.start_as_current_span("get_user") as span:
        span.set_attribute("user.id", user_id)
        time.sleep(random.uniform(0.01, 0.05))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "SELECT id, name, email FROM users WHERE id = %s",
                    (user_id,),
                )
                row = cur.fetchone()

        if not row:
            span.set_attribute("http.status_code", 404)
            return jsonify({"error": "User not found"}), 404

        return jsonify({
            "id": row[0],
            "name": row[1],
            "email": row[2],
        }), 200


@app.route("/api/users", methods=["POST"])
def create_user():
    with tracer.start_as_current_span("create_user") as span:
        data = request.json or {}

        if not data.get("name") or not data.get("email"):
            span.set_attribute("error", True)
            return jsonify({"error": "Name and email required"}), 400

        time.sleep(random.uniform(0.05, 0.15))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO users (name, email) VALUES (%s, %s) RETURNING id",
                    (data["name"], data["email"]),
                )
                user_id = cur.fetchone()[0]
            conn.commit()

        get_redis_connection().delete("users:all")

        span.add_event("User created", {"user.id": user_id})

        return jsonify({
            "id": user_id,
            "name": data["name"],
            "email": data["email"],
        }), 201


@app.route("/api/orders", methods=["POST"])
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        data = request.json or {}

        user_id = data.get("user_id")
        product = data.get("product")
        amount = data.get("amount")

        if not all([user_id, product, amount]):
            span.set_attribute("error", True)
            return jsonify({"error": "Missing required fields"}), 400

        # Check user
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id FROM users WHERE id = %s", (user_id,))
                if not cur.fetchone():
                    return jsonify({"error": "User not found"}), 404

        # Payment simulation
        delay = random.uniform(0.1, 0.5)
        time.sleep(delay)

        # Save order
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO orders (user_id, product, amount, status)
                    VALUES (%s, %s, %s, %s)
                    RETURNING id
                """, (user_id, product, amount, "completed"))
                order_id = cur.fetchone()[0]
            conn.commit()

        return jsonify({
            "id": order_id,
            "user_id": user_id,
            "product": product,
            "amount": amount,
            "status": "completed",
        }), 201


@app.route("/api/orders/<int:order_id>", methods=["GET"])
def get_order(order_id):
    with tracer.start_as_current_span("get_order") as span:
        span.set_attribute("order.id", order_id)
        time.sleep(random.uniform(0.01, 0.05))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT id, user_id, product, amount, status
                    FROM orders WHERE id = %s
                """, (order_id,))
                row = cur.fetchone()

        if not row:
            return jsonify({"error": "Order not found"}), 404

        return jsonify({
            "id": row[0],
            "user_id": row[1],
            "product": row[2],
            "amount": float(row[3]),
            "status": row[4],
        }), 200


@app.route("/api/slow")
def slow_endpoint():
    with tracer.start_as_current_span("slow_endpoint") as span:
        delay = random.uniform(2, 5)
        span.set_attribute("delay.seconds", delay)
        time.sleep(delay)
        return jsonify({"delay": delay}), 200


@app.route("/api/error")
def error_endpoint():
    with tracer.start_as_current_span("error_endpoint") as span:
        try:
            raise Exception("Simulated error")
        except Exception as e:
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            return jsonify({"error": str(e)}), 500


# =========================
# Entry point
# =========================

if __name__ == "__main__":
    init_db()
    app.run(host="0.0.0.0", port=5000, debug=False)

```
4. **–°–æ–∑–¥–∞–π demo-app/frontend (–ø—Ä–æ—Å—Ç–æ–π HTML + JS)**:

`demo-app/frontend/Dockerfile`:
```dockerfile
FROM nginx:alpine

COPY index.html /usr/share/nginx/html/
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 8080
```

`demo-app/frontend/index.html`:
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tracing Demo App</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
        }
        .section {
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        button {
            background-color: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            margin: 5px;
        }
        button:hover {
            background-color: #0056b3;
        }
        .error {
            background-color: #dc3545;
        }
        .error:hover {
            background-color: #c82333;
        }
        .slow {
            background-color: #ffc107;
        }
        .slow:hover {
            background-color: #e0a800;
        }
        #output {
            margin-top: 20px;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 4px;
            min-height: 100px;
            white-space: pre-wrap;
            font-family: monospace;
        }
        input {
            padding: 8px;
            margin: 5px;
            border: 1px solid #ddd;
            border-radius: 4px;
            width: 200px;
        }
        .links {
            margin-top: 30px;
            padding: 20px;
            background-color: #e9ecef;
            border-radius: 4px;
        }
        .links a {
            display: block;
            margin: 10px 0;
            color: #007bff;
            text-decoration: none;
        }
        .links a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üîç Distributed Tracing Demo</h1>
        
        <div class="section">
            <h2>User Operations</h2>
            <button onclick="getUsers()">Get All Users</button>
            <button onclick="createUser()">Create Random User</button>
            <br>
            <input type="number" id="userId" placeholder="User ID">
            <button onclick="getUser()">Get User by ID</button>
        </div>
        
        <div class="section">
            <h2>Order Operations</h2>
            <input type="number" id="orderUserId" placeholder="User ID">
            <input type="text" id="product" placeholder="Product">
            <input type="number" id="amount" placeholder="Amount">
            <button onclick="createOrder()">Create Order</button>
            <br><br>
            <input type="number" id="orderId" placeholder="Order ID">
            <button onclick="getOrder()">Get Order by ID</button>
        </div>
        
        <div class="section">
            <h2>Test Scenarios</h2>
            <button class="slow" onclick="testSlow()">Test Slow Endpoint (2-5s)</button>
            <button class="error" onclick="testError()">Test Error Endpoint</button>
            <button onclick="stressTest()">Stress Test (10 requests)</button>
        </div>
        
        <div id="output">Response will appear here...</div>
        
        <div class="links">
            <h3>üìä Monitoring Links</h3>
            <a href="http://localhost:16686" target="_blank">üîç Jaeger UI - View Traces</a>
            <a href="http://localhost:3000" target="_blank">üìà Grafana - Metrics & Traces</a>
            <a href="http://localhost:5000/health" target="_blank">üíö Backend Health Check</a>
        </div>
    </div>

    <script>
        const API_URL = 'http://localhost:5000/api';
        const output = document.getElementById('output');

        function log(message, data = null) {
            const timestamp = new Date().toISOString();
            let logMessage = `[${timestamp}] ${message}`;
            if (data) {
                logMessage += '\n' + JSON.stringify(data, null, 2);
            }
            output.textContent = logMessage;
            console.log(message, data);
        }

        async function getUsers() {
            try {
                log('Fetching all users...');
                const response = await fetch(`${API_URL}/users`);
                const data = await response.json();
                log('‚úÖ Users retrieved:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function getUser() {
            const userId = document.getElementById('userId').value;
            if (!userId) {
                log('‚ùå Please enter a user ID');
                return;
            }
            
            try {
                log(`Fetching user ${userId}...`);
                const response = await fetch(`${API_URL}/users/${userId}`);
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ User retrieved:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function createUser() {
            const randomNum = Math.floor(Math.random() * 1000);
            const userData = {
                name: `User ${randomNum}`,
                email: `user${randomNum}@example.com`
            };
            
            try {
                log('Creating user...', userData);
                const response = await fetch(`${API_URL}/users`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(userData)
                });
                const data = await response.json();
                log('‚úÖ User created:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function createOrder() {
            const orderData = {
                user_id: parseInt(document.getElementById('orderUserId').value),
                product: document.getElementById('product').value || 'Product',
                amount: parseFloat(document.getElementById('amount').value) || 99.99
            };
            
            if (!orderData.user_id) {
                log('‚ùå Please enter a user ID');
                return;
            }
            
            try {
                log('Creating order...', orderData);
                const response = await fetch(`${API_URL}/orders`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(orderData)
                });
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ Order created:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function getOrder() {
            const orderId = document.getElementById('orderId').value;
            if (!orderId) {
                log('‚ùå Please enter an order ID');
                return;
            }
            
            try {
                log(`Fetching order ${orderId}...`);
                const response = await fetch(`${API_URL}/orders/${orderId}`);
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ Order retrieved:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function testSlow() {
            try {
                log('‚è≥ Testing slow endpoint (this will take 2-5 seconds)...');
                const start = Date.now();
                const response = await fetch(`${API_URL}/slow`);
                const data = await response.json();
                const duration = ((Date.now() - start) / 1000).toFixed(2);
                log(`‚úÖ Slow endpoint completed in ${duration}s:`, data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function testError() {
            try {
                log('üí• Testing error endpoint...');
                const response = await fetch(`${API_URL}/error`);
                const data = await response.json();
                log('‚ùå Expected error:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function stressTest() {
            log('üî• Starting stress test with 10 parallel requests...');
            const promises = [];
            
            for (let i = 0; i < 10; i++) {
                promises.push(fetch(`${API_URL}/users`));
            }
            
            try {
                const start = Date.now();
                await Promise.all(promises);
                const duration = ((Date.now() - start) / 1000).toFixed(2);
                log(`‚úÖ Stress test completed in ${duration}s (10 requests)`);
            } catch (error) {
                log('‚ùå Stress test failed:', error.message);
            }
        }

        // Initial message
        log('üëã Welcome! Click any button to start generating traces.');
    </script>
</body>
</html>
```

`demo-app/frontend/nginx.conf`:
```nginx
server {
    listen 8080;
    server_name localhost;
    
    location / {
        root /usr/share/nginx/html;
        index index.html;
    }
    
    # CORS –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
    location /api {
        if ($request_method = 'OPTIONS') {
            add_header 'Access-Control-Allow-Origin' '*';
            add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
            add_header 'Access-Control-Allow-Headers' 'Content-Type';
            return 204;
        }
    }
}
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:
```yaml
apiVersion: 1

datasources:
  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger:16686
    isDefault: true
    editable: true
    jsonData:
      tracesToLogsV2:
        datasourceUid: 'loki'
        spanStartTimeShift: '-1h'
        spanEndTimeShift: '1h'
        filterByTraceID: true
        filterBySpanID: false
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p demo-app/frontend demo-app/backend

# –ó–∞–ø—É—Å—Ç–∏ stack
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
docker-compose ps

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤
docker-compose logs -f backend

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Jaeger
curl http://localhost:16686

# –ü—Ä–æ–≤–µ—Ä–∫–∞ backend health
curl http://localhost:5000/health
```

7. **–û—Ç–∫—Ä–æ–π UI –∏ —Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# Frontend demo app
open http://localhost:8080

# Jaeger UI
open http://localhost:16686

# Grafana
open http://localhost:3000

# –°–æ–∑–¥–∞–π —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
curl -X POST http://localhost:5000/api/users \
  -H "Content-Type: application/json" \
  -d '{"name": "Test User", "email": "test@example.com"}'

curl -X POST http://localhost:5000/api/orders \
  -H "Content-Type: application/json" \
  -d '{"user_id": 1, "product": "Test Product", "amount": 99.99}'
```

8. **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–π—Å–æ–≤ –≤ Jaeger**:
	1. –û—Ç–∫—Ä–æ–π Jaeger UI: [http://localhost:16686](http://localhost:16686)
	2. Search traces:
	    - Service: backend
	    - Operation: create_order
	    - Min Duration: 1s (–¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö)
	    - Tags: error=true (–¥–ª—è –æ—à–∏–±–æ–∫)
	3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π:
	    - Timeline view - –≥–¥–µ –≤—Ä–µ–º—è —Ç—Ä–∞—Ç–∏—Ç—Å—è
	    - Span details - –∞—Ç—Ä–∏–±—É—Ç—ã, —Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏
	    - Service graph - –∫–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
	    - Trace comparison - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ã—Å—Ç—Ä—ã—Ö –∏ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Tempo (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Jaeger)**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  tempo:
    image: grafana/tempo:2.3.1
    container_name: tempo
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"   # Tempo UI
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    restart: unless-stopped

volumes:
  tempo-data:
```

`tempo.yaml`:
```yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

ingester:
  max_block_duration: 5m

compactor:
  compaction:
    block_retention: 168h  # 7 days

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/blocks
    wal:
      path: /tmp/tempo/wal

metrics_generator:
  registry:
    external_labels:
      source: tempo
  storage:
    path: /tmp/tempo/generator/wal
  traces_storage:
    path: /tmp/tempo/generator/traces
```

**2. –°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è load testing —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º**:

`load_test.py`:
```python
#!/usr/bin/env python3
"""
Load testing —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç—Ä–µ–π—Å–æ–≤
"""
import concurrent.futures
import requests
import time
import random
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource

# Setup tracing
resource = Resource.create({"service.name": "load-tester"})
provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint="http://localhost:4317", insecure=True)
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

API_URL = "http://localhost:5000/api"

def make_request(endpoint, method="GET", data=None):
    """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º"""
    with tracer.start_as_current_span(f"{method} {endpoint}") as span:
        span.set_attribute("http.method", method)
        span.set_attribute("http.url", f"{API_URL}{endpoint}")
        
        try:
            if method == "GET":
                response = requests.get(f"{API_URL}{endpoint}")
            else:
                response = requests.post(
                    f"{API_URL}{endpoint}",
                    json=data,
                    headers={"Content-Type": "application/json"}
                )
            
            span.set_attribute("http.status_code", response.status_code)
            
            if response.status_code >= 400:
                span.set_attribute("error", True)
                
            return response
            
        except Exception as e:
            span.record_exception(e)
            span.set_attribute("error", True)
            raise

def user_flow():
    """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–π user flow"""
    with tracer.start_as_current_span("user_flow") as span:
        # 1. –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_data = {
            "name": f"LoadTest User {random.randint(1, 1000)}",
            "email": f"test{random.randint(1, 1000)}@example.com"
        }
        response = make_request("/users", "POST", user_data)
        
        if response.status_code != 201:
            span.set_attribute("flow.failed", True)
            return
        
        user_id = response.json()["id"]
        span.set_attribute("user.id", user_id)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        time.sleep(random.uniform(0.1, 0.5))
        make_request(f"/users/{user_id}")
        
        # 3. –°–æ–∑–¥–∞–µ–º –∑–∞–∫–∞–∑
        time.sleep(random.uniform(0.1, 0.5))
        order_data = {
            "user_id": user_id,
            "product": f"Product {random.randint(1, 100)}",
            "amount": round(random.uniform(10, 500), 2)
        }
        response = make_request("/orders", "POST", order_data)
        
        if response.status_code == 201:
            order_id = response.json()["id"]
            span.set_attribute("order.id", order_id)
            
            # 4. –ü–æ–ª—É—á–∞–µ–º –∑–∞–∫–∞–∑
            time.sleep(random.uniform(0.1, 0.5))
            make_request(f"/orders/{order_id}")
        
        span.set_attribute("flow.completed", True)

def run_load_test(num_users=10, concurrent=5):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç load test"""
    print(f"Starting load test: {num_users} users, {concurrent} concurrent")
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent) as executor:
        futures = [executor.submit(user_flow) for _ in range(num_users)]
        
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Error: {e}")
    
    duration = time.time() - start_time
    print(f"Load test completed in {duration:.2f}s")
    print(f"Average: {duration/num_users:.2f}s per user")
    print(f"Throughput: {num_users/duration:.2f} users/sec")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Load testing with tracing')
    parser.add_argument('--users', type=int, default=10, help='Number of users')
    parser.add_argument('--concurrent', type=int, default=5, help='Concurrent requests')
    
    args = parser.parse_args()
    
    run_load_test(num_users=args.users, concurrent=args.concurrent)
```

**3. –°–æ–∑–¥–∞–π dashboard –¥–ª—è APM –≤ Grafana**:

`grafana-dashboards/apm-dashboard.json`:
```json
{
  "dashboard": {
    "title": "Application Performance Monitoring",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "targets": [
          {
            "expr": "sum(rate(traces_spanmetrics_calls_total[5m])) by (service_name)"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 2,
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(traces_spanmetrics_calls_total{status_code=\"STATUS_CODE_ERROR\"}[5m])) by (service_name)"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 3,
        "title": "Latency (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(traces_spanmetrics_latency_bucket[5m])) by (le, service_name))"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 4,
        "title": "Service Map",
        "type": "nodeGraph",
        "targets": [
          {
            "queryType": "serviceMap"
          }
        ]
      }
    ]
  }
}
```

**4. Continuous Profiling —Å Pyroscope**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  pyroscope:
    image: grafana/pyroscope:latest
    container_name: pyroscope
    ports:
      - "4040:4040"
    restart: unless-stopped
```

–û–±–Ω–æ–≤–∏ Python app –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è:
```python
import pyroscope

pyroscope.configure(
    application_name="backend",
    server_address="http://pyroscope:4040",
    tags={
        "environment": "development",
    }
)
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 6

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ distributed tracing (trace, span, context)
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å OpenTelemetry –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Jaeger –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–π—Å–æ–≤
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å sampling –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å performance bottlenecks
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Service Map –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å continuous profiling
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å APM dashboards
‚úÖ –û—Ç–ª–∞–∂–∏–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö

**–ö–ª—é—á–µ–≤—ã–µ takeaways:**
1. –¢—Ä–µ–π—Å–∏–Ω–≥ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ - –±–µ–∑ –Ω–µ–≥–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç–ª–∞–¥–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
2. OpenTelemetry - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ
3. –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–π –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (tail sampling)
4. –°–≤—è–∑—ã–≤–∞–π —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ —á–µ—Ä–µ–∑ trace_id
5. –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
6. Service Map –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–∏—Å—Ç–µ–º—ã
7. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä–µ–π—Å–∏–Ω–≥ –¥–ª—è deep analysis
8. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling —ç–∫–æ–Ω–æ–º–∏—Ç –¥–µ–Ω—å–≥–∏ –∏ storage

## –ú–æ–¥—É–ª—å 7: Kubernetes Monitoring - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ (45 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Kubernetes –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Control Plane                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ   API    ‚îÇ  ‚îÇ  etcd    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  Server  ‚îÇ  ‚îÇ          ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇScheduler ‚îÇ  ‚îÇController‚îÇ            ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ Manager  ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node 1 ‚îÇ         ‚îÇ Node 2 ‚îÇ
‚îÇ        ‚îÇ         ‚îÇ        ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇPod ‚îÇ ‚îÇ         ‚îÇ ‚îÇPod ‚îÇ ‚îÇ
‚îÇ ‚îÇ    ‚îÇ ‚îÇ         ‚îÇ ‚îÇ    ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇPod ‚îÇ ‚îÇ         ‚îÇ ‚îÇPod ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ        ‚îÇ         ‚îÇ        ‚îÇ
‚îÇkubelet ‚îÇ         ‚îÇkubelet ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ K8s:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Application Level                   ‚îÇ  - –ë–∏–∑–Ω–µ—Å –º–µ—Ç—Ä–∏–∫–∏
‚îÇ  (–≤–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ)                   ‚îÇ  - Custom metrics
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Container Level                     ‚îÇ  - CPU, Memory, Network
‚îÇ  (Docker/containerd)                 ‚îÇ  - Restart count
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Pod Level                           ‚îÇ  - Pod status
‚îÇ  (K8s workload)                      ‚îÇ  - Resource limits
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Node Level                          ‚îÇ  - Node resources
‚îÇ  (Worker nodes)                      ‚îÇ  - Disk, Network
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Cluster Level                       ‚îÇ  - API server health
‚îÇ  (Control plane)                     ‚îÇ  - etcd, scheduler
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ K8s:**

**Cluster metrics:**

```
- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ nodes
- Nodes ready/not ready
- Total CPU/Memory capacity
- Total CPU/Memory usage
- API server request rate
- API server latency
- etcd latency
- Scheduler latency
```

**Node metrics:**

```
- CPU usage/limits
- Memory usage/limits
- Disk usage/IOPS
- Network traffic
- Pod count per node
- Node conditions (Ready, DiskPressure, MemoryPressure)
```

**Pod metrics:**

```
- CPU usage/requests/limits
- Memory usage/requests/limits
- Restart count
- Pod phase (Pending, Running, Failed, Succeeded)
- Container state
- Network I/O
```

**Container metrics:**

```
- CPU usage
- Memory usage (RSS, cache, swap)
- Disk I/O
- Network I/O
- OOM kills
```

**–í–∞–∂–Ω—ã–µ K8s —Å–æ—Å—Ç–æ—è–Ω–∏—è:**

```
Pod Phases:
- Pending     - –ñ–¥–µ—Ç scheduling
- Running     - –ó–∞–ø—É—â–µ–Ω –Ω–∞ node
- Succeeded   - –í—Å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å
- Failed      - –•–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä failed
- Unknown     - –°–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ

Container States:
- Waiting     - –ñ–¥–µ—Ç –∑–∞–ø—É—Å–∫–∞
- Running     - –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
- Terminated  - –ó–∞–≤–µ—Ä—à–µ–Ω

Node Conditions:
- Ready              - Node –≥–æ—Ç–æ–≤ –ø—Ä–∏–Ω–∏–º–∞—Ç—å pods
- MemoryPressure     - –ú–∞–ª–æ –ø–∞–º—è—Ç–∏
- DiskPressure       - –ú–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ
- PIDPressure        - –ú–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
- NetworkUnavailable - –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é
```

**Prometheus –≤ Kubernetes:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Prometheus Operator             ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç:                        ‚îÇ
‚îÇ  - Deployment Prometheus                ‚îÇ
‚îÇ  - Service Discovery                    ‚îÇ
‚îÇ  - Scrape configuration                 ‚îÇ
‚îÇ  - Alert rules                          ‚îÇ
‚îÇ  - ServiceMonitor CRDs                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Prometheus Server(s)               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  –°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å:                    ‚îÇ
‚îÇ  - kubelet (cAdvisor)                   ‚îÇ
‚îÇ  - API server                           ‚îÇ
‚îÇ  - Node exporters                       ‚îÇ
‚îÇ  - Application pods                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Kube-state-metrics vs Metrics Server:**

```
Metrics Server:
- –ë–∞–∑–æ–≤—ã–µ CPU/Memory –º–µ—Ç—Ä–∏–∫–∏
- –î–ª—è Horizontal Pod Autoscaler (HPA)
- –î–ª—è kubectl top
- Real-time –¥–∞–Ω–Ω—ã–µ
- –ù–µ —Ö—Ä–∞–Ω–∏—Ç –∏—Å—Ç–æ—Ä–∏—é

Kube-state-metrics:
- –ú–µ—Ç—Ä–∏–∫–∏ –æ K8s –æ–±—ä–µ–∫—Ç–∞—Ö (Deployments, Pods, etc)
- –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
- –î–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ alerting
- –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤ Prometheus —Ñ–æ—Ä–º–∞—Ç–µ
- –î–æ–ø–æ–ª–Ω—è–µ—Ç Metrics Server
```

**ServiceMonitor –∏ PodMonitor:**

yaml

```yaml
# ServiceMonitor - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π scraping —á–µ—Ä–µ–∑ Service
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app
  labels:
    team: backend
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

# PodMonitor - –ø—Ä—è–º–æ–π scraping pods
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: my-app-pods
spec:
  selector:
    matchLabels:
      app: my-app
  podMetricsEndpoints:
  - port: metrics
    interval: 30s
```

**Resource Requests –∏ Limits:**

yaml

```yaml
resources:
  requests:
    cpu: 100m        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–æ–ª—É—á–∏—Ç
    memory: 128Mi
  limits:
    cpu: 500m        # –ú–∞–∫—Å–∏–º—É–º –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    memory: 512Mi    # OOM kill –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—Å–∏—Ç

QoS Classes:
1. Guaranteed  - requests == limits (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç highest)
2. Burstable   - requests < limits
3. BestEffort  - –Ω–µ—Ç requests/limits (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç lowest)
```

**Horizontal Pod Autoscaler (HPA):**

yaml

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Target 70% CPU
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
```

**Vertical Pod Autoscaler (VPA):**

yaml

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreate, Initial, Off
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
```

**–í–∞–∂–Ω—ã–µ PromQL –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è K8s:**

promql

````promql
# CPU
## Node CPU usage
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

## Pod CPU usage
sum(rate(container_cpu_usage_seconds_total{pod!=""}[5m])) by (pod, namespace)

## CPU throttling
rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0

# Memory
## Pod memory usage
sum(container_memory_working_set_bytes{pod!=""}) by (pod, namespace)

## Memory usage vs limit
sum(container_memory_working_set_bytes{pod!=""}) by (pod)
/
sum(container_spec_memory_limit_bytes{pod!=""}) by (pod) * 100

## OOM kills
rate(container_oom_events_total[5m]) > 0

# Disk
## Disk usage per node
(1 - (node_filesystem_avail_bytes{mountpoint="/"} 
/ node_filesystem_size_bytes{mountpoint="/"})) * 100

## Pod disk I/O
rate(container_fs_reads_bytes_total[5m])
rate(container_fs_writes_bytes_total[5m])

# Network
## Pod network traffic
rate(container_network_receive_bytes_total[5m])
rate(container_network_transmit_bytes_total[5m])

## Network errors
rate(container_network_receive_errors_total[5m])
rate(container_network_transmit_errors_total[5m])

# Kubernetes objects
## Pods not ready
kube_pod_status_phase{phase!~"Running|Succeeded"} > 0

## Deployment replicas mismatch
kube_deployment_spec_replicas != kube_deployment_status_replicas_available

## Pod restarts
rate(kube_pod_container_status_restarts_total[15m]) > 0

## Failed pods
kube_pod_status_phase{phase="Failed"} > 0

## Pending pods (–¥–æ–ª–≥–æ)
kube_pod_status_phase{phase="Pending"} > 0

# Resources
## CPU requests vs limits
sum(kube_pod_container_resource_requests{resource="cpu"})
/
sum(kube_pod_container_resource_limits{resource="cpu"})

## Memory requests vs limits
sum(kube_pod_container_resource_requests{resource="memory"})
/
sum(kube_pod_container_resource_limits{resource="memory"})

## Node capacity vs allocatable
sum(kube_node_status_capacity{resource="cpu"})
sum(kube_node_status_allocatable{resource="cpu"})

# API Server
## Request rate
rate(apiserver_request_total[5m])

## Request latency
histogram_quantile(0.99, 
  rate(apiserver_request_duration_seconds_bucket[5m])
)

## Request errors
rate(apiserver_request_total{code=~"5.."}[5m])

# etcd
## Leader changes
rate(etcd_server_leader_changes_seen_total[5m])

## Proposal failures
rate(etcd_server_proposals_failed_total[5m])

## DB size
etcd_mvcc_db_total_size_in_bytes

# Scheduler
## Scheduling latency
histogram_quantile(0.99,
  rate(scheduler_scheduling_duration_seconds_bucket[5m])
)

## Pending pods in queue
scheduler_pending_pods

# HPA
## Current replicas vs desired
kube_horizontalpodautoscaler_status_current_replicas
vs
kube_horizontalpodautoscaler_status_desired_replicas

## HPA metric value
kube_horizontalpodautoscaler_status_current_metrics_value
````

**–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ K8s –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
1. ‚úÖ –í—Å–µ–≥–¥–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π resource requests/limits
2. ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å –≤—Å–µ —É—Ä–æ–≤–Ω–∏: cluster ‚Üí node ‚Üí pod ‚Üí container
3. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π ServiceMonitor –¥–ª—è auto-discovery
4. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π alerting –Ω–∞ Pod restarts –∏ OOM kills
5. ‚úÖ –û—Ç—Å–ª–µ–∂–∏–≤–∞–π CPU throttling
6. ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å kube-state-metrics –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ K8s
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π HPA –¥–ª—è auto-scaling
8. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π PodDisruptionBudget –¥–ª—è availability
9. ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å control plane –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
10. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π namespace –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ –∏ multi-tenancy
11. ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ /metrics endpoint
12. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π labels –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ filtering
````

**Namespace isolation:**

yaml

```yaml
# ResourceQuota - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    pods: "50"

# LimitRange - default limits –¥–ª—è pods
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container
```

**PodDisruptionBudget (–¥–ª—è HA):**

yaml

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-app-pdb
spec:
  minAvailable: 2  # –∏–ª–∏ maxUnavailable: 1
  selector:
    matchLabels:
      app: my-app
```

**Liveness –∏ Readiness –ø—Ä–æbes:**

yaml

````yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 2

# Startup probe (–¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π)
startupProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 0
  periodSeconds: 10
  failureThreshold: 30  # 300s total
````

**–¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∏—Ö –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**
```
–ü—Ä–æ–±–ª–µ–º–∞: Pod –ø–æ—Å—Ç–æ—è–Ω–Ω–æ restarts
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- kubectl describe pod <pod-name>
- kubectl logs <pod-name> --previous
- –ü—Ä–æ–≤–µ—Ä—å liveness probe
- –ü—Ä–æ–≤–µ—Ä—å OOM kills: kube_pod_container_status_terminated_reason{reason="OOMKilled"}

–ü—Ä–æ–±–ª–µ–º–∞: High CPU throttling
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- rate(container_cpu_cfs_throttled_seconds_total[5m])
- –£–≤–µ–ª–∏—á—å CPU limits –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –∫–æ–¥

–ü—Ä–æ–±–ª–µ–º–∞: Pod Pending –¥–æ–ª–≥–æ
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- kubectl describe pod <pod-name>
- –ü—Ä–æ–≤–µ—Ä—å Events
- –ü—Ä–∏—á–∏–Ω—ã: insufficient resources, node selector mismatch, PVC issues

–ü—Ä–æ–±–ª–µ–º–∞: High memory usage
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- container_memory_working_set_bytes
- –ü—Ä–æ–≤–µ—Ä—å memory leaks
- –ù–∞—Å—Ç—Ä–æ–π VPA –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

–ü—Ä–æ–±–ª–µ–º–∞: Slow API requests
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- apiserver_request_duration_seconds
- –ü—Ä–æ–≤–µ—Ä—å etcd latency
- –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–π API server replicas
```

**Grafana dashboards –¥–ª—è K8s:**
```
–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ community dashboards:

1. Kubernetes Cluster Monitoring (315)
   - –û–±—â–∏–π –æ–±–∑–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
   - Nodes, Pods, CPU, Memory

2. Kubernetes / Compute Resources / Cluster (7249)
   - Resource usage –ø–æ namespace
   - Requests vs Limits

3. Kubernetes / Compute Resources / Namespace (Pods) (7630)
   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ pods

4. Node Exporter Full (1860)
   - –î–µ—Ç–∞–ª–∏ –ø–æ nodes

5. Kubernetes apiserver (12006)
   - API server metrics

–ò–º–ø–æ—Ä—Ç: Grafana ‚Üí Dashboards ‚Üí Import ‚Üí ID
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Kubernetes –∫–ª–∞—Å—Ç–µ—Ä–∞:

1. **–°–æ–∑–¥–∞–π –ª–æ–∫–∞–ª—å–Ω—ã–π K8s –∫–ª–∞—Å—Ç–µ—Ä —Å kind**:

`kind-config.yaml`:

yaml

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: monitoring-cluster
nodes:
  - role: control-plane
    image: kindest/node:v1.29.0
    extraPortMappings:
      - containerPort: 30000
        hostPort: 9090
        protocol: TCP
      - containerPort: 30001
        hostPort: 3000
        protocol: TCP
      - containerPort: 30002
        hostPort: 16686
        protocol: TCP
  - role: worker
    image: kindest/node:v1.29.0
  - role: worker
    image: kindest/node:v1.29.0
```

–°–æ–∑–¥–∞–π –∫–ª–∞—Å—Ç–µ—Ä:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ kind –µ—Å–ª–∏ –Ω–µ—Ç
# Mac: brew install kind
# Linux: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64

# –°–æ–∑–¥–∞–π –∫–ª–∞—Å—Ç–µ—Ä
kind create cluster --config kind-config.yaml

# –ü—Ä–æ–≤–µ—Ä—å
kubectl cluster-info
kubectl get nodes
```

2. **–£—Å—Ç–∞–Ω–æ–≤–∏ kube-prometheus-stack (Prometheus Operator)**:

bash

```bash
# –î–æ–±–∞–≤—å Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# –°–æ–∑–¥–∞–π namespace
kubectl create namespace monitoring

# –£—Å—Ç–∞–Ω–æ–≤–∏ kube-prometheus-stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
  --set prometheus.service.type=NodePort \
  --set prometheus.service.nodePort=30000 \
  --set grafana.service.type=NodePort \
  --set grafana.service.nodePort=30001 \
  --set grafana.adminPassword=admin

# –ü—Ä–æ–≤–µ—Ä—å —É—Å—Ç–∞–Ω–æ–≤–∫—É
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

3. **–°–æ–∑–¥–∞–π demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏**:

`k8s-manifests/demo-app-deployment.yaml`:

yaml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: demo-app

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-app
  namespace: demo-app
  labels:
    app: demo-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo-app
  template:
    metadata:
      labels:
        app: demo-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: quay.io/brancz/prometheus-example-app:v0.5.0
        ports:
        - containerPort: 8080
          name: metrics
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 3

---
apiVersion: v1
kind: Service
metadata:
  name: demo-app
  namespace: demo-app
  labels:
    app: demo-app
spec:
  selector:
    app: demo-app
  ports:
  - port: 8080
    targetPort: 8080
    name: metrics
  type: ClusterIP

---
# ServiceMonitor –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ scraping
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: demo-app
  namespace: demo-app
  labels:
    app: demo-app
spec:
  selector:
    matchLabels:
      app: demo-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# HPA –¥–ª—è –∞–≤—Ç–æ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: demo-app-hpa
  namespace: demo-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
# PodDisruptionBudget –¥–ª—è HA
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: demo-app-pdb
  namespace: demo-app
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: demo-app

---
# ResourceQuota –¥–ª—è namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo-app-quota
  namespace: demo-app
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    pods: "20"
```

–ü—Ä–∏–º–µ–Ω–∏:

bash

```bash
kubectl apply -f k8s-manifests/demo-app-deployment.yaml

# –ü—Ä–æ–≤–µ—Ä—å
kubectl get all -n demo-app
kubectl get servicemonitor -n demo-app
kubectl get hpa -n demo-app
```

4. **–°–æ–∑–¥–∞–π PrometheusRule –¥–ª—è alerting**:

`k8s-manifests/prometheus-rules.yaml`:

yaml

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: kubernetes.rules
    interval: 30s
    rules:
    # Pod alerts
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        component: pod
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod has restarted {{ $value }} times in the last 15 minutes"
        dashboard: "http://localhost:3000/d/kubernetes-pods"

    - alert: PodNotReady
      expr: |
        sum by (namespace, pod) (
          kube_pod_status_phase{phase!~"Running|Succeeded"}
        ) > 0
      for: 10m
      labels:
        severity: warning
        component: pod
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
        description: "Pod has been in {{ $labels.phase }} state for more than 10 minutes"

    - alert: PodOOMKilled
      expr: |
        sum by (namespace, pod) (
          rate(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m])
        ) > 0
      for: 1m
      labels:
        severity: critical
        component: pod
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} OOMKilled"
        description: "Pod was killed due to out of memory"
        runbook: "Increase memory limits or fix memory leak"

    # Container alerts
    - alert: ContainerCPUThrottling
      expr: |
        rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0.5
      for: 10m
      labels:
        severity: warning
        component: container
      annotations:
        summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} CPU throttling"
        description: "Container is being throttled {{ $value | humanizePercentage }}"
        runbook: "Increase CPU limits"

    - alert: ContainerHighMemoryUsage
      expr: |
        (
          sum by (namespace, pod, container) (container_memory_working_set_bytes)
          /
          sum by (namespace, pod, container) (container_spec_memory_limit_bytes)
        ) > 0.9
      for: 5m
      labels:
        severity: warning
        component: container
      annotations:
        summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} high memory"
        description: "Memory usage is {{ $value | humanizePercentage }}"

    # Deployment alerts
    - alert: DeploymentReplicasMismatch
      expr: |
        kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 10m
      labels:
        severity: warning
        component: deployment
      annotations:
        summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
        description: "Desired: {{ $value }}, Available: {{ $labels.replicas_available }}"

    # Node alerts
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        component: node
      annotations:
        summary: "Node {{ $labels.node }} not ready"
        description: "Node has been unready for more than 5 minutes"

    - alert: NodeHighCPUUsage
      expr: |
        100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
        component: node
      annotations:
        summary: "Node {{ $labels.instance }} high CPU"
        description: "CPU usage is {{ $value | humanize }}%"

    - alert: NodeHighMemoryUsage
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
        component: node
      annotations:
        summary: "Node {{ $labels.instance }} high memory"
        description: "Memory usage is {{ $value | humanize }}%"

    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
        component: node
      annotations:
        summary: "Node {{ $labels.node }} disk pressure"
        description: "Node is experiencing disk pressure"

    # HPA alerts
    - alert: HPAMaxedOut
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas
        ==
        kube_horizontalpodautoscaler_spec_max_replicas
      for: 15m
      labels:
        severity: warning
        component: hpa
      annotations:
        summary: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} maxed out"
        description: "HPA has been at max replicas ({{ $value }}) for 15 minutes"
        runbook: "Consider increasing max replicas"

    - alert: HPAScalingDisabled
      expr: |
        kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false"} == 1
      for: 5m
      labels:
        severity: warning
        component: hpa
      annotations:
        summary: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} scaling disabled"
        description: "HPA is unable to compute metrics"

    # Control plane alerts
    - alert: APIServerHighLatency
      expr: |
        histogram_quantile(0.99,
          sum by (le) (rate(apiserver_request_duration_seconds_bucket[5m]))
        ) > 1
      for: 5m
      labels:
        severity: warning
        component: apiserver
      annotations:
        summary: "API Server high latency"
        description: "P99 latency is {{ $value }}s"

    - alert: APIServerErrorRate
      expr: |
        sum(rate(apiserver_request_total{code=~"5.."}[5m]))
        /
        sum(rate(apiserver_request_total[5m])) > 0.05
      for: 5m
      labels:
        severity: critical
        component: apiserver
      annotations:
        summary: "API Server high error rate"
        description: "Error rate is {{ $value | humanizePercentage }}"

    - alert: EtcdHighLatency
      expr: |
        histogram_quantile(0.99,
          rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])
        ) > 0.5
      for: 5m
      labels:
        severity: warning
        component: etcd
      annotations:
        summary: "etcd high latency"
        description: "P99 fsync latency is {{ $value }}s"

    # PersistentVolume alerts
    - alert: PersistentVolumeFillingUp
      expr: |
        (
          kubelet_volume_stats_available_bytes
          /
          kubelet_volume_stats_capacity_bytes
        ) < 0.1
      for: 5m
      labels:
        severity: warning
        component: pv
      annotations:
        summary: "PV {{ $labels.persistentvolumeclaim }} filling up"
        description: "Only {{ $value | humanizePercentage }} available"

````

–ü—Ä–∏–º–µ–Ω–∏:
```bash
kubectl apply -f k8s-manifests/prometheus-rules.yaml

# –ü—Ä–æ–≤–µ—Ä—å rules –≤ Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# –û—Ç–∫—Ä–æ–π http://localhost:9090/rules
```

5. **–°–æ–∑–¥–∞–π load generator –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**:

`k8s-manifests/load-generator.yaml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  namespace: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: load-generator
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      containers:
      - name: load-generator
        image: busybox:latest
        command:
        - /bin/sh
        - -c
        - |
          while true; do
            # –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            for i in $(seq 1 10); do
              wget -q -O- http://demo-app.demo-app.svc.cluster.local:8080/ > /dev/null 2>&1
              sleep 0.1
            done
            
            # –°–ª—É—á–∞–π–Ω—ã–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            if [ $((RANDOM % 10)) -eq 0 ]; then
              echo "Generating slow request..."
              wget -q -O- http://demo-app.demo-app.svc.cluster.local:8080/?sleep=3 > /dev/null 2>&1
            fi
            
            sleep 1
          done
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi

---
# Job –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞
apiVersion: batch/v1
kind: Job
metadata:
  name: stress-test
  namespace: demo-app
spec:
  parallelism: 5
  completions: 5
  template:
    spec:
      containers:
      - name: stress
        image: busybox:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Starting stress test..."
          for i in $(seq 1 100); do
            wget -q -O- http://demo-app.demo-app.svc.cluster.local:8080/ > /dev/null 2>&1 &
          done
          wait
          echo "Stress test complete"
      restartPolicy: Never
  backoffLimit: 4
```

–ü—Ä–∏–º–µ–Ω–∏:
```bash
kubectl apply -f k8s-manifests/load-generator.yaml

# –ó–∞–ø—É—Å—Ç–∏ —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç
kubectl apply -f k8s-manifests/load-generator.yaml

# –ù–∞–±–ª—é–¥–∞–π –∑–∞ HPA
watch kubectl get hpa -n demo-app

# –ü—Ä–æ–≤–µ—Ä—å pods
watch kubectl get pods -n demo-app
```

6. **–î–æ—Å—Ç—É–ø –∫ UI**:
```bash
# Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# http://localhost:9090

# Grafana (admin/admin)
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# http://localhost:3000

# –ò–ª–∏ —á–µ—Ä–µ–∑ NodePort (–µ—Å–ª–∏ kind —Å portMapping)
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000
```

7. **–°–æ–∑–¥–∞–π custom Grafana dashboard**:

–°–æ—Ö—Ä–∞–Ω–∏ –∫–∞–∫ `k8s-dashboard.json` –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π –≤ Grafana:
```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Overview",
    "tags": ["kubernetes", "cluster"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "stat",
        "title": "Cluster Status",
        "targets": [
          {
            "expr": "sum(kube_node_status_condition{condition=\"Ready\",status=\"true\"})",
            "legendFormat": "Ready Nodes"
          },
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods"
          }
        ]
      },
      {
        "id": 2,
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "type": "timeseries",
        "title": "Cluster CPU Usage",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total[5m])) by (namespace)",
            "legendFormat": "{{ namespace }}"
          }
        ]
      },
      {
        "id": 3,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "type": "timeseries",
        "title": "Cluster Memory Usage",
        "targets": [
          {
            "expr": "sum(container_memory_working_set_bytes) by (namespace)",
            "legendFormat": "{{ namespace }}"
          }
        ]
      },
      {
        "id": 4,
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "type": "table",
        "title": "Top Pods by CPU",
        "targets": [
          {
            "expr": "topk(10, sum(rate(container_cpu_usage_seconds_total[5m])) by (namespace, pod))",
            "format": "table",
            "instant": true
          }
        ]
      }
    ]
  }
}
```

8. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è**:
```bash
# –ü—Ä–æ–≤–µ—Ä—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è
kubectl exec -n monitoring prometheus-kube-prometheus-prometheus-0 -- \
  promtool query instant http://localhost:9090 'up'

# –ü—Ä–æ–≤–µ—Ä—å ServiceMonitor –æ–±–Ω–∞—Ä—É–∂–µ–Ω
kubectl get servicemonitor -A

# –ü—Ä–æ–≤–µ—Ä—å targets –≤ Prometheus
# http://localhost:9090/targets

# –ü—Ä–æ–≤–µ—Ä—å alerts
# http://localhost:9090/alerts

# –°–∏–º—É–ª–∏—Ä—É–π –ø—Ä–æ–±–ª–µ–º—ã
# OOMKill
kubectl run oom-test --image=polinux/stress --restart=Never -- \
  stress --vm 1 --vm-bytes 1G --timeout 10s

# CPU stress –¥–ª—è HPA
kubectl run cpu-stress --image=polinux/stress --restart=Never -- \
  stress --cpu 4 --timeout 60s

# –ù–∞–±–ª—é–¥–∞–π –∑–∞ scaling
watch kubectl get hpa -n demo-app
watch kubectl get pods -n demo-app
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –£—Å—Ç–∞–Ω–æ–≤–∏ Metrics Server –¥–ª—è kubectl top**:
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# –î–ª—è kind –Ω—É–∂–µ–Ω –ø–∞—Ç—á (insecure TLS)
kubectl patch deployment metrics-server -n kube-system --type='json' \
  -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

# –ü—Ä–æ–≤–µ—Ä—å
kubectl top nodes
kubectl top pods -A
```

**2. Vertical Pod Autoscaler**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ VPA
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh

# –°–æ–∑–¥–∞–π VPA –¥–ª—è demo-app
cat <<EOF | kubectl apply -f -
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: demo-app-vpa
  namespace: demo-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: demo-app
  updatePolicy:
    updateMode: "Off"  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±–µ–∑ –∞–≤—Ç–æ–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 1
        memory: 1Gi
EOF

# –ü—Ä–æ–≤–µ—Ä—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
kubectl describe vpa demo-app-vpa -n demo-app
```

**3. Kube-state-metrics custom metrics**:

–°–æ–∑–¥–∞–π ConfigMap —Å custom resource state metrics:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-state-metrics-customresourcestate-config
  namespace: monitoring
data:
  config.yaml: |
    kind: CustomResourceStateMetrics
    spec:
      resources:
        - groupVersionKind:
            group: "apps"
            version: "v1"
            kind: "Deployment"
          metricNamePrefix: "kube_deployment"
          metrics:
            - name: "replicas_custom"
              help: "Custom deployment replicas metric"
              each:
                type: Gauge
                gauge:
                  path: [spec, replicas]
```

**4. Cost monitoring —Å OpenCost**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ OpenCost
helm install opencost opencost/opencost \
  --namespace opencost --create-namespace \
  --set prometheus.internal.enabled=false \
  --set prometheus.external.url=http://prometheus-kube-prometheus-prometheus.monitoring:9090

# Port-forward
kubectl port-forward -n opencost svc/opencost 9090:9090

# –û—Ç–∫—Ä–æ–π UI
# http://localhost:9090
```

**5. Cluster autoscaler (–¥–ª—è cloud)**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.29.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --cloud-provider=aws  # –∏–ª–∏ gce, azure
        - --nodes=2:10:worker-nodes
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
```

**6. Network Policy monitoring**:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: demo-app-netpol
  namespace: demo-app
spec:
  podSelector:
    matchLabels:
      app: demo-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: demo-app
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 53  # DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443  # HTTPS
```

**7. –°–æ–∑–¥–∞–π script –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º**:

`k8s-troubleshoot.sh`:
```bash
#!/bin/bash

echo "=== Kubernetes Cluster Health Check ==="
echo ""

# Nodes
echo "üì¶ Nodes Status:"
kubectl get nodes -o wide
echo ""

echo "‚ö†Ô∏è  Not Ready Nodes:"
kubectl get nodes --field-selector spec.unschedulable=false | grep -v "Ready" || echo "All nodes ready"
echo ""

# Pods
echo "üî¥ Failed/Pending Pods:"
kubectl get pods -A --field-selector status.phase!=Running,status.phase!=Succeeded
echo ""

echo "üîÑ Restarting Pods (last hour):"
kubectl get pods -A -o json | jq -r '.items[] | select(.status.containerStatuses[]?.restartCount > 0) | "\(.metadata.namespace)/\(.metadata.name): \(.status.containerStatuses[0].restartCount) restarts"'
echo ""

# Resources
echo "üìä Top Resource Consumers:"
echo "CPU:"
kubectl top pods -A --sort-by=cpu | head -10
echo ""
echo "Memory:"
kubectl top pods -A --sort-by=memory | head -10
echo ""

# Events
echo "‚ö° Recent Events (errors):"
kubectl get events -A --sort-by='.lastTimestamp' | grep -i "error\|fail\|warning" | tail -20
echo ""

# HPA Status
echo "üìà HPA Status:"
kubectl get hpa -A
echo ""

# PVC Status
echo "üíæ PVC Status:"
kubectl get pvc -A
echo ""

echo "=== Health Check Complete ==="
```

**8. Monitoring Helm chart values –¥–ª—è production**:

`prometheus-values-prod.yaml`:
```yaml
prometheus:
  prometheusSpec:
    retention: 30d
    retentionSize: "50GB"
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi
    
    # High availability
    replicas: 2
    
    # Remote write –¥–ª—è long-term storage
    remoteWrite:
    - url: "http://thanos-receive:19291/api/v1/receive"
    
    # Service monitors
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false

alertmanager:
  alertmanagerSpec:
    replicas: 3
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

grafana:
  replicas: 2
  persistence:
    enabled: true
    size: 10Gi
  
  # SSO integration
  grafana.ini:
    auth.generic_oauth:
      enabled: true
      name: OAuth
      allow_sign_up: true
      client_id: your-client-id
      client_secret: your-client-secret
      scopes: openid profile email
      auth_url: https://auth.example.com/authorize
      token_url: https://auth.example.com/token
      api_url: https://auth.example.com/userinfo

# Node exporter –Ω–∞ –≤—Å–µ—Ö nodes
prometheus-node-exporter:
  tolerations:
  - effect: NoSchedule
    operator: Exists

# Kube-state-metrics
kube-state-metrics:
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 200m
      memory: 512Mi
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 7

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Kubernetes –∏ —É—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚úÖ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å kube-prometheus-stack
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å ServiceMonitor –¥–ª—è auto-discovery
‚úÖ –ü–∏—Å–∞—Ç—å PrometheusRule –¥–ª—è K8s alerting
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å HPA –∏ VPA –¥–ª—è autoscaling
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å control plane –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å pod restarts, OOM kills, CPU throttling
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å Grafana dashboards –¥–ª—è K8s
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å kubectl top –∏ Metrics Server
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å ResourceQuota –∏ LimitRange
‚úÖ Troubleshooting –ø—Ä–æ–±–ª–µ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å cost monitoring

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ K8s:**

```
Cluster:  Nodes ready, API latency, etcd health
Nodes:    CPU/Memory usage, disk pressure
Pods:     Restarts, OOM kills, phase
Workload: Replicas mismatch, HPA status
Network:  Traffic, errors, latency
```


**Production checklist:**
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã resource requests/limits –¥–ª—è –≤—Å–µ—Ö pods
- ‚úÖ HPA –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
- ‚úÖ PodDisruptionBudget –¥–ª—è HA
- ‚úÖ Liveness/Readiness probes
- ‚úÖ Monitoring –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π (cluster ‚Üí node ‚Üí pod ‚Üí container)
- ‚úÖ Alerting –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
- ‚úÖ Grafana dashboards –¥–ª—è –≤—Å–µ–π –∫–æ–º–∞–Ω–¥—ã
- ‚úÖ ServiceMonitor –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
- ‚úÖ ResourceQuota –¥–ª—è namespaces
- ‚úÖ Network policies –¥–ª—è security
- ‚úÖ Regular backup etcd
- ‚úÖ Cost tracking –∏ optimization


## –ú–æ–¥—É–ª—å 8: SRE –ü—Ä–∞–∫—Ç–∏–∫–∏ - SLO/SLI/SLA –∏ Error Budget (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**SRE (Site Reliability Engineering) - —á—Ç–æ —ç—Ç–æ:**

```
SRE = Software Engineering + Operations

–¶–µ–ª—å: –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å—é —Å–∏—Å—Ç–µ–º—ã

–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
1. –ò–∑–º–µ—Ä—è–µ–º–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å (SLI/SLO/SLA)
2. Error Budget - –º–æ–∂–µ–º —Ä–∏—Å–∫–æ–≤–∞—Ç—å
3. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è - toil reduction
4. –ü–æ—Å—Ç–º–æ—Ä—Ç–µ–º—ã –±–µ–∑ –æ–±–≤–∏–Ω–µ–Ω–∏–π
5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–∫ –∫–æ–¥
```

**SLI/SLO/SLA - —Ä–∞–∑–Ω–∏—Ü–∞:**

```
SLI (Service Level Indicator) - –ß–¢–û –º—ã –∏–∑–º–µ—Ä—è–µ–º
‚îú‚îÄ Availability: % —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
‚îú‚îÄ Latency: –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (p50, p95, p99)
‚îú‚îÄ Throughput: –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
‚îú‚îÄ Durability: % —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
‚îî‚îÄ Correctness: % –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤

SLO (Service Level Objective) - –¶–ï–õ–¨ –∫–æ—Ç–æ—Ä—É—é —Ö–æ—Ç–∏–º –¥–æ—Å—Ç–∏—á—å
‚îú‚îÄ "99.9% –∑–∞–ø—Ä–æ—Å–æ–≤ —É—Å–ø–µ—à–Ω—ã"
‚îú‚îÄ "95% –∑–∞–ø—Ä–æ—Å–æ–≤ < 200ms"
‚îú‚îÄ "99.95% –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã"
‚îî‚îÄ –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ü–µ–ª—å –∫–æ–º–∞–Ω–¥—ã

SLA (Service Level Agreement) - –ö–û–ù–¢–†–ê–ö–¢ —Å –∫–ª–∏–µ–Ω—Ç–æ–º
‚îú‚îÄ "99.5% uptime –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º"
‚îú‚îÄ –ï—Å–ª–∏ –Ω–∞—Ä—É—à–∞–µ–º ‚Üí —à—Ç—Ä–∞—Ñ/–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è
‚îî‚îÄ SLA ‚â§ SLO (–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–µ–±–µ –∑–∞–ø–∞—Å)

–ü—Ä–∞–≤–∏–ª–æ: SLI ‚Üê SLO ‚Üê SLA
         –ò–∑–º–µ—Ä–µ–Ω–∏–µ ‚Üê –¶–µ–ª—å ‚Üê –î–æ–≥–æ–≤–æ—Ä
```

**–ü—Ä–∏–º–µ—Ä SLI/SLO/SLA:**

```
–°–µ—Ä–≤–∏—Å: API –¥–ª—è –ø–ª–∞—Ç–µ–∂–µ–π

SLI (–∏–∑–º–µ—Ä–µ–Ω–∏—è):
- Availability = successful_requests / total_requests
- Latency (p99) = 99-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
- Error rate = failed_requests / total_requests

SLO (—Ü–µ–ª–∏):
- Availability ‚â• 99.9% (monthly)
- Latency p99 ‚â§ 500ms (monthly)
- Error rate ‚â§ 0.1% (monthly)

SLA (–∫–æ–Ω—Ç—Ä–∞–∫—Ç —Å –∫–ª–∏–µ–Ω—Ç–æ–º):
- Availability ‚â• 99.5% (monthly)
- –ï—Å–ª–∏ < 99.5% ‚Üí –≤–æ–∑–≤—Ä–∞—Ç 10% —Å—Ç–æ–∏–º–æ—Å—Ç–∏
- –ï—Å–ª–∏ < 99.0% ‚Üí –≤–æ–∑–≤—Ä–∞—Ç 25% —Å—Ç–æ–∏–º–æ—Å—Ç–∏
```

**Error Budget - —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è:**

```
Error Budget = 100% - SLO

–ü—Ä–∏–º–µ—Ä:
SLO = 99.9% ‚Üí Error Budget = 0.1%

–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç:
- 0.1% –∑–∞–ø—Ä–æ—Å–æ–≤ –ú–û–ì–£–¢ —Ñ–µ–π–ª–∏—Ç—å—Å—è
- –ó–∞ –º–µ—Å—è—Ü (30 –¥–Ω–µ–π) = 43 –º–∏–Ω—É—Ç—ã downtime
- –≠—Ç–æ –ù–ï –ø–ª–æ—Ö–æ - —ç—Ç–æ –ë–Æ–î–ñ–ï–¢ –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã!

–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
‚úÖ Error budget –µ—Å—Ç—å ‚Üí –º–æ–∂–µ–º:
   - –î–µ–ø–ª–æ–∏—Ç—å —á–∞—â–µ
   - –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
   - –†–∏—Å–∫–æ–≤–∞—Ç—å —Å –Ω–æ–≤—ã–º–∏ —Ñ–∏—á–∞–º–∏

‚ùå Error budget –∏—Å—á–µ—Ä–ø–∞–Ω ‚Üí –Ω—É–∂–Ω–æ:
   - Freeze –Ω–∞ –Ω–æ–≤—ã–µ —Ñ–∏—á–∏
   - –§–æ–∫—É—Å –Ω–∞ stability
   - –ë–∞–≥-—Ñ–∏–∫—Å—ã –∏ reliability work
```

**Error Budget calculation:**

```
Monthly SLO = 99.9%
Total time = 30 days = 43,200 minutes

Allowed downtime = 43,200 * (1 - 0.999) = 43.2 minutes

–¢–µ–∫—É—â–∏–π –º–µ—Å—è—Ü:
- –ü—Ä–æ—à–ª–æ: 15 –¥–Ω–µ–π
- Downtime: 20 –º–∏–Ω—É—Ç
- Budget used: 20 / 43.2 = 46.3%
- Budget remaining: 53.7% (23.2 –º–∏–Ω—É—Ç—ã)

–°—Ç–∞—Ç—É—Å: ‚úÖ OK - –º–æ–∂–µ–º –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –¥–µ–ø–ª–æ–∏—Ç—å
```

**Burn Rate - —Å–∫–æ—Ä–æ—Å—Ç—å —Å–∂–∏–≥–∞–Ω–∏—è –±—é–¥–∂–µ—Ç–∞:**

```
Burn Rate = –∫–∞–∫ –±—ã—Å—Ç—Ä–æ —Ç—Ä–∞—Ç–∏–º error budget

Normal rate = 1.0
- –¢—Ä–∞—Ç–∏–º —Ä–æ–≤–Ω–æ —Å—Ç–æ–ª—å–∫–æ, —Å–∫–æ–ª—å–∫–æ –∑–∞–ª–æ–∂–µ–Ω–æ
- 1% –±—é–¥–∂–µ—Ç–∞ –≤ —á–∞—Å –ø—Ä–∏ 100 —á–∞—Å–∞—Ö –≤ –±—é–¥–∂–µ—Ç–µ

High burn rate > 1.0 - –ø—Ä–æ–±–ª–µ–º–∞!
- Burn rate = 10 ‚Üí –±—é–¥–∂–µ—Ç –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –∑–∞ 1/10 –≤—Ä–µ–º–µ–Ω–∏
- –ù—É–∂–Ω—ã –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è

–ü—Ä–∏–º–µ—Ä:
30-day budget = 43.2 minutes
Current burn rate = 5.0
‚Üí –ë—é–¥–∂–µ—Ç –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –∑–∞ 6 –¥–Ω–µ–π –≤–º–µ—Å—Ç–æ 30!
```

**Multi-window multi-burn-rate alerts:**

```
–ò–¥–µ—è –æ—Ç Google SRE:
–†–∞–∑–Ω—ã–µ –æ–∫–Ω–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö burn rates

Fast burn (–∫—Ä–∏—Ç–∏—á–Ω–æ):
- 5min window + 1h window
- Burn rate > 14.4
- Alert: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤ PagerDuty
- –î–µ–π—Å—Ç–≤–∏–µ: –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ

Slow burn (–≤–∞–∂–Ω–æ):
- 30min window + 6h window
- Burn rate > 6
- Alert: ticket –≤ Jira
- –î–µ–π—Å—Ç–≤–∏–µ: –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è

–ü—Ä–∏–º–µ—Ä rule:
(
  error_rate[5m] > 14.4 * (1 - SLO)  # 5-–º–∏–Ω—É—Ç–Ω–æ–µ –æ–∫–Ω–æ
  AND
  error_rate[1h] > 14.4 * (1 - SLO)  # 1-—á–∞—Å–æ–≤–æ–µ –æ–∫–Ω–æ
)
‚Üí Page on-call engineer
```

**SLO tiers (—Ä–∞–∑–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã = —Ä–∞–∑–Ω—ã–µ SLO):**

```
Tier 1 - Critical (99.95%)
‚îú‚îÄ Payment processing
‚îú‚îÄ Authentication
‚îî‚îÄ User-facing API

Tier 2 - Important (99.9%)
‚îú‚îÄ Analytics API
‚îú‚îÄ Recommendations
‚îî‚îÄ Search

Tier 3 - Best effort (99.0%)
‚îú‚îÄ Admin panel
‚îú‚îÄ Internal tools
‚îî‚îÄ Batch jobs

Tier 4 - Eventual (95.0%)
‚îú‚îÄ Data backfill
‚îú‚îÄ ML training
‚îî‚îÄ Reports
```

**SLO document template:**

yaml

````yaml
service: payment-api
owner: payments-team
tier: tier-1

slo:
  availability:
    target: 99.95%
    window: 30d
    measurement:
      sli: |
        sum(rate(http_requests_total{status!~"5.."}[5m]))
        /
        sum(rate(http_requests_total[5m]))
      
  latency:
    target: 95%  # 95% –∑–∞–ø—Ä–æ—Å–æ–≤ < 200ms
    threshold: 200ms
    window: 30d
    measurement:
      sli: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket[5m])
        )

error_budget:
  policy:
    - remaining > 50%: Normal deployment cadence
    - remaining 25-50%: Review each deployment
    - remaining < 25%: Freeze deployments, focus on reliability
    - remaining < 0%: Incident, all hands on deck

dependencies:
  - service: auth-service
    criticality: hard  # Cannot work without it
  - service: cache-service
    criticality: soft  # Degraded without it

alerts:
  - name: ErrorBudgetFastBurn
    severity: page
    condition: burn_rate[5m] > 14.4 AND burn_rate[1h] > 14.4
  
  - name: ErrorBudgetSlowBurn
    severity: ticket
    condition: burn_rate[30m] > 6 AND burn_rate[6h] > 6

runbook: https://wiki.company.com/runbooks/payment-api
dashboard: https://grafana.company.com/d/payment-api
````

**Choosing good SLIs:**
```
‚úÖ Good SLI:
- –ò–∑–º–µ—Ä—è–µ—Ç user experience
- –õ–µ–≥–∫–æ –∏–∑–º–µ—Ä–∏—Ç—å
- –ò–º–µ–µ—Ç –±–∏–∑–Ω–µ—Å-impact
- –ú–æ–∂–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ –Ω–µ–≥–æ

‚ùå Bad SLI:
- –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –º–µ—Ç—Ä–∏–∫–∞ (queue depth, CPU)
- –ù–µ —Å–≤—è–∑–∞–Ω–∞ —Å user experience
- –ù–µ –º–æ–∂–µ–º –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å
- –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ –∏–∑–º–µ—Ä–∏—Ç—å

–ü—Ä–∏–º–µ—Ä—ã:

‚úÖ "95% search queries return in < 100ms"
   ‚Üí User experience, measurable, controllable

‚ùå "Average server CPU < 70%"
   ‚Üí Internal metric, –Ω–µ —Å–≤—è–∑–∞–Ω–æ —Å UX

‚úÖ "99.9% of payments processed successfully"
   ‚Üí Business impact, clear

‚ùå "Database connection pool utilization < 80%"
   ‚Üí Internal, –Ω–µ user-facing
```

**SLO –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–µ—Ä–≤–∏—Å–æ–≤:**
```
Request/Response (API):
- Availability: % —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- Latency: p50, p95, p99
- Throughput: requests/sec

Data Processing (Pipeline):
- Freshness: –∑–∞–¥–µ—Ä–∂–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
- Coverage: % –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- Correctness: % –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

Storage:
- Durability: % —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- Availability: % —É—Å–ø–µ—à–Ω—ã—Ö read/write
- Latency: time to first byte

Batch Jobs:
- Success rate: % —É—Å–ø–µ—à–Ω—ã—Ö jobs
- Throughput: records/hour
- Freshness: –≤—Ä–µ–º—è –¥–æ completion
```

**Toil - —á—Ç–æ —ç—Ç–æ –∏ –∫–∞–∫ –∏–∑–º–µ—Ä–∏—Ç—å:**
```
Toil = –†—É—á–Ω–∞—è, –ø–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º–∞—è —Ä–∞–±–æ—Ç–∞

–ü—Ä–∏–∑–Ω–∞–∫–∏ toil:
‚úì Manual - —Ç—Ä–µ–±—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–∞
‚úì Repetitive - –¥–µ–ª–∞–µ—Ç—Å—è —Å–Ω–æ–≤–∞ –∏ —Å–Ω–æ–≤–∞
‚úì Automatable - –º–æ–∂–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
‚úì Tactical - –Ω–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞
‚úì No enduring value - –Ω–µ —Å–æ–∑–¥–∞–µ—Ç —Ü–µ–Ω–Ω–æ—Å—Ç—å
‚úì Linear scaling - —Ä–∞—Å—Ç–µ—Ç —Å –Ω–∞–≥—Ä—É–∑–∫–æ–π

–ü—Ä–∏–º–µ—Ä—ã toil:
- –†—É—á–Ω–æ–π restart —Å–µ—Ä–≤–∏—Å–æ–≤
- –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤ –≤—Ä—É—á–Ω—É—é
- –†—É—á–Ω–æ–µ scaling
- Checking alerts manually
- –†—É—á–Ω–æ–π deployment

–¶–µ–ª—å SRE: < 50% –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ toil

–ò–∑–º–µ—Ä–µ–Ω–∏–µ:
- –¢—Ä–µ–∫–∞–π –≤—Ä–µ–º—è –Ω–∞ –∫–∞–∂–¥—É—é –∑–∞–¥–∞—á—É
- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑—É–π: engineering vs toil
- Target: 50% engineering work
```

**–ü–æ—Å—Ç–º–æ—Ä—Ç–µ–º (Post-mortem) - —É—á–∏–º—Å—è –Ω–∞ –æ—à–∏–±–∫–∞—Ö:**
```
–ü—Ä–∞–≤–∏–ª–æ #1: Blameless - –Ω–µ –≤–∏–Ω–∏–º –ª—é–¥–µ–π!

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Å—Ç–º–æ—Ä—Ç–µ–º–∞:

1. Summary
   - –ß—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
   - Impact: –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, downtime
   - –ù–∞—á–∞–ª–æ: –≤—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞
   - –ö–æ–Ω–µ—Ü: –≤—Ä–µ–º—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
   - Duration: –æ–±—â–µ–µ –≤—Ä–µ–º—è

2. Timeline
   - [10:00] Deploy v2.3.4
   - [10:15] Error rate spike to 15%
   - [10:20] Rollback initiated
   - [10:25] Error rate back to normal

3. Root Cause
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–∏—á–∏–Ω–∞ (–Ω–µ "–∫—Ç–æ-—Ç–æ –æ—à–∏–±—Å—è")
   - Configuration bug in new version
   - Missing validation in deployment pipeline

4. Impact
   - 15 minutes of degraded service
   - 5% of users affected
   - 0 data loss
   - $5000 estimated revenue loss

5. What Went Well
   - Fast detection (5 minutes)
   - Clear runbook followed
   - Good communication

6. What Went Wrong
   - No canary deployment
   - Testing didn't catch the bug
   - Monitoring delayed alert

7. Action Items
   - [P0] Add canary stage to pipeline (Owner: Alice, ETA: 1 week)
   - [P1] Improve test coverage (Owner: Bob, ETA: 2 weeks)
   - [P2] Tune alert thresholds (Owner: Carol, ETA: 3 days)

8. Lessons Learned
   - Always canary deploy
   - Integration tests needed
   - Monitoring can be improved
```

**On-call rotation best practices:**
```
–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
- Primary on-call: –ø–µ—Ä–≤–∞—è –ª–∏–Ω–∏—è
- Secondary on-call: backup
- Escalation: –µ—Å–ª–∏ primary –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç

Shifts:
- 7 –¥–Ω–µ–π –Ω–∞ —á–µ–ª–æ–≤–µ–∫–∞ (–Ω–µ –±–æ–ª—å—à–µ!)
- Handoff meeting: –ø–µ—Ä–µ–¥–∞—á–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- Follow-the-sun: —Ä–∞–∑–Ω—ã–µ timezone

Compensation:
- –ö–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è –∑–∞ on-call –≤—Ä–µ–º—è
- Time off –ø–æ—Å–ª–µ —Ç—è–∂–µ–ª–æ–≥–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞
- Rotation –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å fair

Tools:
- PagerDuty / Opsgenie / VictorOps
- Clear escalation policy
- Runbooks –¥–ª—è –≤—Å–µ—Ö alerts

Health:
- Limit pages: < 2 per night acceptable
- If more ‚Üí —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–∑–¥–æ—Ä–æ–≤–∞
- Fix –∏–ª–∏ adjust alerts
```

**Reliability hierarchy:**
```
Level 4: Self-healing systems
         ‚îú‚îÄ Auto-remediation
         ‚îî‚îÄ Zero human intervention

Level 3: Automated response
         ‚îú‚îÄ Auto-scaling
         ‚îú‚îÄ Auto-rollback
         ‚îî‚îÄ Circuit breakers

Level 2: Good observability
         ‚îú‚îÄ Metrics, logs, traces
         ‚îú‚îÄ Dashboards
         ‚îî‚îÄ Alerts with runbooks

Level 1: Manual operations
         ‚îú‚îÄ SSH into servers
         ‚îú‚îÄ Manual restarts
         ‚îî‚îÄ No monitoring

Target: Level 3-4 –¥–ª—è production —Å–∏—Å—Ç–µ–º
```

**Capacity planning:**
```
–ü—Ä–∞–≤–∏–ª–æ: Plan for 2x current peak

Steps:
1. Measure current usage
   - Peak requests/sec
   - Peak CPU/Memory
   - Peak disk I/O

2. Forecast growth
   - Historical trends
   - Business projections
   - Seasonal patterns

3. Calculate headroom
   - Current capacity: 1000 req/s
   - Peak usage: 600 req/s
   - Headroom: 40%
   - Target: 50%+ headroom

4. Plan ahead
   - When hits 80% ‚Üí order more capacity
   - Lead time: 3-6 months for hardware
   - Cloud: easier, but still plan

5. Load testing
   - Regular load tests
   - Verify capacity estimates
   - Find bottlenecks early
```

**Change management:**
```
Change Types:

Low risk:
- Config updates (reviewed)
- Scaling up
- Monitoring changes
‚Üí Normal approval

Medium risk:
- Code deployments
- Database schema changes
- Infrastructure updates
‚Üí Change review + testing

High risk:
- Data migrations
- Major architecture changes
- Dependency upgrades
‚Üí Change advisory board + extensive testing

Change windows:
- Production: Tue-Thu, 10am-4pm
- No Fridays (weekend recovery)
- No holidays
- Emergency: any time (with approval)

Rollback plan:
- Every change needs rollback plan
- Test rollback procedure
- Time limit: can rollback in < 10 min
```

**Golden Signals (4 –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏):**
```
1. Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)
   - –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
   - p50, p95, p99
   - Separate success vs error latency

2. Traffic (–¢—Ä–∞—Ñ–∏–∫)
   - –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
   - Transactions per second
   - Network I/O

3. Errors (–û—à–∏–±–∫–∏)
   - Rate of failed requests
   - 5xx errors, timeouts
   - % of total requests

4. Saturation (–ù–∞—Å—ã—â–µ–Ω–∏–µ)
   - Resource utilization
   - CPU, Memory, Disk
   - Queue depth, thread pool

If you can only monitor 4 things, monitor these!
````

**PromQL –¥–ª—è SLO:**

promql

```promql
# Availability SLI
sum(rate(http_requests_total{status!~"5.."}[30d]))
/
sum(rate(http_requests_total[30d]))

# Error budget remaining (30 days)
1 - (
  (1 - availability_sli)  # actual error rate
  /
  (1 - 0.999)             # target error rate (SLO = 99.9%)
)

# Burn rate (5 minutes)
(
  sum(rate(http_requests_total{status=~"5.."}[5m]))
  /
  sum(rate(http_requests_total[5m]))
)
/
(1 - 0.999)  # Normalize to SLO

# Latency SLI (95% < 200ms)
histogram_quantile(0.95,
  sum(rate(http_request_duration_seconds_bucket[30d])) by (le)
) < 0.2

# Error budget consumption rate
rate(http_requests_total{status=~"5.."}[1h])
/
(
  sum(rate(http_requests_total[30d])) * (1 - 0.999) / 24 / 30
)
# –ï—Å–ª–∏ > 1.0 ‚Üí —Ç—Ä–∞—Ç–∏–º –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–∏
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é SRE —Å–∏—Å—Ç–µ–º—É —Å SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º:

1. **–°–æ–∑–¥–∞–π SLO configuration**:

`slo-config.yaml`:

yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slo-config
  namespace: monitoring
data:
  slo-config.yaml: |
    # Payment API SLO
    payment-api:
      tier: tier-1
      owner: payments-team
      
      availability:
        target: 0.999  # 99.9%
        window: 30d
        sli_query: |
          sum(rate(http_requests_total{service="payment-api",status!~"5.."}[5m]))
          /
          sum(rate(http_requests_total{service="payment-api"}[5m]))
      
      latency:
        target: 0.95  # 95% requests < 200ms
        threshold_ms: 200
        window: 30d
        sli_query: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{service="payment-api"}[5m])) by (le)
          )
      
      error_budget_policy:
        - remaining: "> 75%"
          action: "Normal operations"
          deployment_frequency: "Multiple per day"
        
        - remaining: "50-75%"
          action: "Cautious"
          deployment_frequency: "Once per day"
        
        - remaining: "25-50%"
          action: "Review each deployment"
          deployment_frequency: "As needed only"
        
        - remaining: "< 25%"
          action: "Freeze deployments"
          deployment_frequency: "Emergency fixes only"
        
        - remaining: "< 0%"
          action: "Incident mode"
          deployment_frequency: "Reliability work only"
    
    # User API SLO
    user-api:
      tier: tier-2
      owner: user-team
      
      availability:
        target: 0.99  # 99%
        window: 30d
        sli_query: |
          sum(rate(http_requests_total{service="user-api",status!~"5.."}[5m]))
          /
          sum(rate(http_requests_total{service="user-api"}[5m]))
      
      latency:
        target: 0.90  # 90% requests < 500ms
        threshold_ms: 500
        window: 30d
        sli_query: |
          histogram_quantile(0.90,
            sum(rate(http_request_duration_seconds_bucket{service="user-api"}[5m])) by (le)
          )
```

2. **–°–æ–∑–¥–∞–π Prometheus recording rules –¥–ª—è SLO**:

`k8s-manifests/slo-recording-rules.yaml`:

yaml

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: slo-recording-rules
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    role: slo-rules
spec:
  groups:
  - name: slo.rules
    interval: 30s
    rules:
    # Payment API SLI
    - record: sli:availability:payment_api
      expr: |
        sum(rate(http_requests_total{service="payment-api",status!~"5.."}[5m]))
        /
        sum(rate(http_requests_total{service="payment-api"}[5m]))
    
    - record: sli:latency:payment_api:p95
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_request_duration_seconds_bucket{service="payment-api"}[5m])) by (le)
        )
    
    - record: sli:latency:payment_api:p99
      expr: |
        histogram_quantile(0.99,
          sum(rate(http_request_duration_seconds_bucket{service="payment-api"}[5m])) by (le)
        )
    
    # Error Budget (30 days)
    - record: slo:error_budget:payment_api:30d
      expr: |
        1 - (
          (
            1 - (
              sum(rate(http_requests_total{service="payment-api",status!~"5.."}[30d]))
              /
              sum(rate(http_requests_total{service="payment-api"}[30d]))
            )
          )
          /
          (1 - 0.999)  # SLO target
        )
    
    # Burn Rate (multiple windows)
    - record: slo:burn_rate:payment_api:5m
      expr: |
        (
          sum(rate(http_requests_total{service="payment-api",status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total{service="payment-api"}[5m]))
        )
        /
        (1 - 0.999)
    
    - record: slo:burn_rate:payment_api:1h
      expr: |
        (
          sum(rate(http_requests_total{service="payment-api",status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total{service="payment-api"}[1h]))
        )
        /
        (1 - 0.999)
    
    - record: slo:burn_rate:payment_api:6h
      expr: |
        (
          sum(rate(http_requests_total{service="payment-api",status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total{service="payment-api"}[6h]))
        )
        /
        (1 - 0.999)
    
    # Time to exhaustion (hours)
    - record: slo:error_budget:payment_api:time_to_exhaustion_hours
      expr: |
        (
          slo:error_budget:payment_api:30d
          /
          max_over_time(slo:burn_rate:payment_api:1h[1h])
        ) * 24 * 30
```

3. **–°–æ–∑–¥–∞–π SLO alert rules**:

`k8s-manifests/slo-alert-rules.yaml`:

yaml

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: slo-alert-rules
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    role: slo-alerts
spec:
  groups:
  - name: slo.alerts
    interval: 30s
    rules:
    # Fast burn - Page immediately
    - alert: ErrorBudgetFastBurn
      expr: |
        slo:burn_rate:payment_api:5m > 14.4
        and
        slo:burn_rate:payment_api:1h > 14.4
      for: 2m
      labels:
        severity: page
        service: payment-api
        slo_type: availability
      annotations:
        summary: "Payment API burning error budget too fast"
        description: |
          Error budget will be exhausted in {{ $value | humanizeDuration }}.
          Current burn rate: {{ printf "%.2f" $value }}x
          5m error rate: {{ with query "rate(http_requests_total{service='payment-api',status=~'5..'}[5m]) / rate(http_requests_total{service='payment-api'}[5m])" }}{{ . | first | value | humanizePercentage }}{{ end }}
        dashboard: "http://grafana:3000/d/slo-payment-api"
        runbook: "https://wiki.example.com/runbooks/error-budget-fast-burn"
        playbook: |
          1. Check recent deployments
          2. Check error logs
          3. Consider rollback if deployment related
          4. Page team lead if not resolved in 15 min
    
    # Slow burn - Create ticket
    - alert: ErrorBudgetSlowBurn
      expr: |
        slo:burn_rate:payment_api:30m > 6
        and
        slo:burn_rate:payment_api:6h > 6
      for: 15m
      labels:
        severity: ticket
        service: payment-api
        slo_type: availability
      annotations:
        summary: "Payment API burning error budget steadily"
        description: |
          Error budget will be exhausted in {{ $value | humanizeDuration }}.
          Current burn rate: {{ printf "%.2f" $value }}x
          30m error rate: {{ with query "rate(http_requests_total{service='payment-api',status=~'5..'}[30m]) / rate(http_requests_total{service='payment-api'}[30m])" }}{{ . | first | value | humanizePercentage }}{{ end }}
        dashboard: "http://grafana:3000/d/slo-payment-api"
    
    # Error budget nearly exhausted
    - alert: ErrorBudgetNearlyExhausted
      expr: |
        slo:error_budget:payment_api:30d < 0.25
        and
        slo:error_budget:payment_api:30d > 0
      for: 5m
      labels:
        severity: warning
        service: payment-api
      annotations:
        summary: "Payment API error budget nearly exhausted"
        description: |
          Only {{ $value | humanizePercentage }} of 30-day error budget remaining.
          Consider deployment freeze and focus on reliability.
        dashboard: "http://grafana:3000/d/slo-payment-api"
    
    # Error budget exhausted - deployment freeze!
    - alert: ErrorBudgetExhausted
      expr: |
        slo:error_budget:payment_api:30d <= 0
      for: 5m
      labels:
        severity: critical
        service: payment-api
      annotations:
        summary: "Payment API error budget EXHAUSTED"
        description: |
          30-day error budget is exhausted!
          FREEZE all deployments except emergency fixes.
          Focus on reliability work only.
        dashboard: "http://grafana:3000/d/slo-payment-api"
        runbook: "https://wiki.example.com/runbooks/error-budget-exhausted"
    
    # Latency SLO violation
    - alert: LatencySLOViolation
      expr: |
        sli:latency:payment_api:p95 > 0.2  # 200ms
      for: 10m
      labels:
        severity: warning
        service: payment-api
        slo_type: latency
      annotations:
        summary: "Payment API latency SLO violation"
        description: |
          P95 latency is {{ $value | humanizeDuration }}, exceeding 200ms target.
        dashboard: "http://grafana:3000/d/slo-payment-api"
    
    # SLO at risk (trending)
    - alert: SLOAtRisk
      expr: |
        predict_linear(slo:error_budget:payment_api:30d[6h], 7*24*3600) < 0
      for: 1h
      labels:
        severity: warning
        service: payment-api
      annotations:
        summary: "Payment API SLO at risk"
        description: |
          Based on current trends, error budget will be exhausted in < 7 days.
          Current remaining: {{ $value | humanizePercentage }}
        dashboard: "http://grafana:3000/d/slo-payment-api"
```

4. **–°–æ–∑–¥–∞–π demo app —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å—é**:

`demo-app/sre-demo-app.py`:

python

```python
from flask import Flask, request, jsonify
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import random
import time
import os

# =========================
# App
# =========================

app = Flask(__name__)

# =========================
# Prometheus metrics
# =========================

REQUEST_COUNT = Counter(
    "http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status", "service"],
)

REQUEST_DURATION = Histogram(
    "http_request_duration_seconds",
    "HTTP request duration",
    ["method", "endpoint", "service"],
    buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0],
)

ERROR_BUDGET = Gauge(
    "error_budget_remaining",
    "Remaining error budget (0-1)",
    ["service", "window"],
)

# =========================
# Config
# =========================

SERVICE_NAME = os.getenv("SERVICE_NAME", "payment-api")
ERROR_RATE = float(os.getenv("ERROR_RATE", "0.001"))  # 0.1%
SLOW_REQUEST_RATE = float(os.getenv("SLOW_REQUEST_RATE", "0.05"))  # 5%
SLOW_REQUEST_DURATION = float(os.getenv("SLOW_REQUEST_DURATION", "2.0"))  # seconds

# =========================
# Routes
# =========================

@app.route("/health")
def health():
    return jsonify({"status": "healthy"}), 200


@app.route("/api/payment", methods=["POST"])
def process_payment():
    start_time = time.time()

    try:
        # Simulate error
        if random.random() < ERROR_RATE:
            REQUEST_COUNT.labels(
                method="POST",
                endpoint="/api/payment",
                status="500",
                service=SERVICE_NAME,
            ).inc()
            return jsonify({"error": "Payment processing failed"}), 500

        # Simulate latency
        if random.random() < SLOW_REQUEST_RATE:
            time.sleep(SLOW_REQUEST_DURATION)
        else:
            time.sleep(random.uniform(0.01, 0.1))

        REQUEST_COUNT.labels(
            method="POST",
            endpoint="/api/payment",
            status="200",
            service=SERVICE_NAME,
        ).inc()

        return jsonify({
            "status": "success",
            "transaction_id": f"txn_{random.randint(1000, 9999)}",
        }), 200

    finally:
        duration = time.time() - start_time
        REQUEST_DURATION.labels(
            method="POST",
            endpoint="/api/payment",
            service=SERVICE_NAME,
        ).observe(duration)


@app.route("/api/user/<user_id>")
def get_user(user_id):
    start_time = time.time()

    try:
        # Simulate error (lower rate)
        if random.random() < ERROR_RATE * 0.5:
            REQUEST_COUNT.labels(
                method="GET",
                endpoint="/api/user",
                status="500",
                service=SERVICE_NAME,
            ).inc()
            return jsonify({"error": "User not found"}), 500

        time.sleep(random.uniform(0.01, 0.05))

        REQUEST_COUNT.labels(
            method="GET",
            endpoint="/api/user",
            status="200",
            service=SERVICE_NAME,
        ).inc()

        return jsonify({
            "user_id": user_id,
            "name": f"User {user_id}",
            "email": f"user{user_id}@example.com",
        }), 200

    finally:
        duration = time.time() - start_time
        REQUEST_DURATION.labels(
            method="GET",
            endpoint="/api/user",
            service=SERVICE_NAME,
        ).observe(duration)


# =========================
# Chaos endpoints
# =========================

@app.route("/chaos/increase_errors")
def increase_errors():
    """Simulate incident by increasing error rate"""
    global ERROR_RATE
    ERROR_RATE = min(ERROR_RATE * 2, 0.5)

    return jsonify({
        "message": "Error rate increased",
        "new_rate": ERROR_RATE,
    }), 200


@app.route("/chaos/decrease_errors")
def decrease_errors():
    """Recover from incident"""
    global ERROR_RATE
    ERROR_RATE = max(ERROR_RATE / 2, 0.001)

    return jsonify({
        "message": "Error rate decreased",
        "new_rate": ERROR_RATE,
    }), 200


@app.route("/chaos/slow_down")
def slow_down():
    """Simulate performance degradation"""
    global SLOW_REQUEST_RATE, SLOW_REQUEST_DURATION

    SLOW_REQUEST_RATE = min(SLOW_REQUEST_RATE * 2, 0.5)
    SLOW_REQUEST_DURATION = min(SLOW_REQUEST_DURATION * 1.5, 10.0)

    return jsonify({
        "message": "Service slowed down",
        "slow_rate": SLOW_REQUEST_RATE,
        "slow_duration": SLOW_REQUEST_DURATION,
    }), 200


@app.route("/chaos/speed_up")
def speed_up():
    """Recover performance"""
    global SLOW_REQUEST_RATE, SLOW_REQUEST_DURATION

    SLOW_REQUEST_RATE = max(SLOW_REQUEST_RATE / 2, 0.01)
    SLOW_REQUEST_DURATION = max(SLOW_REQUEST_DURATION / 1.5, 0.1)

    return jsonify({
        "message": "Service sped up",
        "slow_rate": SLOW_REQUEST_RATE,
        "slow_duration": SLOW_REQUEST_DURATION,
    }), 200


# =========================
# Metrics
# =========================

@app.route("/metrics")
def metrics():
    return generate_latest(), 200, {
        "Content-Type": "text/plain; charset=utf-8",
    }


# =========================
# Entry point
# =========================

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

`demo-app/Dockerfile`:
````dockerfile
FROM python:3.11-slim

WORKDIR /app

RUN pip install flask prometheus-client

COPY sre-demo-app.py .

EXPOSE 5000

CMD ["python", "sre-demo-app.py"]
````

5. **Deploy demo app –≤ Kubernetes**:

`k8s-manifests/sre-demo-app.yaml`:
````yaml
apiVersion: v1
kind: Namespace
metadata:
  name: sre-demo

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment-api
  namespace: sre-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: payment-api
  template:
    metadata:
      labels:
        app: payment-api
        service: payment-api
    spec:
      containers:
      - name: app
        image: sre-demo-app:latest
        imagePullPolicy: Never  # For local kind cluster
        ports:
        - containerPort: 5000
        env:
        - name: SERVICE_NAME
          value: "payment-api"
        - name: ERROR_RATE
          value: "0.001"  # 0.1%
        - name: SLOW_REQUEST_RATE
          value: "0.05"    # 5%
        - name: SLOW_REQUEST_DURATION
          value: "2.0"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: payment-api
  namespace: sre-demo
  labels:
    app: payment-api
spec:
  selector:
    app: payment-api
  ports:
  - port: 5000
    targetPort: 5000
    name: http

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: payment-api
  namespace: sre-demo
spec:
  selector:
    matchLabels:
      app: payment-api
  endpoints:
  - port: http
    path: /metrics
    interval: 15s
````

Build and load image to kind:
````bash
# Build image
cd demo-app
docker build -t sre-demo-app:latest .

# Load to kind cluster
kind load docker-image sre-demo-app:latest --name monitoring-cluster

# Deploy
kubectl apply -f ../k8s-manifests/sre-demo-app.yaml
kubectl apply -f ../k8s-manifests/slo-recording-rules.yaml
kubectl apply -f ../k8s-manifests/slo-alert-rules.yaml

# Check
kubectl get pods -n sre-demo
kubectl logs -n sre-demo -l app=payment-api
````

6. **–°–æ–∑–¥–∞–π load generator**:

`load-generator.sh`:
````bash
#!/bin/bash

PAYMENT_API="http://localhost:30003"  # Adjust NodePort

echo "Starting load generation..."
echo "Payment API: $PAYMENT_API"

# Normal load
normal_load() {
    while true; do
        curl -s -X POST $PAYMENT_API/api/payment \
          -H "Content-Type: application/json" \
          -d '{"amount": 100, "currency": "USD"}' \
          > /dev/null 2>&1
        
        sleep 0.1
    done
}

# Start multiple workers
for i in {1..5}; do
    normal_load &
done

echo "Load generation started (5 workers)"
echo "Press Ctrl+C to stop"

wait
````

7. **–°–æ–∑–¥–∞–π Grafana dashboard –¥–ª—è SLO**:

Import –≤ Grafana (`slo-dashboard.json`):
````json
{
  "dashboard": {
    "title": "SLO Dashboard - Payment API",
    "tags": ["slo", "sre", "payment-api"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 0},
        "type": "stat",
        "title": "Availability SLI (30d)",
        "targets": [
          {
            "expr": "sli:availability:payment_api",
            "legendFormat": "Availability"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 0.995, "color": "yellow"},
                {"value": 0.999, "color": "green"}
              ]
            },
            "mappings": [],
            "min": 0.99,
            "max": 1.0
          }
        }
      },
      {
        "id": 2,
        "gridPos": {"h": 6, "w": 8, "x": 8, "y": 0},
        "type": "stat",
        "title": "Error Budget Remaining",
        "targets": [
          {
            "expr": "slo:error_budget:payment_api:30d",
            "legendFormat": "Budget"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 0.25, "color": "orange"},
                {"value": 0.5, "color": "yellow"},
                {"value": 0.75, "color": "green"}
              ]
            },
            "min": 0,
            "max": 1.0
          }
        }
      },
      {
        "id": 3,
        "gridPos": {"h": 6, "w": 8, "x": 16, "y": 0},
        "type": "stat",
        "title": "P95 Latency",
        "targets": [
          {
            "expr": "sli:latency:payment_api:p95",
            "legendFormat": "P95"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 0.2, "color": "yellow"},
                {"value": 0.5, "color": "red"}
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 6},
        "type": "timeseries",
        "title": "Error Budget Over Time",
        "targets": [
          {
            "expr": "slo:error_budget:payment_api:30d",
            "legendFormat": "Error Budget"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "custom": {
              "lineWidth": 2
            }
          },
          "overrides": [
            {
              "matcher": {"id": "byName", "options": "Error Budget"},
              "properties": [
                {
                  "id": "color",
                  "value": {"mode": "thresholds"}
                },
                {
                  "id": "thresholds",
                  "value": {
                    "steps": [
                      {"value": 0, "color": "red"},
                      {"value": 0.25, "color": "orange"},
                      {"value": 0.5, "color": "yellow"},
                      {"value": 0.75, "color": "green"}
                    ]
                  }
                }
              ]
            }
          ]
        }
      },
      {
        "id": 5,
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 6},
        "type": "timeseries",
        "title": "Burn Rate (Multiple Windows)",
        "targets": [
          {
            "expr": "slo:burn_rate:payment_api:5m",
            "legendFormat": "5m window"
          },
          {
            "expr": "slo:burn_rate:payment_api:1h",
            "legendFormat": "1h window"
          },
          {
            "expr": "slo:burn_rate:payment_api:6h",
            "legendFormat": "6h window"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "custom": {
              "lineWidth": 2
            }
          }
        }
      },
      {
        "id": 6,
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 14},
        "type": "timeseries",
        "title": "Request Rate and Error Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{service='payment-api'}[5m]))",
            "legendFormat": "Total Requests/s"
          },
          {
            "expr": "sum(rate(http_requests_total{service='payment-api',status=~'5..'}[5m]))",
            "legendFormat": "Errors/s"
          }
        ]
      },
      {
        "id": 7,
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 22},
        "type": "timeseries",
        "title": "Latency Percentiles",
        "targets": [
          {
            "expr": "sli:latency:payment_api:p95",
            "legendFormat": "P95"
          },
          {
            "expr": "sli:latency:payment_api:p99",
            "legendFormat": "P99"
          },
          {
            "expr": "histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{service='payment-api'}[5m])) by (le))",
            "legendFormat": "P50"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s"
          }
        }
      }
    ],
    "refresh": "30s"
  }
}
````

8. **–°–æ–∑–¥–∞–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è incident simulation**:

`simulate-incident.sh`:
````bash
#!/bin/bash

API_URL="http://localhost:30003"  # Adjust

case "$1" in
  "start-error-spike")
    echo "üî• Starting error spike incident..."
    curl -s $API_URL/chaos/increase_errors
    echo "Error rate increased. Watch SLO dashboard!"
    ;;
  
  "stop-error-spike")
    echo "‚úÖ Recovering from error spike..."
    curl -s $API_URL/chaos/decrease_errors
    echo "Error rate decreased."
    ;;
  
  "start-latency-spike")
    echo "üêå Starting latency spike incident..."
    curl -s $API_URL/chaos/slow_down
    echo "Service slowed down. Watch latency metrics!"
    ;;
  
  "stop-latency-spike")
    echo "‚ö° Recovering from latency spike..."
    curl -s $API_URL/chaos/speed_up
    echo "Service sped up."
    ;;
  
  "burndown-test")
    echo "üî•üî•üî• BURN DOWN TEST - This will exhaust error budget!"
    echo "Are you sure? (yes/no)"
    read confirmation
    if [ "$confirmation" = "yes" ]; then
      # Dramatically increase errors
      for i in {1..5}; do
        curl -s $API_URL/chaos/increase_errors
      done
      echo "Error budget burning fast! Watch alerts fire!"
    fi
    ;;
  
  *)
    echo "Usage: $0 {start-error-spike|stop-error-spike|start-latency-spike|stop-latency-spike|burndown-test}"
    exit 1
    ;;
esac
````

9. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ SLO —Å–∏—Å—Ç–µ–º—ã**:
````bash
# Expose service
kubectl port-forward -n sre-demo svc/payment-api 30003:5000 &

# Start load
./load-generator.sh &

# Watch SLO metrics
watch -n 5 'kubectl exec -n monitoring prometheus-kube-prometheus-prometheus-0 -- promtool query instant http://localhost:9090 "slo:error_budget:payment_api:30d"'

# Simulate incident
./simulate-incident.sh start-error-spike

# Watch alerts fire
kubectl logs -n monitoring -l app.kubernetes.io/name=alertmanager -f

# Check Grafana dashboard
open http://localhost:3000/d/slo-payment-api

# Recover
./simulate-incident.sh stop-error-spike

# Check error budget recovered
````

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –°–æ–∑–¥–∞–π SLO report generator**:

`slo-report.py`:
````python
#!/usr/bin/env python3
"""
SLO Monthly Report Generator
"""
import requests
from datetime import datetime, timedelta
import json

PROMETHEUS_URL = "http://localhost:9090"

def query_prometheus(query):
    """Query Prometheus"""
    response = requests.get(
        f"{PROMETHEUS_URL}/api/v1/query",
        params={'query': query}
    )
    return response.json()['data']['result']

def generate_slo_report(service_name, days=30):
    """Generate SLO report for a service"""
    
    print(f"=" * 60)
    print(f"SLO REPORT: {service_name}")
    print(f"Period: Last {days} days")
    print(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"=" * 60)
    print()
    
    # Availability SLI
    availability_query = f'sli:availability:{service_name}'
    availability_result = query_prometheus(availability_query)
    if availability_result:
        availability = float(availability_result[0]['value'][1])
        print(f"üìä Availability SLI: {availability*100:.3f}%")
        print(f"   Target: 99.900%")
        if availability >= 0.999:
            print(f"   Status: ‚úÖ Meeting SLO")
        else:
            print(f"   Status: ‚ùå Violating SLO")
        print()
    
    # Error Budget
    budget_query = f'slo:error_budget:{service_name}:30d'
    budget_result = query_prometheus(budget_query)
    if budget_result:
        budget = float(budget_result[0]['value'][1])
        print(f"üí∞ Error Budget Remaining: {budget*100:.1f}%")
        
        if budget > 0.75:
            status = "‚úÖ Healthy - Normal operations"
        elif budget > 0.5:
            status = "‚ö†Ô∏è  Caution - Review deployments"
        elif budget > 0.25:
            status = "üî∂ Warning - Slow down deployments"
        elif budget > 0:
            status = "üî¥ Critical - Freeze deployments"
        else:
            status = "üíÄ EXHAUSTED - Incident mode"
        
        print(f"   Status: {status}")
        print()
    
    # Latency
    latency_p95_query = f'sli:latency:{service_name}:p95'
    latency_p95_result = query_prometheus(latency_p95_query)
    if latency_p95_result:
        latency_p95 = float(latency_p95_result[0]['value'][1])
        print(f"‚è±Ô∏è  Latency P95: {latency_p95*1000:.0f}ms")
        print(f"   Target: 200ms")
        if latency_p95 <= 0.2:
            print(f"   Status: ‚úÖ Meeting SLO")
        else:
            print(f"   Status: ‚ùå Violating SLO")
        print()
    
    # Error rate
    error_rate_query = f'sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[{days}d])) / sum(rate(http_requests_total{{service="{service_name}"}}[{days}d]))'
    error_rate_result = query_prometheus(error_rate_query)
    if error_rate_result:
        error_rate = float(error_rate_result[0]['value'][1])
        print(f"‚ùå Error Rate: {error_rate*100:.3f}%")
        print(f"   Target: < 0.100%")
        print()
    
    # Recommendations
    print("üìã Recommendations:")
    if budget > 0.75:
        print("   ‚Ä¢ Continue normal deployment cadence")
        print("   ‚Ä¢ Consider experimenting with new features")
    elif budget > 0.5:
        print("   ‚Ä¢ Review each deployment carefully")
        print("   ‚Ä¢ Increase test coverage")
    elif budget > 0.25:
        print("   ‚Ä¢ Slow down deployment frequency")
        print("   ‚Ä¢ Focus on bug fixes")
        print("   ‚Ä¢ Conduct reliability review")
    elif budget > 0:
        print("   ‚Ä¢ FREEZE deployments except emergency fixes")
        print("   ‚Ä¢ All hands on reliability")
        print("   ‚Ä¢ Schedule incident review")
    else:
        print("   ‚Ä¢ INCIDENT MODE - Error budget exhausted")
        print("   ‚Ä¢ No deployments allowed")
        print("   ‚Ä¢ Emergency reliability work only")
    
    print()
    print("=" * 60)

if __name__ == "__main__":
    import sys
    
    service = sys.argv[1] if len(sys.argv) > 1 else "payment_api"
    days = int(sys.argv[2]) if len(sys.argv) > 2 else 30
    
    generate_slo_report(service, days)
````

**2. Post-mortem template**:

`postmortem-template.md`:
````markdown
# Post-Mortem: [Incident Title]

**Date:** YYYY-MM-DD
**Duration:** X hours Y minutes
**Severity:** P0 / P1 / P2
**Status:** Draft / Under Review / Closed

## Summary
<!-- 1-2 sentences describing what happened -->

## Impact
- **Users Affected:** X% / X users
- **Services Affected:** [list services]
- **Duration:** From HH:MM to HH:MM UTC
- **Estimated Revenue Loss:** $X
- **SLO Impact:** X% error budget consumed

## Timeline (All times UTC)
- **HH:MM** - Initial symptoms detected
- **HH:MM** - Alert fired
- **HH:MM** - Engineer acknowledged
- **HH:MM** - Root cause identified
- **HH:MM** - Fix deployed
- **HH:MM** - Service recovered
- **HH:MM** - Incident closed

## Root Cause
<!-- Technical root cause - what broke and why -->

## Detection
- **How was it detected?** Alert / User report / Monitoring
- **Time to detect:** X minutes from start
- **What worked well?**
- **What could be improved?**

## Response
- **Time to acknowledge:** X minutes
- **Time to mitigate:** X minutes
- **Time to resolve:** X minutes
- **What worked well?**
- **What could be improved?**

## Resolution
<!-- How was it fixed -->

## Five Whys
1. Why did X happen? Because Y
2. Why did Y happen? Because Z
3. Why did Z happen? Because...
4. ...
5. Root cause: ...

## What Went Well
-
-

## What Went Wrong
-
-

## Action Items
| Action | Owner | Priority | ETA | Status |
|--------|-------|----------|-----|--------|
| [P0] Implement circuit breaker | Alice | High | 2024-01-15 | In Progress |
| [P1] Add integration tests | Bob | Medium | 2024-01-20 | Not Started |
| [P2] Improve monitoring | Carol | Low | 2024-01-25 | Not Started |

## Lessons Learned
-
-

## Appendix
### Relevant Logs
````
[paste relevant logs]
`````

### Metrics/Graphs

Show Image Show Image

### Related Incidents

- [INC-123] Similar issue on...

`````

**3. On-call runbook template**:

`runbook-template.md`:
````markdown
# Runbook: [Alert Name]

## Alert Details
- **Alert Name:** ErrorBudgetFastBurn
- **Severity:** Critical / Warning / Info
- **Service:** payment-api
- **SLO Impact:** High / Medium / Low

## What This Means
<!-- Explain in plain English what this alert means -->
The service is consuming error budget at 14x normal rate. If this continues, the monthly error budget will be exhausted in < 2 days.

## Impact
- **User Impact:** Users seeing 5xx errors, payment failures
- **Business Impact:** Revenue loss, customer complaints
- **Dependencies:** May affect downstream services

## Quick Checks
```bash
# Check error rate
kubectl logs -n prod -l app=payment-api --tail=100 | grep ERROR

# Check recent deployments
kubectl rollout history deployment/payment-api -n prod

# Check pod status
kubectl get pods -n prod -l app=payment-api
```

## Common Causes
1. Recent deployment with bugs
   - **Check:** Recent deployments in last hour
   - **Fix:** Rollback deployment

2. Dependency failure
   - **Check:** Database/cache/external API status
   - **Fix:** Restart dependency or switch to backup

3. Resource exhaustion
   - **Check:** CPU/Memory usage
   - **Fix:** Scale up replicas

4. Traffic spike
   - **Check:** Request rate
   - **Fix:** Enable rate limiting or scale

## Investigation Steps
1. Check Grafana dashboard: [link]
2. Check recent changes: `kubectl rollout history`
3. Check error logs: `kubectl logs -n prod -l app=payment-api`
4. Check dependencies: [list monitoring links]
5. Check metrics: [Prometheus queries]

## Resolution Steps
### If caused by recent deployment:
```bash
# Rollback
kubectl rollout undo deployment/payment-api -n prod

# Verify
kubectl rollout status deployment/payment-api -n prod

# Check metrics recovering
# [Prometheus/Grafana link]
```

### If caused by dependency:
```bash
# Check dependency health
curl https://dependency-api/health

# If unhealthy, restart
kubectl rollout restart deployment/dependency -n prod
```

### If caused by resource exhaustion:
```bash
# Scale up
kubectl scale deployment/payment-api -n prod --replicas=10

# Check HPA
kubectl get hpa -n prod
```

## Escalation
- **Primary:** @payments-team
- **Secondary:** @platform-team
- **Manager:** @engineering-manager

## Communication
- **Slack:** #incidents
- **Status Page:** Update at status.company.com
- **Template:** "We're investigating issues with [service]. ETA for resolution: [time]"

## Post-Incident
- [ ] Create Jira ticket
- [ ] Schedule post-mortem
- [ ] Update runbook with learnings
- [ ] Review SLO impact

## Related Links
- Dashboard: [Grafana link]
- Logs: [Loki/Kibana link]
- Traces: [Jaeger link]
- Playbook: [Confluence link]
- Previous incidents: [Links]
````

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 8

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É SLI/SLO/SLA
‚úÖ –†–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å Error Budget
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å multi-window multi-burn-rate alerts
‚úÖ –í—ã–±–∏—Ä–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ SLI –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å SLO-based alerting –≤ Prometheus
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO dashboards –≤ Grafana
‚úÖ –ü–∏—Å–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ post-mortem –¥–æ–∫—É–º–µ–Ω—Ç—ã
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Error Budget –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
‚úÖ –ò–∑–º–µ—Ä—è—Ç—å –∏ —Å–Ω–∏–∂–∞—Ç—å toil
‚úÖ –ü—Ä–æ–≤–æ–¥–∏—Ç—å reliability reviews
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å runbooks –¥–ª—è on-call
‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å SRE –ø—Ä–∏–Ω—Ü–∏–ø—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã SRE:**
1. –ò–∑–º–µ—Ä—è–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ (SLI/SLO)
2. Error Budget = –ø—Ä–∞–≤–æ –Ω–∞ –æ—à–∏–±–∫—É
3. –ë–∞–ª–∞–Ω—Å: velocity vs stability
4. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è > —Ä—É—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞
5. –ü–æ—Å—Ç–º–æ—Ä—Ç–µ–º—ã –±–µ–∑ –æ–±–≤–∏–Ω–µ–Ω–∏–π
6. Toil < 50% –≤—Ä–µ–º–µ–Ω–∏
7. On-call –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å sustainable

**SLO Formula:**

Availability SLO = successful_requests / total_requests >= target
Error Budget = 1 - SLO Example: 99.9% SLO ‚Üí 0.1% error budget ‚Üí 43.2 min/month downtime
Burn Rate = actual_error_rate / budgeted_error_rate

**Production Checklist:**
- ‚úÖ SLO –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
- ‚úÖ SLI –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∏ –º–æ–Ω–∏—Ç–æ—Ä—è—Ç—Å—è
- ‚úÖ Error budget tracking dashboard
- ‚úÖ Multi-window burn rate alerts
- ‚úÖ Runbooks –¥–ª—è –≤—Å–µ—Ö alerts
- ‚úÖ Post-mortem –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞–ª–∞–∂–µ–Ω
- ‚úÖ On-call rotation —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—ã–π
- ‚úÖ Toil –∏–∑–º–µ—Ä—è–µ—Ç—Å—è –∏ —Å–Ω–∏–∂–∞–µ—Ç—Å—è
- ‚úÖ Incident response –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω
- ‚úÖ Regular SLO reviews



## –ú–æ–¥—É–ª—å 9: Security Monitoring –∏ Audit Logging - –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –∫–æ–º–ø–ª–∞–µ–Ω—Å (45 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Security Monitoring - —á—Ç–æ —ç—Ç–æ:**

```
Security Monitoring = –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ —É–≥—Ä–æ–∑

–û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
1. Access Control - –∫—Ç–æ –∏ –∫—É–¥–∞ –∑–∞—Ö–æ–¥–∏—Ç
2. Vulnerability Detection - –ø–æ–∏—Å–∫ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
3. Intrusion Detection - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—Ç–æ—Ä–∂–µ–Ω–∏–π
4. Compliance - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
5. Audit Logging - –∂—É—Ä–Ω–∞–ª –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π
6. Threat Detection - –≤—ã—è–≤–ª–µ–Ω–∏–µ —É–≥—Ä–æ–∑
```

**CIA Triad - –æ—Å–Ω–æ–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Confidentiality (–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü.)   ‚îÇ  - –¢–æ–ª—å–∫–æ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø
‚îÇ          ‚ñ≤                       ‚îÇ
‚îÇ          ‚îÇ                       ‚îÇ
‚îÇ          ‚îÇ                       ‚îÇ
‚îÇ  Integrity ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Availability   ‚îÇ  - –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å ‚óÑ‚îÄ‚îÄ‚ñ∫ –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
‚îÇ                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–æ–ª–∂–µ–Ω –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å –≤—Å–µ —Ç—Ä–∏ –∞—Å–ø–µ–∫—Ç–∞!
```

**–¢–∏–ø—ã security —Å–æ–±—ã—Ç–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
Authentication Events:
- Failed login attempts
- Brute force attacks
- Privilege escalation
- Account lockouts
- Password changes

Authorization Events:
- Unauthorized access attempts
- Permission changes
- Role modifications
- Policy violations

Network Events:
- Port scanning
- DDoS attacks
- Unusual traffic patterns
- Connection from blacklisted IPs
- Data exfiltration attempts

Application Events:
- SQL injection attempts
- XSS attacks
- API abuse
- Unusual API calls
- File upload attacks

System Events:
- Root/admin logins
- System file modifications
- Service starts/stops
- Configuration changes
- Package installations

Data Events:
- Sensitive data access
- Data exports
- Encryption key usage
- Database queries with PII
- File downloads
```

**SIEM (Security Information and Event Management):**

```
SIEM = Centralized security monitoring

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Log Collection                   ‚îÇ  - –°–±–æ—Ä –ª–æ–≥–æ–≤ —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Normalization                    ‚îÇ  - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Correlation                      ‚îÇ  - –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å–æ–±—ã—Ç–∏–π
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Analysis                         ‚îÇ  - –ê–Ω–∞–ª–∏–∑ –Ω–∞ —É–≥—Ä–æ–∑—ã
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Alerting                         ‚îÇ  - –û–ø–æ–≤–µ—â–µ–Ω–∏—è –æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–∞—Ö
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Reporting                        ‚îÇ  - –û—Ç—á–µ—Ç—ã –¥–ª—è compliance
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ SIEM:
- Splunk (commercial)
- Elastic Security (ELK)
- Wazuh (open source)
- Graylog (open source)
- IBM QRadar (commercial)
```

**Audit Logging - —á—Ç–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å:**

```
WHO - –ö—Ç–æ –≤—ã–ø–æ–ª–Ω–∏–ª –¥–µ–π—Å—Ç–≤–∏–µ
‚îú‚îÄ User ID
‚îú‚îÄ Session ID
‚îú‚îÄ IP Address
‚îî‚îÄ User Agent

WHAT - –ß—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ
‚îú‚îÄ Action type (CREATE, READ, UPDATE, DELETE)
‚îú‚îÄ Resource type (user, file, database)
‚îú‚îÄ Resource ID
‚îî‚îÄ Operation status (success/failure)

WHEN - –ö–æ–≥–¥–∞ –ø—Ä–æ–∏–∑–æ—à–ª–æ
‚îú‚îÄ Timestamp (UTC)
‚îú‚îÄ Duration
‚îî‚îÄ Sequence number

WHERE - –ì–¥–µ –ø—Ä–æ–∏–∑–æ—à–ª–æ
‚îú‚îÄ Service/application
‚îú‚îÄ Server/container
‚îú‚îÄ Geographic location
‚îî‚îÄ Network zone

WHY - –ö–æ–Ω—Ç–µ–∫—Å—Ç
‚îú‚îÄ Request ID
‚îú‚îÄ Session context
‚îú‚îÄ Business reason
‚îî‚îÄ Approval ID (if applicable)

RESULT - –†–µ–∑—É–ª—å—Ç–∞—Ç
‚îú‚îÄ Status code
‚îú‚îÄ Error message
‚îú‚îÄ Data changed (before/after)
‚îî‚îÄ Side effects
```

**Audit log format (JSON):**

json

````json
{
  "timestamp": "2025-01-15T10:30:00.000Z",
  "event_id": "evt_abc123",
  "event_type": "user.login.success",
  "severity": "info",
  
  "actor": {
    "user_id": "user_123",
    "username": "john.doe",
    "email": "john.doe@example.com",
    "ip_address": "192.168.1.100",
    "user_agent": "Mozilla/5.0...",
    "session_id": "sess_xyz789"
  },
  
  "action": {
    "type": "authentication",
    "operation": "login",
    "resource_type": "session",
    "resource_id": "sess_xyz789",
    "status": "success"
  },
  
  "context": {
    "service": "auth-service",
    "server": "auth-01",
    "region": "eu-west-1",
    "environment": "production",
    "request_id": "req_456"
  },
  
  "metadata": {
    "mfa_used": true,
    "login_method": "password",
    "previous_login": "2025-01-14T08:00:00Z",
    "risk_score": 0.1
  }
}
```

**Compliance —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã:**
```
PCI DSS (Payment Card Industry):
- –õ–æ–≥–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –º–∏–Ω–∏–º—É–º 1 –≥–æ–¥
- –ó–∞—â–∏—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç
- –†–µ–≥—É–ª—è—Ä–Ω—ã–µ security –∞—É–¥–∏—Ç—ã
- Access control –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

GDPR (General Data Protection Regulation):
- –ü—Ä–∞–≤–æ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- Data breach notification (72 hours)
- Audit trail –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å PII
- Encryption at rest and in transit

HIPAA (Healthcare):
- –ó–∞—â–∏—Ç–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- Audit logs –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–æ–≤ –∫ PHI
- Encryption required
- Access control

SOC 2:
- Security controls
- Availability monitoring
- Confidentiality
- Processing integrity

ISO 27001:
- Information security management
- Risk assessment
- Incident management
- Continuous monitoring
````

**Authentication monitoring:**

promql

````promql
# Failed login attempts (possible brute force)
sum(rate(auth_login_attempts_total{status="failed"}[5m])) by (username, ip) > 5

# Successful login from new location
auth_login_success{location!~"known_locations"}

# Multiple failed logins followed by success (credential stuffing)
(
  sum(increase(auth_login_attempts_total{status="failed"}[5m])) by (username) > 10
  and
  sum(increase(auth_login_attempts_total{status="success"}[5m])) by (username) > 0
)

# Login from multiple IPs simultaneously
count(auth_login_success) by (username) > 2

# Privileged account access
auth_login_success{role=~"admin|root|superuser"}
```

**API security monitoring:**
```
–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

Rate Limiting:
- Requests per IP
- Requests per user
- Requests per endpoint

Anomaly Detection:
- Unusual request patterns
- Spike in errors
- New endpoints accessed
- Unusual request size

Authentication:
- Invalid token attempts
- Expired token usage
- Missing authentication
- Token reuse

Authorization:
- 403 Forbidden errors
- Permission escalation attempts
- Cross-tenant access attempts

Input Validation:
- SQL injection patterns
- XSS attempts
- Path traversal
- Command injection
````

**Network security monitoring:**

bash

````bash
# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–µ—Ç–∏:

Connection tracking:
- New connections per second
- Connection states (ESTABLISHED, SYN_SENT, etc)
- Connections per IP
- Unusual ports accessed

Traffic analysis:
- Bandwidth usage
- Protocol distribution (HTTP, HTTPS, SSH, etc)
- Traffic to/from suspicious IPs
- DNS queries patterns

Firewall events:
- Blocked connections
- Rule violations
- Port scan attempts
- Failed connection attempts

IDS/IPS events:
- Signature matches
- Anomaly detection
- Blocked attacks
- False positives
```

**Container security:**
```
Security scanning:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Image Scanning                ‚îÇ
‚îÇ   - Trivy, Clair, Anchore       ‚îÇ
‚îÇ   - CVE detection               ‚îÇ
‚îÇ   - License compliance          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Runtime Security              ‚îÇ
‚îÇ   - Falco (syscall monitoring)  ‚îÇ
‚îÇ   - AppArmor/SELinux            ‚îÇ
‚îÇ   - Pod Security Standards      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Network Policies              ‚îÇ
‚îÇ   - Ingress/Egress rules        ‚îÇ
‚îÇ   - Service mesh security       ‚îÇ
‚îÇ   - mTLS enforcement            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Secrets Management            ‚îÇ
‚îÇ   - Vault, Sealed Secrets       ‚îÇ
‚îÇ   - Rotation policies           ‚îÇ
‚îÇ   - Access auditing             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
````

**Falco - runtime security –¥–ª—è Kubernetes:**

yaml

```yaml
# Falco –ø—Ä–∞–≤–∏–ª–∞

# Detect shell in container
- rule: Terminal shell in container
  desc: A shell was used as the entrypoint/exec point
  condition: >
    spawned_process and container
    and shell_procs
    and proc.tty != 0
  output: >
    Shell spawned in container
    (user=%user.name container_id=%container.id
    container_name=%container.name shell=%proc.name
    parent=%proc.pname cmdline=%proc.cmdline)
  priority: WARNING

# Detect sensitive file access
- rule: Read sensitive file
  desc: Attempt to read sensitive files
  condition: >
    open_read and container
    and fd.name in (sensitive_files)
  output: >
    Sensitive file opened for reading
    (user=%user.name file=%fd.name
    container=%container.name)
  priority: WARNING

# Detect privilege escalation
- rule: Privilege Escalation
  desc: Detect setuid or setgid
  condition: >
    spawned_process and container
    and (proc.name in (setuid_binaries))
  output: >
    Privilege escalation detected
    (user=%user.name proc=%proc.name
    container=%container.name)
  priority: CRITICAL
```

**Vulnerability scanning:**

bash

```bash
# Trivy - —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤
trivy image nginx:latest

# –†–µ–∑—É–ª—å—Ç–∞—Ç:
Total: 145 (UNKNOWN: 0, LOW: 45, MEDIUM: 75, HIGH: 20, CRITICAL: 5)

# –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ filesystem
trivy fs /path/to/project

# Kubernetes —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
trivy k8s --report summary cluster

# Integration –≤ CI/CD
trivy image --severity CRITICAL,HIGH --exit-code 1 myapp:latest
```

**Security metrics (Prometheus):**

promql

````promql
# Failed authentication rate
rate(auth_failed_attempts_total[5m])

# 403 Forbidden rate (authorization failures)
rate(http_requests_total{status="403"}[5m])

# 401 Unauthorized rate
rate(http_requests_total{status="401"}[5m])

# Suspicious IPs
count(http_requests_total{ip=~"suspicious_ip_list"})

# Certificate expiration
(cert_expiry_timestamp_seconds - time()) / 86400 < 30  # < 30 days

# Vulnerability count by severity
sum(trivy_vulnerabilities) by (severity)

# Security scan failures
rate(security_scan_failures_total[5m])

# WAF blocks
rate(waf_blocked_requests_total[5m])

# Rate limit violations
rate(rate_limit_exceeded_total[5m])
```

**WAF (Web Application Firewall) monitoring:**
```
ModSecurity/OWASP Core Rule Set:

–¢–∏–ø—ã –∞—Ç–∞–∫ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è:
- SQL Injection
- XSS (Cross-Site Scripting)
- CSRF (Cross-Site Request Forgery)
- Path Traversal
- Command Injection
- XXE (XML External Entity)
- File Upload attacks
- Session Hijacking

–ú–µ—Ç—Ä–∏–∫–∏ WAF:
- Blocked requests by rule
- False positive rate
- Top attacking IPs
- Attack type distribution
- Response time impact
````

**Secrets detection:**

bash

````bash
# Detect secrets in code/logs
trufflehog git https://github.com/example/repo

# Common patterns to detect:
- AWS keys: AKIA[0-9A-Z]{16}
- GitHub tokens: ghp_[0-9a-zA-Z]{36}
- Private keys: -----BEGIN PRIVATE KEY-----
- Passwords in code: password = "..."
- API keys: api_key = "..."
- Database URLs: postgres://user:pass@host

# Gitleaks - alternative
gitleaks detect --source . --verbose

# Integration –≤ pre-commit hook
```

**Data Loss Prevention (DLP):**
```
DLP Monitoring:

Sensitive data patterns:
- Credit card numbers (PCI)
- Social Security Numbers
- Email addresses
- Phone numbers
- API keys / tokens
- Personal Health Information (PHI)

Monitoring points:
- File uploads
- API responses
- Database queries
- Email attachments
- Logs (prevent leaking secrets)
- External integrations

Actions:
- Block transmission
- Mask/redact data
- Alert security team
- Log incident
````

**Compliance reporting:**

sql

````sql
-- Audit queries for compliance reports

-- Who accessed user data (GDPR)
SELECT 
  timestamp,
  actor_user_id,
  action_type,
  resource_type,
  resource_id,
  status
FROM audit_logs
WHERE resource_type = 'user_data'
  AND timestamp > NOW() - INTERVAL '90 days'
ORDER BY timestamp DESC;

-- Failed access attempts (PCI DSS)
SELECT 
  DATE_TRUNC('day', timestamp) as date,
  COUNT(*) as failed_attempts,
  actor_ip_address
FROM audit_logs
WHERE action_type = 'authentication'
  AND status = 'failed'
  AND timestamp > NOW() - INTERVAL '1 year'
GROUP BY date, actor_ip_address
HAVING COUNT(*) > 5
ORDER BY failed_attempts DESC;

-- Privileged access report (SOC 2)
SELECT 
  actor_user_id,
  COUNT(*) as access_count,
  MAX(timestamp) as last_access
FROM audit_logs
WHERE actor_role IN ('admin', 'root', 'superuser')
  AND timestamp > NOW() - INTERVAL '30 days'
GROUP BY actor_user_id
ORDER BY access_count DESC;

-- Data modification audit (ISO 27001)
SELECT 
  timestamp,
  actor_user_id,
  resource_type,
  resource_id,
  action_type,
  metadata->>'before' as before_value,
  metadata->>'after' as after_value
FROM audit_logs
WHERE action_type IN ('UPDATE', 'DELETE')
  AND timestamp > NOW() - INTERVAL '7 days'
ORDER BY timestamp DESC;
```

**Incident response workflow:**
```
Detection ‚Üí Triage ‚Üí Investigation ‚Üí Containment ‚Üí Eradication ‚Üí Recovery ‚Üí Post-Incident

1. Detection (Automated)
   ‚îú‚îÄ SIEM alert fires
   ‚îú‚îÄ Anomaly detected
   ‚îî‚îÄ User report

2. Triage (5-15 min)
   ‚îú‚îÄ Verify alert (true positive?)
   ‚îú‚îÄ Assess severity
   ‚îú‚îÄ Assign incident owner
   ‚îî‚îÄ Create incident ticket

3. Investigation (15-60 min)
   ‚îú‚îÄ Analyze logs
   ‚îú‚îÄ Check timeline
   ‚îú‚îÄ Identify affected systems
   ‚îú‚îÄ Determine attack vector
   ‚îî‚îÄ Assess impact

4. Containment (Immediate)
   ‚îú‚îÄ Isolate affected systems
   ‚îú‚îÄ Block malicious IPs
   ‚îú‚îÄ Disable compromised accounts
   ‚îú‚îÄ Prevent spread
   ‚îî‚îÄ Preserve evidence

5. Eradication (Hours-Days)
   ‚îú‚îÄ Remove malware
   ‚îú‚îÄ Close vulnerabilities
   ‚îú‚îÄ Patch systems
   ‚îî‚îÄ Reset credentials

6. Recovery (Hours-Days)
   ‚îú‚îÄ Restore from backups
   ‚îú‚îÄ Verify system integrity
   ‚îú‚îÄ Monitor for reinfection
   ‚îî‚îÄ Resume normal operations

7. Post-Incident (Days-Weeks)
   ‚îú‚îÄ Incident report
   ‚îú‚îÄ Lessons learned
   ‚îú‚îÄ Update runbooks
   ‚îú‚îÄ Improve detection
   ‚îî‚îÄ Train team
```

**Security alert priorities:**
```
P0 - Critical (Page immediately)
‚îú‚îÄ Active breach detected
‚îú‚îÄ Data exfiltration in progress
‚îú‚îÄ Ransomware detected
‚îú‚îÄ Root/admin compromise
‚îî‚îÄ DDoS attack

P1 - High (Alert within 15 min)
‚îú‚îÄ Multiple failed auth from same IP
‚îú‚îÄ Privilege escalation attempt
‚îú‚îÄ Suspicious admin activity
‚îú‚îÄ Vulnerability exploitation attempt
‚îî‚îÄ Certificate about to expire

P2 - Medium (Alert within 1 hour)
‚îú‚îÄ Port scan detected
‚îú‚îÄ Unusual API usage
‚îú‚îÄ Policy violation
‚îú‚îÄ Configuration drift
‚îî‚îÄ Suspicious login location

P3 - Low (Daily digest)
‚îú‚îÄ Informational security events
‚îú‚îÄ Compliance audit findings
‚îú‚îÄ Low severity vulnerabilities
‚îî‚îÄ Best practice recommendations
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É security monitoring:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ Falco –¥–ª—è runtime security**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Falco –≤ Kubernetes
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

kubectl create namespace falco

helm install falco falcosecurity/falco \
  --namespace falco \
  --set falcosidekick.enabled=true \
  --set falcosidekick.webui.enabled=true

# –ü—Ä–æ–≤–µ—Ä–∫–∞
kubectl get pods -n falco
```

2. **–°–æ–∑–¥–∞–π custom Falco rules**:

`k8s-manifests/falco-custom-rules.yaml`:

yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-custom-rules
  namespace: falco
data:
  custom-rules.yaml: |
    # Custom security rules
    
    - rule: Unauthorized Process in Container
      desc: Detect unauthorized process execution
      condition: >
        spawned_process and container
        and not proc.name in (allowed_processes)
      output: >
        Unauthorized process started in container
        (user=%user.name process=%proc.name
        container=%container.name
        parent=%proc.pname cmdline=%proc.cmdline)
      priority: WARNING
      tags: [container, process]
    
    - rule: Sensitive File Access
      desc: Detect access to sensitive files
      condition: >
        open_read and container
        and (fd.name startswith /etc/shadow or
             fd.name startswith /etc/passwd or
             fd.name contains /id_rsa or
             fd.name contains /authorized_keys)
      output: >
        Sensitive file accessed
        (user=%user.name file=%fd.name
        container=%container.name
        process=%proc.name)
      priority: CRITICAL
      tags: [file, credentials]
    
    - rule: Reverse Shell Detected
      desc: Detect reverse shell attempts
      condition: >
        spawned_process and container
        and proc.name in (shell_binaries)
        and (proc.args contains ">" or proc.args contains "&")
        and fd.name contains "/dev/tcp"
      output: >
        Potential reverse shell detected
        (user=%user.name container=%container.name
        process=%proc.name args=%proc.args)
      priority: CRITICAL
      tags: [shell, network]
    
    - rule: Crypto Mining Activity
      desc: Detect cryptocurrency mining
      condition: >
        spawned_process and container
        and (proc.name in (xmrig, minerd, ccminer, ethminer) or
             proc.cmdline contains "stratum+tcp" or
             proc.cmdline contains "pool.minergate.com")
      output: >
        Cryptocurrency mining detected
        (user=%user.name container=%container.name
        process=%proc.name cmdline=%proc.cmdline)
      priority: CRITICAL
      tags: [malware, mining]
    
    - rule: Container Drift Detected
      desc: Detect executable created in container
      condition: >
        container and (
          open_write and
          fd.name startswith /bin/ or
          fd.name startswith /usr/bin/
        )
      output: >
        Executable file created in container (drift)
        (user=%user.name file=%fd.name
        container=%container.name)
      priority: ERROR
      tags: [container, drift]
    
    - rule: Privileged Container Launch
      desc: Detect privileged container
      condition: >
        container_started and container.privileged=true
      output: >
        Privileged container started
        (user=%user.name container=%container.name
        image=%container.image.repository)
      priority: WARNING
      tags: [container, privileges]
    
    - rule: SSH Connection from Container
      desc: Detect outbound SSH from container
      condition: >
        outbound and container
        and fd.sport != 22
        and fd.dport = 22
      output: >
        Outbound SSH connection from container
        (user=%user.name container=%container.name
        dest_ip=%fd.rip dest_port=%fd.rport)
      priority: WARNING
      tags: [network, ssh]
    
    - rule: Package Manager in Container
      desc: Detect package installation in running container
      condition: >
        spawned_process and container
        and proc.name in (apt, apt-get, yum, dnf, apk, pip, npm)
      output: >
        Package manager executed in container
        (user=%user.name container=%container.name
        process=%proc.name args=%proc.args)
      priority: ERROR
      tags: [container, package]
```

3. **–°–æ–∑–¥–∞–π audit logging system**:

`audit-logging/audit-logger.py`:

python

```python
from flask import Flask, request, g
from functools import wraps
import json
import time
import hashlib
from datetime import datetime
import logging

app = Flask(__name__)

# Audit log configuration
audit_logger = logging.getLogger('audit')
audit_logger.setLevel(logging.INFO)
handler = logging.FileHandler('/var/log/audit/audit.log')
formatter = logging.Formatter('%(message)s')
handler.setFormatter(formatter)
audit_logger.addHandler(handler)

# Prometheus metrics
from prometheus_client import Counter, Histogram

AUDIT_EVENTS = Counter(
    'audit_events_total',
    'Total audit events',
    ['event_type', 'status', 'severity']
)

AUDIT_DURATION = Histogram(
    'audit_event_duration_seconds',
    'Audit event processing time',
    ['event_type']
)

def get_client_ip():
    """Get real client IP"""
    if request.headers.get('X-Forwarded-For'):
        return request.headers.get('X-Forwarded-For').split(',')[0]
    return request.remote_addr

def get_user_context():
    """Get current user context"""
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ —ç—Ç–æ –∏–∑ JWT token –∏–ª–∏ session
    return {
        'user_id': g.get('user_id', 'anonymous'),
        'username': g.get('username', 'anonymous'),
        'email': g.get('email', None),
        'roles': g.get('roles', []),
        'session_id': g.get('session_id', None)
    }

def create_audit_event(event_type, action, resource_type, resource_id, 
                       status, severity='info', metadata=None):
    """Create structured audit log entry"""
    
    user_context = get_user_context()
    
    audit_event = {
        'timestamp': datetime.utcnow().isoformat() + 'Z',
        'event_id': hashlib.sha256(
            f"{time.time()}{event_type}{user_context['user_id']}".encode()
        ).hexdigest()[:16],
        'event_type': event_type,
        'severity': severity,
        
        'actor': {
            'user_id': user_context['user_id'],
            'username': user_context['username'],
            'email': user_context['email'],
            'roles': user_context['roles'],
            'ip_address': get_client_ip(),
            'user_agent': request.headers.get('User-Agent'),
            'session_id': user_context['session_id']
        },
        
        'action': {
            'type': action,
            'resource_type': resource_type,
            'resource_id': resource_id,
            'status': status
        },
        
        'context': {
            'service': 'audit-api',
            'request_id': g.get('request_id'),
            'request_method': request.method,
            'request_path': request.path,
            'request_query': dict(request.args)
        },
        
        'metadata': metadata or {}
    }
    
    # Log to file
    audit_logger.info(json.dumps(audit_event))
    
    # Metrics
    AUDIT_EVENTS.labels(
        event_type=event_type,
        status=status,
        severity=severity
    ).inc()
    
    return audit_event

def audit_log(event_type, resource_type):
    """Decorator for automatic audit logging"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            start_time = time.time()
            resource_id = kwargs.get('resource_id', 'unknown')
            
            try:
                result = f(*args, **kwargs)
                
                # Success audit
                create_audit_event(
                    event_type=event_type,
                    action=request.method,
                    resource_type=resource_type,
                    resource_id=resource_id,
                    status='success',
                    severity='info',
                    metadata={
                        'duration_ms': (time.time() - start_time) * 1000
                    }
                )
                
                return result
                
            except Exception as e:
                # Failure audit
                create_audit_event(
                    event_type=event_type,
                    action=request.method,
                    resource_type=resource_type,
                    resource_id=resource_id,
                    status='failure',
                    severity='error',
                    metadata={
                        'error': str(e),
                        'duration_ms': (time.time() - start_time) * 1000
                    }
                )
                raise
            
            finally:
                AUDIT_DURATION.labels(event_type=event_type).observe(
                    time.time() - start_time
                )
        
        return decorated_function
    return decorator

# Example protected endpoints

@app.route('/api/user/<user_id>', methods=['GET'])
@audit_log(event_type='user.read', resource_type='user')
def get_user(user_id):
    """Get user - audited endpoint"""
    # Check if accessing sensitive PII
    if 'ssn' in request.args or 'credit_card' in request.args:
        create_audit_event(
            event_type='pii.access',
            action='READ',
            resource_type='user',
            resource_id=user_id,
            status='success',
            severity='warning',
            metadata={'fields_accessed': list(request.args.keys())}
        )
    
    return {'user_id': user_id, 'name': 'Test User'}

@app.route('/api/user/<user_id>', methods=['PUT'])
@audit_log(event_type='user.update', resource_type='user')
def update_user(user_id):
    """Update user - audited with before/after"""
    data = request.json
    
    # Log before and after for critical changes
    if 'email' in data or 'role' in data:
        create_audit_event(
            event_type='user.critical_update',
            action='UPDATE',
            resource_type='user',
            resource_id=user_id,
            status='success',
            severity='warning',
            metadata={
                'changes': data,
                'before': {'email': 'old@example.com', 'role': 'user'},
                'after': data
            }
        )
    
    return {'status': 'updated'}

@app.route('/api/user/<user_id>', methods=['DELETE'])
@audit_log(event_type='user.delete', resource_type='user')
def delete_user(user_id):
    """Delete user - critical audit event"""
    create_audit_event(
        event_type='user.delete',
        action='DELETE',
        resource_type='user',
        resource_id=user_id,
        status='success',
        severity='critical',
        metadata={
            'permanent': True,
            'reason': request.args.get('reason', 'not_specified')
        }
    )
    
    return {'status': 'deleted'}

@app.route('/api/admin/role', methods=['POST'])
@audit_log(event_type='role.grant', resource_type='permission')
def grant_role():
    """Grant admin role - security critical"""
    data = request.json
    
    create_audit_event(
        event_type='privilege.escalation',
        action='GRANT',
        resource_type='permission',
        resource_id=data.get('role'),
        status='success',
        severity='critical',
        metadata={
            'target_user': data.get('user_id'),
            'role_granted': data.get('role'),
            'granted_by': get_user_context()['user_id']
        }
    )
    
    return {'status': 'role_granted'}

@app.route('/api/export/data', methods=['POST'])
@audit_log(event_type='data.export', resource_type='data')
def export_data():
    """Data export - DLP monitoring"""
    data = request.json
    
    create_audit_event(
        event_type='data.export',
        action='EXPORT',
        resource_type='data',
        resource_id='bulk_export',
        status='success',
        severity='warning',
        metadata={
            'record_count': data.get('count', 0),
            'data_types': data.get('types', []),
            'destination': data.get('destination'),
            'format': data.get('format')
        }
    )
    
    return {'status': 'exported'}

# Security events

@app.route('/auth/login', methods=['POST'])
def login():
    """Login with audit"""
    data = request.json
    username = data.get('username')
    
    # Simulate authentication
    import random
    success = random.random() > 0.1  # 90% success rate
    
    if success:
        create_audit_event(
            event_type='auth.login.success',
            action='AUTHENTICATE',
            resource_type='session',
            resource_id='sess_' + hashlib.md5(username.encode()).hexdigest()[:8],
            status='success',
            severity='info',
            metadata={
                'method': 'password',
                'mfa_used': data.get('mfa', False)
            }
        )
        return {'status': 'success', 'token': 'fake_token'}
    else:
        create_audit_event(
            event_type='auth.login.failure',
            action='AUTHENTICATE',
            resource_type='session',
            resource_id='unknown',
            status='failure',
            severity='warning',
			metadata={ 'method': 'password', 'reason': 'invalid_credentials', 'username_attempted': username } ) return {'status': 'failed', 'error': 'Invalid credentials'}, 401

@app.route('/api/config', methods=['PUT']) @audit_log(event_type='config.change', resource_type='configuration') def update_config(): """Configuration change - critical audit""" data = request.json


create_audit_event(
    event_type='config.change',
    action='UPDATE',
    resource_type='configuration',
    resource_id=data.get('key'),
    status='success',
    severity='critical',
    metadata={
        'key': data.get('key'),
        'old_value': '***REDACTED***',  # Never log secrets
        'new_value': '***REDACTED***',
        'change_type': 'manual'
    }
)

return {'status': 'config_updated'}


if **name** == '**main**': app.run(host='0.0.0.0', port=5000)

````

4. **–°–æ–∑–¥–∞–π security monitoring dashboards**:

`k8s-manifests/security-prometheus-rules.yaml`:
```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: security-alerts
  namespace: monitoring
spec:
  groups:
  - name: security.rules
    interval: 30s
    rules:
    # Authentication alerts
    - alert: BruteForceAttempt
      expr: |
        sum(rate(audit_events_total{event_type="auth.login.failure"}[5m])) by (actor_ip_address) > 10
      for: 2m
      labels:
        severity: critical
        category: security
      annotations:
        summary: "Brute force attack detected"
        description: "IP {{ $labels.actor_ip_address }} has {{ $value }} failed login attempts/sec"
        runbook: "Block IP and investigate"
    
    - alert: MultipleFailedLoginsBeforeSuccess
      expr: |
        (
          sum(increase(audit_events_total{event_type="auth.login.failure"}[5m])) by (actor_user_id) > 5
          and
          sum(increase(audit_events_total{event_type="auth.login.success"}[5m])) by (actor_user_id) > 0
        )
      for: 1m
      labels:
        severity: warning
        category: security
      annotations:
        summary: "Possible credential stuffing"
        description: "User {{ $labels.actor_user_id }} had multiple failed attempts then success"
    
    # Privilege escalation
    - alert: PrivilegeEscalation
      expr: |
        sum(rate(audit_events_total{event_type="privilege.escalation"}[5m])) > 0
      for: 1m
      labels:
        severity: critical
        category: security
      annotations:
        summary: "Privilege escalation detected"
        description: "Admin role granted to user"
    
    # Data exfiltration
    - alert: MassiveDataExport
      expr: |
        sum(rate(audit_events_total{event_type="data.export"}[5m])) > 10
      for: 5m
      labels:
        severity: critical
        category: security
      annotations:
        summary: "Possible data exfiltration"
        description: "Unusual volume of data exports detected"
    
    # PII access
    - alert: UnauthorizedPIIAccess
      expr: |
        sum(rate(audit_events_total{event_type="pii.access",status="failure"}[5m])) > 0
      for: 1m
      labels:
        severity: critical
        category: security
      annotations:
        summary: "Unauthorized PII access attempt"
        description: "Failed attempt to access sensitive personal data"
    
    # Configuration changes
    - alert: CriticalConfigurationChange
      expr: |
        sum(increase(audit_events_total{event_type="config.change",severity="critical"}[5m])) > 0
      for: 1m
      labels:
        severity: warning
        category: security
      annotations:
        summary: "Critical configuration changed"
        description: "System configuration was modified"
    
    # Anomalous behavior
    - alert: UnusualAPIActivity
      expr: |
        sum(rate(http_requests_total[5m])) by (user_id) >
        (avg_over_time(sum(rate(http_requests_total[5m])) by (user_id)[1h]) * 3)
      for: 10m
      labels:
        severity: warning
        category: security
      annotations:
        summary: "Unusual API activity detected"
        description: "User {{ $labels.user_id }} has 3x normal API activity"
    
    # Certificate expiration
    - alert: CertificateExpiringSoon
      expr: |
        (ssl_certificate_expiry_timestamp - time()) / 86400 < 30
      for: 1h
      labels:
        severity: warning
        category: security
      annotations:
        summary: "SSL certificate expiring soon"
        description: "Certificate {{ $labels.cn }} expires in {{ $value }} days"
    
    # Falco security events
    - alert: FalcoSecurityEvent
      expr: |
        sum(rate(falco_events_total{priority=~"Critical|Error"}[5m])) > 0
      for: 1m
      labels:
        severity: critical
        category: security
      annotations:
        summary: "Falco security event detected"
        description: "Runtime security violation: {{ $labels.rule }}"
    
    # WAF blocks
    - alert: HighWAFBlockRate
      expr: |
        sum(rate(waf_blocked_requests_total[5m])) > 10
      for: 5m
      labels:
        severity: warning
        category: security
      annotations:
        summary: "High WAF block rate"
        description: "WAF is blocking {{ $value }} requests/sec"
```

5. **Deploy audit system**:
```bash
# Build audit logger
cd audit-logging
docker build -t audit-logger:latest .
kind load docker-image audit-logger:latest --name monitoring-cluster

# Deploy
kubectl create namespace security
kubectl apply -f k8s-manifests/audit-logger-deployment.yaml
kubectl apply -f k8s-manifests/security-prometheus-rules.yaml
kubectl apply -f k8s-manifests/falco-custom-rules.yaml
```

`k8s-manifests/audit-logger-deployment.yaml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audit-logger
  namespace: security
spec:
  replicas: 2
  selector:
    matchLabels:
      app: audit-logger
  template:
    metadata:
      labels:
        app: audit-logger
    spec:
      containers:
      - name: app
        image: audit-logger:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 5000
        volumeMounts:
        - name: audit-logs
          mountPath: /var/log/audit
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
      volumes:
      - name: audit-logs
        persistentVolumeClaim:
          claimName: audit-logs-pvc

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: audit-logs-pvc
  namespace: security
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: audit-logger
  namespace: security
spec:
  selector:
    app: audit-logger
  ports:
  - port: 5000
    targetPort: 5000

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: audit-logger
  namespace: security
spec:
  selector:
    matchLabels:
      app: audit-logger
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
```

6. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ security monitoring**:
```bash
# Test authentication failures (brute force)
for i in {1..20}; do
  curl -X POST http://localhost:5000/auth/login \
    -H "Content-Type: application/json" \
    -d '{"username":"admin","password":"wrong"}' &
done

# Watch alerts
kubectl logs -n monitoring -l app.kubernetes.io/name=alertmanager -f

# Check Falco events
kubectl logs -n falco -l app.kubernetes.io/name=falco -f

# Test privilege escalation
curl -X POST http://localhost:5000/api/admin/role \
  -H "Content-Type: application/json" \
  -d '{"user_id":"user123","role":"admin"}'

# Test data export
curl -X POST http://localhost:5000/api/export/data \
  -H "Content-Type: application/json" \
  -d '{"count":10000,"types":["user","payment"],"destination":"s3"}'

# Check audit logs
kubectl exec -n security audit-logger-xxx -- tail -f /var/log/audit/audit.log | jq
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Vulnerability scanning automation**:

`vulnerability-scanner.yaml`:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: trivy-scanner
  namespace: security
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: trivy
            image: aquasec/trivy:latest
            args:
            - "image"
            - "--severity"
            - "CRITICAL,HIGH"
            - "--format"
            - "json"
            - "--output"
            - "/reports/trivy-report.json"
            - "myapp:latest"
            volumeMounts:
            - name: reports
              mountPath: /reports
          volumes:
          - name: reports
            persistentVolumeClaim:
              claimName: scan-reports
          restartPolicy: OnFailure
```

**2. Secret scanning in CI/CD**:

`.github/workflows/security-scan.yml`:
```yaml
name: Security Scan

on: [push, pull_request]

jobs:
  secret-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: TruffleHog Secret Scan
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
    
    - name: Gitleaks Scan
      uses: gitleaks/gitleaks-action@v2
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  
  vulnerability-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'
    
    - name: Upload to Security Tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
```

**3. Compliance report generator**:

`compliance-reporter.py`:
```python
#!/usr/bin/env python3
"""
Compliance Report Generator
Supports: PCI DSS, GDPR, HIPAA, SOC 2
"""
import psycopg2
from datetime import datetime, timedelta
import json

class ComplianceReporter:
    def __init__(self, db_conn):
        self.conn = db_conn
    
    def generate_pci_dss_report(self, start_date, end_date):
        """PCI DSS Compliance Report"""
        report = {
            'standard': 'PCI DSS v4.0',
            'period': f"{start_date} to {end_date}",
            'generated': datetime.now().isoformat(),
            'findings': []
        }
        
        # Requirement 10: Track and monitor all access
        cursor = self.conn.cursor()
        
        # 10.2.1: User access to cardholder data
        cursor.execute("""
            SELECT COUNT(*), actor_user_id
            FROM audit_logs
            WHERE event_type LIKE 'payment%'
              AND timestamp BETWEEN %s AND %s
            GROUP BY actor_user_id
        """, (start_date, end_date))
        
        report['findings'].append({
            'requirement': '10.2.1',
            'description': 'User access to cardholder data',
            'access_count': cursor.fetchall()
        })
        
        # 10.2.2: Administrative actions
        cursor.execute("""
            SELECT COUNT(*)
            FROM audit_logs
            WHERE actor_role IN ('admin', 'root')
              AND timestamp BETWEEN %s AND %s
        """, (start_date, end_date))
        
        report['findings'].append({
            'requirement': '10.2.2',
            'description': 'Administrative actions',
            'count': cursor.fetchone()[0]
        })
        
        # 10.2.4: Invalid access attempts
        cursor.execute("""
            SELECT COUNT(*), actor_ip_address
            FROM audit_logs
            WHERE status = 'failure'
              AND event_type LIKE 'auth%'
              AND timestamp BETWEEN %s AND %s
            GROUP BY actor_ip_address
            HAVING COUNT(*) > 5
        """, (start_date, end_date))
        
        report['findings'].append({
            'requirement': '10.2.4',
            'description': 'Invalid access attempts',
            'suspicious_ips': cursor.fetchall()
        })
        
        return report
    
    def generate_gdpr_report(self, start_date, end_date):
        """GDPR Compliance Report"""
        cursor = self.conn.cursor()
        
        report = {
            'regulation': 'GDPR',
            'period': f"{start_date} to {end_date}",
            'generated': datetime.now().isoformat(),
            'data_processing': {}
        }
        
        # Personal data access
        cursor.execute("""
            SELECT 
              COUNT(*) as access_count,
              actor_user_id,
              resource_type
            FROM audit_logs
            WHERE event_type IN ('pii.access', 'user.read')
              AND timestamp BETWEEN %s AND %s
            GROUP BY actor_user_id, resource_type
        """, (start_date, end_date))
        
        report['data_processing']['access'] = cursor.fetchall()
        
        # Data deletions (right to be forgotten)
        cursor.execute("""
            SELECT COUNT(*), metadata->>'reason'
            FROM audit_logs
            WHERE event_type = 'user.delete'
              AND timestamp BETWEEN %s AND %s
            GROUP BY metadata->>'reason'
        """, (start_date, end_date))
        
        report['data_processing']['deletions'] = cursor.fetchall()
        
        # Data breaches (must report within 72h)
        cursor.execute("""
            SELECT *
            FROM audit_logs
            WHERE event_type LIKE 'security.breach%'
              AND timestamp BETWEEN %s AND %s
        """, (start_date, end_date))
        
        report['data_breaches'] = cursor.fetchall()
        
        return report

if __name__ == "__main__":
    conn = psycopg2.connect("postgresql://user:pass@localhost/audit")
    reporter = ComplianceReporter(conn)
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    # Generate reports
    pci_report = reporter.generate_pci_dss_report(start_date, end_date)
    gdpr_report = reporter.generate_gdpr_report(start_date, end_date)
    
    print(json.dumps(pci_report, indent=2))
    print(json.dumps(gdpr_report, indent=2))
```

**4. Security dashboard –≤ Grafana**:

Import dashboard `security-dashboard.json`:
```json
{
  "dashboard": {
    "title": "Security Monitoring Dashboard",
    "tags": ["security", "audit"],
    "panels": [
      {
        "id": 1,
        "title": "Authentication Failures",
        "targets": [
          {
            "expr": "sum(rate(audit_events_total{event_type='auth.login.failure'}[5m])) by (actor_ip_address)",
            "legendFormat": "{{ actor_ip_address }}"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 2,
        "title": "Falco Security Events",
        "targets": [
          {
            "expr": "sum(rate(falco_events_total[5m])) by (rule, priority)",
            "legendFormat": "{{ rule }} ({{ priority }})"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 3,
        "title": "Privilege Escalations",
        "targets": [
          {
            "expr": "sum(increase(audit_events_total{event_type='privilege.escalation'}[1h]))"
          }
        ],
        "type": "stat"
      },
      {
        "id": 4,
        "title": "Top Failed Auth IPs",
        "targets": [
          {
            "expr": "topk(10, sum(increase(audit_events_total{event_type='auth.login.failure'}[24h])) by (actor_ip_address))",
            "format": "table"
          }
        ],
        "type": "table"
      },
      {
        "id": 5,
        "title": "Vulnerability Scan Results",
        "targets": [
          {
            "expr": "sum(trivy_vulnerabilities) by (severity)",
            "legendFormat": "{{ severity }}"
          }
        ],
        "type": "piechart"
      }
    ]
  }
}
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 9

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –æ—Å–Ω–æ–≤—ã security monitoring
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Falco –¥–ª—è runtime security
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å structured audit logs
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å authentication –∏ authorization —Å–æ–±—ã—Ç–∏—è
‚úÖ –û–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å security threats –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å vulnerability scanning
‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å compliance reports
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å security alerts –≤ Prometheus
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SIEM –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
‚úÖ –ü—Ä–æ–≤–æ–¥–∏—Ç—å incident response
‚úÖ –ó–∞—â–∏—â–∞—Ç—å secrets –∏ sensitive data
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å container security

**Security Monitoring Pillars:**
```

1. Prevention - Prevent attacks before they happen
2. Detection - Detect threats in real-time
3. Response - Respond quickly to incidents
4. Recovery - Recover and learn from incidents
5. Compliance - Meet regulatory requirements

```

**Key Security Metrics:**
```

- Failed authentication rate
- Privilege escalation attempts
- Unusual access patterns
- Vulnerability count by severity
- Certificate expiration
- Security scan failures
- Falco rule violations
- WAF block rate

```

**Production Security Checklist:**
- ‚úÖ Audit logging –¥–ª—è –≤—Å–µ—Ö –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- ‚úÖ Falco –∏–ª–∏ –∞–Ω–∞–ª–æ–≥ –¥–ª—è runtime security
- ‚úÖ Vulnerability scanning –≤ CI/CD
- ‚úÖ Secret scanning –ø–µ—Ä–µ–¥ commit
- ‚úÖ Security alerts –≤ Prometheus/Alertmanager
- ‚úÖ SIEM –∏–ª–∏ centralized log analysis
- ‚úÖ Regular compliance reports
- ‚úÖ Incident response runbooks
- ‚úÖ Security dashboard –≤ Grafana
- ‚úÖ Encryption at rest and in transit
- ‚úÖ Regular security audits
- ‚úÖ Penetration testing schedule


## –ú–æ–¥—É–ª—å 10: Advanced Topics - Observability as Code –∏ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è (50 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Observability as Code - —á—Ç–æ —ç—Ç–æ:**

```
Observability as Code = Infrastructure as Code –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

–ü—Ä–∏–Ω—Ü–∏–ø—ã:
1. –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - –≤—Å–µ –≤ Git
2. –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ - DRY principle
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - validate –ø–µ—Ä–µ–¥ apply
4. Automation - CI/CD –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
5. Self-service - –∫–æ–º–∞–Ω–¥—ã —Å–∞–º–∏ —É–ø—Ä–∞–≤–ª—è—é—Ç —Å–≤–æ–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º

–ß—Ç–æ —Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ –∫–æ–¥:
‚îú‚îÄ Dashboards (Grafana JSON)
‚îú‚îÄ Alerts (Prometheus rules)
‚îú‚îÄ Recording rules
‚îú‚îÄ SLO definitions
‚îú‚îÄ Runbooks (Markdown)
‚îî‚îÄ Monitoring configuration
```

**GitOps –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Git Repository                    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ dashboards/                   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ alerts/                       ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ recording-rules/              ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ slo/                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CI/CD Pipeline                    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Validate syntax               ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Test queries                  ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Check best practices          ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Deploy to clusters            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Monitoring Stack                  ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Prometheus                    ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Grafana                       ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Alertmanager                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Jsonnet –¥–ª—è Grafana dashboards:**

jsonnet

```jsonnet
// Reusable dashboard library
local grafana = import 'grafonnet/grafana.libsonnet';
local dashboard = grafana.dashboard;
local prometheus = grafana.prometheus;

// –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ dashboard
local createServiceDashboard(service_name) = 
  dashboard.new(
    'Service Dashboard - ' + service_name,
    tags=['service', service_name],
    refresh='30s',
  )
  .addPanel(
    // CPU panel
    grafana.graphPanel.new(
      'CPU Usage',
      datasource='Prometheus',
      format='percent',
    )
    .addTarget(
      prometheus.target(
        'rate(container_cpu_usage_seconds_total{service="' + service_name + '"}[5m]) * 100'
      )
    ), gridPos={x: 0, y: 0, w: 12, h: 8}
  )
  .addPanel(
    // Memory panel
    grafana.graphPanel.new(
      'Memory Usage',
      datasource='Prometheus',
      format='bytes',
    )
    .addTarget(
      prometheus.target(
        'container_memory_working_set_bytes{service="' + service_name + '"}'
      )
    ), gridPos={x: 12, y: 0, w: 12, h: 8}
  );

// Generate dashboards –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
{
  'payment-api-dashboard.json': createServiceDashboard('payment-api'),
  'user-api-dashboard.json': createServiceDashboard('user-api'),
  'order-api-dashboard.json': createServiceDashboard('order-api'),
}
```

**Terraform –¥–ª—è Grafana:**

hcl

```hcl
# Provider configuration
terraform {
  required_providers {
    grafana = {
      source = "grafana/grafana"
      version = "~> 2.0"
    }
  }
}

provider "grafana" {
  url  = "http://grafana.example.com"
  auth = var.grafana_api_key
}

# Datasource
resource "grafana_data_source" "prometheus" {
  type = "prometheus"
  name = "Prometheus"
  url  = "http://prometheus:9090"
  
  json_data_encoded = jsonencode({
    httpMethod    = "POST"
    timeInterval  = "30s"
  })
}

# Dashboard from file
resource "grafana_dashboard" "service_overview" {
  config_json = file("${path.module}/dashboards/service-overview.json")
  
  folder = grafana_folder.services.id
}

# Folder
resource "grafana_folder" "services" {
  title = "Service Dashboards"
}

# Alert notification channel
resource "grafana_notification_channel" "slack" {
  name = "Slack Alerts"
  type = "slack"
  
  settings = {
    url         = var.slack_webhook_url
    recipient   = "#alerts"
    uploadImage = true
  }
}

# Variables
variable "grafana_api_key" {
  type      = string
  sensitive = true
}

variable "slack_webhook_url" {
  type      = string
  sensitive = true
}
```

**Prometheus Operator CRDs:**

yaml

```yaml
# PrometheusRule - alerts as code
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: app-alerts
  namespace: monitoring
spec:
  groups:
  - name: app.rules
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) 
        / 
        rate(http_requests_total[5m]) > 0.05
      for: 5m
      labels:
        severity: warning
        team: backend
      annotations:
        summary: "High error rate on {{ $labels.service }}"

# ServiceMonitor - auto-discovery
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: app-metrics
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: myapp
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

# PodMonitor - –¥–ª—è pods –±–µ–∑ service
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: pod-metrics
  namespace: monitoring
spec:
  selector:
    matchLabels:
      monitoring: enabled
  podMetricsEndpoints:
  - port: metrics
    interval: 30s
```

**Monitoring configuration validation:**

python

```python
#!/usr/bin/env python3
"""
Validate monitoring configuration before deployment
"""
import yaml
import json
import sys
from pathlib import Path

class MonitoringValidator:
    def __init__(self):
        self.errors = []
        self.warnings = []
    
    def validate_prometheus_rules(self, rules_file):
        """Validate Prometheus alert rules"""
        with open(rules_file) as f:
            rules = yaml.safe_load(f)
        
        for group in rules.get('groups', []):
            # Check group name
            if not group.get('name'):
                self.errors.append(f"Group missing name in {rules_file}")
            
            for rule in group.get('rules', []):
                # Check required fields
                if 'alert' in rule:
                    self._validate_alert(rule, rules_file)
                elif 'record' in rule:
                    self._validate_recording_rule(rule, rules_file)
    
    def _validate_alert(self, alert, filename):
        """Validate alert rule"""
        required = ['alert', 'expr', 'labels', 'annotations']
        for field in required:
            if field not in alert:
                self.errors.append(
                    f"Alert {alert.get('alert', 'unknown')} missing {field} in {filename}"
                )
        
        # Check severity label
        if 'labels' in alert:
            if 'severity' not in alert['labels']:
                self.warnings.append(
                    f"Alert {alert['alert']} missing severity label"
                )
            
            valid_severities = ['critical', 'warning', 'info']
            if alert['labels'].get('severity') not in valid_severities:
                self.errors.append(
                    f"Alert {alert['alert']} has invalid severity"
                )
        
        # Check for duration
        if 'for' not in alert:
            self.warnings.append(
                f"Alert {alert['alert']} missing 'for' clause - will fire immediately"
            )
        
        # Check annotations
        if 'annotations' in alert:
            required_annotations = ['summary', 'description']
            for anno in required_annotations:
                if anno not in alert['annotations']:
                    self.warnings.append(
                        f"Alert {alert['alert']} missing '{anno}' annotation"
                    )
    
    def _validate_recording_rule(self, rule, filename):
        """Validate recording rule"""
        if not rule.get('record'):
            self.errors.append(f"Recording rule missing name in {filename}")
        
        if not rule.get('expr'):
            self.errors.append(f"Recording rule missing expr in {filename}")
        
        # Check naming convention
        record_name = rule.get('record', '')
        if not ':' in record_name:
            self.warnings.append(
                f"Recording rule {record_name} doesn't follow naming convention (level:metric:operations)"
            )
    
    def validate_grafana_dashboard(self, dashboard_file):
        """Validate Grafana dashboard"""
        with open(dashboard_file) as f:
            dashboard = json.load(f)
        
        # Check required fields
        if 'dashboard' in dashboard:
            dash = dashboard['dashboard']
        else:
            dash = dashboard
        
        if not dash.get('title'):
            self.errors.append(f"Dashboard missing title in {dashboard_file}")
        
        if not dash.get('panels'):
            self.warnings.append(f"Dashboard has no panels in {dashboard_file}")
        
        # Check panel queries
        for panel in dash.get('panels', []):
            if 'targets' in panel:
                for target in panel['targets']:
                    if not target.get('expr'):
                        self.warnings.append(
                            f"Panel {panel.get('title', 'unknown')} has empty query"
                        )
    
    def validate_slo_config(self, slo_file):
        """Validate SLO configuration"""
        with open(slo_file) as f:
            slo = yaml.safe_load(f)
        
        required_fields = ['service', 'slo', 'error_budget_policy']
        for field in required_fields:
            if field not in slo:
                self.errors.append(f"SLO config missing {field} in {slo_file}")
        
        # Validate SLO targets
        if 'slo' in slo:
            for slo_type, config in slo['slo'].items():
                if 'target' not in config:
                    self.errors.append(f"SLO {slo_type} missing target")
                
                target = config.get('target', 0)
                if not (0 < target <= 1):
                    self.errors.append(f"SLO {slo_type} target must be between 0 and 1")
    
    def report(self):
        """Print validation report"""
        print("\n" + "="*60)
        print("MONITORING CONFIGURATION VALIDATION REPORT")
        print("="*60 + "\n")
        
        if self.errors:
            print(f"‚ùå ERRORS ({len(self.errors)}):")
            for error in self.errors:
                print(f"  - {error}")
            print()
        
        if self.warnings:
            print(f"‚ö†Ô∏è  WARNINGS ({len(self.warnings)}):")
            for warning in self.warnings:
                print(f"  - {warning}")
            print()
        
        if not self.errors and not self.warnings:
            print("‚úÖ All validations passed!")
        
        print("="*60 + "\n")
        
        return len(self.errors) == 0

# Usage
if __name__ == "__main__":
    validator = MonitoringValidator()
    
    # Validate all configs
    for rules_file in Path('alerts/').glob('*.yaml'):
        validator.validate_prometheus_rules(rules_file)
    
    for dashboard_file in Path('dashboards/').glob('*.json'):
        validator.validate_grafana_dashboard(dashboard_file)
    
    for slo_file in Path('slo/').glob('*.yaml'):
        validator.validate_slo_config(slo_file)
    
    # Report and exit
    success = validator.report()
    sys.exit(0 if success else 1)
```

**CI/CD –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

`.github/workflows/monitoring-ci.yml`:

yaml

```yaml
name: Monitoring CI/CD

on:
  pull_request:
    paths:
      - 'monitoring/**'
  push:
    branches:
      - main
    paths:
      - 'monitoring/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install pyyaml jsonschema promtool-cli
    
    - name: Validate Prometheus rules
      run: |
        promtool check rules monitoring/alerts/*.yaml
    
    - name: Validate dashboards
      run: |
        python scripts/validate_monitoring.py
    
    - name: Lint Jsonnet
      uses: jsonnet-libs/jsonnet-action@main
      with:
        files: monitoring/dashboards/*.jsonnet
    
    - name: Test queries
      run: |
        python scripts/test_queries.py
  
  preview:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
    - uses: actions/checkout@v3
    
    - name: Generate dashboard previews
      run: |
        docker run --rm -v $PWD:/workspace \
          grafana/grafonnet-lib:latest \
          jsonnet -J /workspace/vendor \
          /workspace/monitoring/dashboards/*.jsonnet
    
    - name: Comment PR with changes
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const changes = fs.readFileSync('changes.txt', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## Monitoring Changes\n\n${changes}`
          });
  
  deploy-staging:
    needs: validate
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: staging
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to staging
      run: |
        kubectl apply -f monitoring/alerts/ --context=staging
        
        # Reload Prometheus
        kubectl rollout restart deployment/prometheus -n monitoring --context=staging
    
    - name: Verify deployment
      run: |
        sleep 30
        # Check Prometheus is healthy
        kubectl exec -n monitoring prometheus-0 --context=staging -- \
          wget -q -O - http://localhost:9090/-/healthy
  
  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    environment: production
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        kubectl apply -f monitoring/alerts/ --context=production
        kubectl rollout restart deployment/prometheus -n monitoring --context=production
    
    - name: Verify and rollback if needed
      run: |
        sleep 60
        if ! kubectl exec -n monitoring prometheus-0 --context=production -- \
          wget -q -O - http://localhost:9090/-/healthy; then
          echo "Deployment failed, rolling back"
          git revert HEAD
          kubectl apply -f monitoring/alerts/ --context=production
          exit 1
        fi
```

**Automated dashboard generation:**

python

```python
#!/usr/bin/env python3
"""
Generate Grafana dashboards from service metadata
"""
import json
import yaml
from typing import Dict, List

class DashboardGenerator:
    def __init__(self):
        self.dashboard_id = 0
        self.panel_id = 0
    
    def generate_service_dashboard(self, service_config: Dict) -> Dict:
        """Generate dashboard from service configuration"""
        service_name = service_config['name']
        
        dashboard = {
            'dashboard': {
                'id': None,
                'uid': f"{service_name}-overview",
                'title': f"{service_name.title()} - Service Overview",
                'tags': ['generated', 'service', service_name],
                'timezone': 'browser',
                'refresh': '30s',
                'time': {
                    'from': 'now-6h',
                    'to': 'now'
                },
                'panels': []
            }
        }
        
        y_pos = 0
        
        # Add standard panels
        dashboard['dashboard']['panels'].append(
            self._create_request_rate_panel(service_name, y_pos)
        )
        y_pos += 8
        
        dashboard['dashboard']['panels'].append(
            self._create_error_rate_panel(service_name, y_pos)
        )
        y_pos += 8
        
        dashboard['dashboard']['panels'].append(
            self._create_latency_panel(service_name, y_pos)
        )
        y_pos += 8
        
        # Add custom metrics from config
        for metric in service_config.get('custom_metrics', []):
            dashboard['dashboard']['panels'].append(
                self._create_custom_panel(metric, service_name, y_pos)
            )
            y_pos += 8
        
        return dashboard
    
    def _create_request_rate_panel(self, service: str, y_pos: int) -> Dict:
        """Create request rate panel"""
        self.panel_id += 1
        return {
            'id': self.panel_id,
            'gridPos': {'x': 0, 'y': y_pos, 'w': 12, 'h': 8},
            'type': 'timeseries',
            'title': 'Request Rate',
            'targets': [{
                'expr': f'sum(rate(http_requests_total{{service="{service}"}}[5m]))',
                'legendFormat': 'Requests/sec',
                'refId': 'A'
            }],
            'fieldConfig': {
                'defaults': {
                    'unit': 'reqps',
                    'custom': {
                        'lineWidth': 2,
                        'fillOpacity': 10
                    }
                }
            }
        }
    
    def _create_error_rate_panel(self, service: str, y_pos: int) -> Dict:
        """Create error rate panel"""
        self.panel_id += 1
        return {
            'id': self.panel_id,
            'gridPos': {'x': 12, 'y': y_pos, 'w': 12, 'h': 8},
            'type': 'timeseries',
            'title': 'Error Rate',
            'targets': [{
                'expr': f'''
                    sum(rate(http_requests_total{{service="{service}",status=~"5.."}}[5m]))
                    /
                    sum(rate(http_requests_total{{service="{service}"}}[5m]))
                ''',
                'legendFormat': 'Error %',
                'refId': 'A'
            }],
            'fieldConfig': {
                'defaults': {
                    'unit': 'percentunit',
                    'thresholds': {
                        'mode': 'absolute',
                        'steps': [
                            {'value': 0, 'color': 'green'},
                            {'value': 0.01, 'color': 'yellow'},
                            {'value': 0.05, 'color': 'red'}
                        ]
                    }
                }
            }
        }
    
    def _create_latency_panel(self, service: str, y_pos: int) -> Dict:
        """Create latency panel"""
        self.panel_id += 1
        return {
            'id': self.panel_id,
            'gridPos': {'x': 0, 'y': y_pos, 'w': 24, 'h': 8},
            'type': 'timeseries',
            'title': 'Latency Percentiles',
            'targets': [
                {
                    'expr': f'histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{{service="{service}"}}[5m])) by (le))',
                    'legendFormat': 'P50',
                    'refId': 'A'
                },
                {
                    'expr': f'histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{{service="{service}"}}[5m])) by (le))',
                    'legendFormat': 'P95',
                    'refId': 'B'
                },
                {
                    'expr': f'histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{{service="{service}"}}[5m])) by (le))',
                    'legendFormat': 'P99',
                    'refId': 'C'
                }
            ],
            'fieldConfig': {
                'defaults': {
                    'unit': 's'
                }
            }
        }
    
    def _create_custom_panel(self, metric: Dict, service: str, y_pos: int) -> Dict:
        """Create custom metric panel"""
        self.panel_id += 1
        return {
            'id': self.panel_id,
            'gridPos': {'x': 0, 'y': y_pos, 'w': 12, 'h': 8},
            'type': metric.get('type', 'timeseries'),
            'title': metric['title'],
            'targets': [{
                'expr': metric['query'].replace('$service', service),
                'legendFormat': metric.get('legend', 'Value'),
                'refId': 'A'
            }],
            'fieldConfig': {
                'defaults': {
                    'unit': metric.get('unit', 'short')
                }
            }
        }

# Usage
if __name__ == "__main__":
    # Load service configurations
    with open('services.yaml') as f:
        services = yaml.safe_load(f)
    
    generator = DashboardGenerator()
    
    # Generate dashboards for all services
    for service in services['services']:
        dashboard = generator.generate_service_dashboard(service)
        
        # Save to file
        filename = f"dashboards/generated/{service['name']}-dashboard.json"
        with open(filename, 'w') as f:
            json.dump(dashboard, f, indent=2)
        
        print(f"‚úÖ Generated dashboard: {filename}")
```

`services.yaml` example:

yaml

```yaml
services:
  - name: payment-api
    type: http
    slo:
      availability: 0.999
      latency_p95: 200ms
    custom_metrics:
      - title: "Payment Success Rate"
        query: 'sum(rate(payments_success_total{service="$service"}[5m])) / sum(rate(payments_total{service="$service"}[5m]))'
        type: "gauge"
        unit: "percentunit"
      
      - title: "Transaction Amount"
        query: 'sum(rate(payment_amount_total{service="$service"}[5m]))'
        type: "timeseries"
        unit: "currencyUSD"
  
  - name: user-api
    type: http
    slo:
      availability: 0.99
      latency_p95: 500ms
    custom_metrics:
      - title: "Active Sessions"
        query: 'count(user_sessions{service="$service",status="active"})'
        type: "stat"
```

**Testing Prometheus queries:**

python

```python
#!/usr/bin/env python3
"""
Test Prometheus queries before deployment
"""
import requests
import yaml
from datetime import datetime

class QueryTester:
    def __init__(self, prometheus_url):
        self.prometheus_url = prometheus_url
    
    def test_query(self, query: str) -> tuple:
        """Test if query is valid and returns data"""
        try:
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={'query': query}
            )
            
            if response.status_code != 200:
                return False, f"HTTP {response.status_code}"
            
            data = response.json()
            
            if data['status'] != 'success':
                return False, data.get('error', 'Unknown error')
            
            if not data['data']['result']:
                return False, "No data returned"
            
            return True, "OK"
        
        except Exception as e:
            return False, str(e)
    
    def test_alert_rules(self, rules_file: str):
        """Test all queries in alert rules"""
        with open(rules_file) as f:
            rules = yaml.safe_load(f)
        
        results = []
        
        for group in rules.get('groups', []):
            for rule in group.get('rules', []):
                query = rule.get('expr')
                rule_name = rule.get('alert') or rule.get('record')
                
                success, message = self.test_query(query)
                
                results.append({
                    'rule': rule_name,
                    'success': success,
                    'message': message,
                    'query': query
                })
        
        return results
    
    def report(self, results):
        """Print test report"""
        print("\n" + "="*60)
        print("QUERY TESTING REPORT")
        print("="*60 + "\n")
        
        passed = sum(1 for r in results if r['success'])
        failed = len(results) - passed
        
        print(f"Total: {len(results)} | Passed: {passed} | Failed: {failed}\n")
        
        if failed > 0:
            print("‚ùå FAILED QUERIES:\n")
            for result in results:
                if not result['success']:
                    print(f"  Rule: {result['rule']}")
                    print(f"  Error: {result['message']}")
                    print(f"  Query: {result['query'][:100]}...")
                    print()
        
        print("="*60 + "\n")
        
        return failed == 0

# Usage
if __name__ == "__main__":
    tester = QueryTester("http://localhost:9090")
    
    results = tester.test_alert_rules("alerts/app-alerts.yaml")
    success = tester.report(results)
    
    exit(0 if success else 1)
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É Observability as Code:

1. **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞**:

bash

```bash
monitoring-as-code/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ monitoring-ci.yml
‚îú‚îÄ‚îÄ alerts/
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure.yaml
‚îÇ   ‚îú‚îÄ‚îÄ applications.yaml
‚îÇ   ‚îî‚îÄ‚îÄ slo.yaml
‚îú‚îÄ‚îÄ dashboards/
‚îÇ   ‚îú‚îÄ‚îÄ jsonnet/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ common.libsonnet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ payment-api.jsonnet
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ user-api.jsonnet
‚îÇ   ‚îî‚îÄ‚îÄ generated/
‚îú‚îÄ‚îÄ recording-rules/
‚îÇ   ‚îú‚îÄ‚îÄ slo-rules.yaml
‚îÇ   ‚îî‚îÄ‚îÄ performance-rules.yaml
‚îú‚îÄ‚îÄ slo/
‚îÇ   ‚îú‚îÄ‚îÄ payment-api.yaml
‚îÇ   ‚îî‚îÄ‚îÄ user-api.yaml
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ validate.py
‚îÇ   ‚îú‚îÄ‚îÄ test-queries.py
‚îÇ   ‚îî‚îÄ‚îÄ generate-dashboards.py
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ grafana.tf
‚îÇ   ‚îî‚îÄ‚îÄ prometheus.tf
‚îî‚îÄ‚îÄ README.md
```

2. **–°–æ–∑–¥–∞–π reusable dashboard library**:

`dashboards/jsonnet/lib/common.libsonnet`:

jsonnet

```jsonnet
{
  // Standard –ø–∞–Ω–µ–ª—å –¥–ª—è CPU
  cpuPanel(service, y_pos=0)::
    {
      id: 1,
      gridPos: { x: 0, y: y_pos, w: 12, h: 8 },
      type: 'timeseries',
      title: 'CPU Usage',
      targets: [{
        expr: 'rate(container_cpu_usage_seconds_total{service="' + service + '"}[5m]) * 100',
        legendFormat: 'CPU %',
        refId: 'A',
      }],
      fieldConfig: {
        defaults: {
          unit: 'percent',
          thresholds: {
            steps: [
              { value: 0, color: 'green' },
              { value: 70, color: 'yellow' },
              { value: 90, color: 'red' },
            ],
          },
        },
      },
    },

  // Standard –ø–∞–Ω–µ–ª—å –¥–ª—è Memory
  memoryPanel(service, y_pos=0)::
    {
      id: 2,
      gridPos: { x: 12, y: y_pos, w: 12, h: 8 },
      type: 'timeseries',
      title: 'Memory Usage',
      targets: [{
        expr: 'container_memory_working_set_bytes{service="' + service + '"}',
        legendFormat: 'Memory',
        refId: 'A',
      }],
      fieldConfig: {
        defaults: {
          unit: 'bytes',
        },
      },
    },

  // Standard –ø–∞–Ω–µ–ª—å –¥–ª—è Request Rate
  requestRatePanel(service, y_pos=0)::
    {
      id: 3,
      gridPos: { x: 0, y: y_pos, w: 12, h: 8 },
      type: 'timeseries',
      title: 'Request Rate',
      targets: [{
        expr: 'sum(rate(http_requests_total{service="' + service + '"}[5m]))',
        legendFormat: 'Req/s',
        refId: 'A',
      }],
      fieldConfig: {
        defaults: {
          unit: 'reqps',
        },
      },
    },

  // Standard –ø–∞–Ω–µ–ª—å –¥–ª—è Error Rate
  errorRatePanel(service, y_pos=0)::
    {
      id: 4,
      gridPos: { x: 12, y: y_pos, w: 12, h: 8 },
      type: 'timeseries',
      title: 'Error Rate',
      targets: [{
        expr: |||
          sum(rate(http_requests_total{service="%s",status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total{service="%s"}[5m]))
        ||| % [service, service],
        legendFormat: 'Error %',
        refId: 'A',
      }],
      fieldConfig: {
        defaults: {
          unit: 'percentunit',
          thresholds: {
            steps: [
              { value: 0, color: 'green' },
              { value: 0.01, color: 'yellow' },
              { value: 0.05, color: 'red' }, ], }, }, }, },

// Template –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ service dashboard serviceDashboard(name, panels=[]):: { dashboard: { title: name + ' - Service Dashboard', tags: ['service', name, 'generated'], timezone: 'browser', refresh: '30s', panels: panels, }, }, }

````

3. **–ò—Å–ø–æ–ª—å–∑—É–π library –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ dashboards**:

`dashboards/jsonnet/services/payment-api.jsonnet`:
```jsonnet
local common = import '../lib/common.libsonnet';

local service = 'payment-api';

common.serviceDashboard(service, [
  common.requestRatePanel(service, y_pos=0),
  common.errorRatePanel(service, y_pos=0),
  common.cpuPanel(service, y_pos=8),
  common.memoryPanel(service, y_pos=8),
  
  // Custom –ø–∞–Ω–µ–ª—å –¥–ª—è payment-specific –º–µ—Ç—Ä–∏–∫
  {
    id: 5,
    gridPos: { x: 0, y: 16, w: 24, h: 8 },
    type: 'timeseries',
    title: 'Payment Success Rate',
    targets: [{
      expr: 'sum(rate(payments_success_total{service="' + service + '"}[5m])) / sum(rate(payments_total{service="' + service + '"}[5m]))',
      legendFormat: 'Success Rate',
      refId: 'A',
    }],
    fieldConfig: {
      defaults: {
        unit: 'percentunit',
        min: 0.9,
        max: 1.0,
      },
    },
  },
])
```

4. **Build script –¥–ª—è Jsonnet**:

`scripts/build-dashboards.sh`:
```bash
#!/bin/bash

set -e

JSONNET_DIR="dashboards/jsonnet"
OUTPUT_DIR="dashboards/generated"

mkdir -p "$OUTPUT_DIR"

echo "Building Grafana dashboards from Jsonnet..."

for jsonnet_file in $JSONNET_DIR/services/*.jsonnet; do
  filename=$(basename "$jsonnet_file" .jsonnet)
  output_file="$OUTPUT_DIR/${filename}-dashboard.json"
  
  echo "  Building $filename..."
  jsonnet -J "$JSONNET_DIR/lib" "$jsonnet_file" > "$output_file"
done

echo "‚úÖ All dashboards built successfully!"
```

5. **Automated alert generation from SLO**:

`scripts/generate-slo-alerts.py`:
```python
#!/usr/bin/env python3
"""
Generate Prometheus alert rules from SLO definitions
"""
import yaml
from pathlib import Path

def generate_burn_rate_alerts(service_name, slo_config):
    """Generate multi-window multi-burn-rate alerts"""
    target = slo_config['availability']['target']
    error_budget = 1 - target
    
    alerts = []
    
    # Fast burn (14.4x)
    alerts.append({
        'alert': f'{service_name.replace("-", "_")}_ErrorBudgetFastBurn',
        'expr': f'''
            (
              sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[5m]))
              /
              sum(rate(http_requests_total{{service="{service_name}"}}[5m]))
            ) / {error_budget} > 14.4
            and
            (
              sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[1h]))
              /
              sum(rate(http_requests_total{{service="{service_name}"}}[1h]))
            ) / {error_budget} > 14.4
        ''',
        'for': '2m',
        'labels': {
            'severity': 'critical',
            'service': service_name,
            'slo_type': 'availability'
        },
        'annotations': {
            'summary': f'{service_name} burning error budget too fast',
            'description': 'Error budget will be exhausted in < 2 days',
            'runbook': f'https://runbooks.example.com/{service_name}/error-budget-burn'
        }
    })
    
    # Slow burn (6x)
    alerts.append({
        'alert': f'{service_name.replace("-", "_")}_ErrorBudgetSlowBurn',
        'expr': f'''
            (
              sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[30m]))
              /
              sum(rate(http_requests_total{{service="{service_name}"}}[30m]))
            ) / {error_budget} > 6
            and
            (
              sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[6h]))
              /
              sum(rate(http_requests_total{{service="{service_name}"}}[6h]))
            ) / {error_budget} > 6
        ''',
        'for': '15m',
        'labels': {
            'severity': 'warning',
            'service': service_name,
            'slo_type': 'availability'
        },
        'annotations': {
            'summary': f'{service_name} burning error budget steadily',
            'description': 'Error budget will be exhausted in < 1 week'
        }
    })
    
    return alerts

def generate_latency_alerts(service_name, slo_config):
    """Generate latency SLO alerts"""
    if 'latency' not in slo_config:
        return []
    
    latency_config = slo_config['latency']
    threshold = latency_config['threshold_ms'] / 1000  # Convert to seconds
    
    return [{
        'alert': f'{service_name.replace("-", "_")}_LatencySLOViolation',
        'expr': f'''
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket{{service="{service_name}"}}[5m])) by (le)
            ) > {threshold}
        ''',
        'for': '10m',
        'labels': {
            'severity': 'warning',
            'service': service_name,
            'slo_type': 'latency'
        },
        'annotations': {
            'summary': f'{service_name} latency SLO violation',
            'description': f'P95 latency is above {latency_config["threshold_ms"]}ms'
        }
    }]

def main():
    output_file = 'alerts/generated-slo-alerts.yaml'
    all_alerts = []
    
    # Load all SLO configs
    for slo_file in Path('slo/').glob('*.yaml'):
        with open(slo_file) as f:
            slo_config = yaml.safe_load(f)
        
        service_name = slo_config['service']
        print(f"Generating alerts for {service_name}...")
        
        # Generate alerts
        alerts = []
        alerts.extend(generate_burn_rate_alerts(service_name, slo_config))
        alerts.extend(generate_latency_alerts(service_name, slo_config))
        
        all_alerts.extend(alerts)
    
    # Write to file
    output = {
        'groups': [{
            'name': 'generated_slo_alerts',
            'interval': '30s',
            'rules': all_alerts
        }]
    }
    
    Path('alerts').mkdir(exist_ok=True)
    with open(output_file, 'w') as f:
        yaml.dump(output, f, default_flow_style=False, sort_keys=False)
    
    print(f"\n‚úÖ Generated {len(all_alerts)} alerts ‚Üí {output_file}")

if __name__ == "__main__":
    main()
```

6. **Complete CI/CD pipeline**:
```bash
# Makefile –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞

.PHONY: all validate test build deploy clean

all: validate test build

validate:
	@echo "Validating monitoring configuration..."
	python scripts/validate.py
	promtool check rules alerts/*.yaml
	promtool check rules recording-rules/*.yaml

test:
	@echo "Testing Prometheus queries..."
	python scripts/test-queries.py

build:
	@echo "Building dashboards from Jsonnet..."
	./scripts/build-dashboards.sh
	
	@echo "Generating SLO alerts..."
	python scripts/generate-slo-alerts.py

deploy-staging:
	@echo "Deploying to staging..."
	kubectl apply -f alerts/ --context=staging --namespace=monitoring
	kubectl apply -f recording-rules/ --context=staging --namespace=monitoring
	./scripts/upload-dashboards.sh staging

deploy-production:
	@echo "Deploying to production..."
	kubectl apply -f alerts/ --context=production --namespace=monitoring
	kubectl apply -f recording-rules/ --context=production --namespace=monitoring
	./scripts/upload-dashboards.sh production

clean:
	rm -rf dashboards/generated/*
	rm -f alerts/generated-*.yaml
```

7. **Testing**:
```bash
# Clone repo
git clone <monitoring-as-code-repo>
cd monitoring-as-code

# Validate
make validate

# Test queries
make test

# Build dashboards
make build

# Check generated files
ls dashboards/generated/
ls alerts/generated-*

# Deploy to staging
make deploy-staging

# If all good, deploy to production
make deploy-production
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Auto-scaling monitoring infrastructure**:

`terraform/prometheus-autoscaling.tf`:
```hcl
resource "kubernetes_horizontal_pod_autoscaler_v2" "prometheus" {
  metadata {
    name      = "prometheus"
    namespace = "monitoring"
  }

  spec {
    scale_target_ref {
      api_version = "apps/v1"
      kind        = "Deployment"
      name        = "prometheus"
    }

    min_replicas = 2
    max_replicas = 10

    metric {
      type = "Resource"
      resource {
        name = "cpu"
        target {
          type                = "Utilization"
          average_utilization = 70
        }
      }
    }

    metric {
      type = "Resource"
      resource {
        name = "memory"
        target {
          type                = "Utilization"
          average_utilization = 80
        }
      }
    }

    behavior {
      scale_up {
        stabilization_window_seconds = 60
        select_policy                = "Max"
        policy {
          type          = "Percent"
          value         = 100
          period_seconds = 60
        }
      }

      scale_down {
        stabilization_window_seconds = 300
        select_policy                = "Min"
        policy {
          type          = "Percent"
          value         = 10
          period_seconds = 60
        }
      }
    }
  }
}
```

**2. Monitoring cost optimization**:

`scripts/analyze-costs.py`:
```python
#!/usr/bin/env python3
"""
Analyze monitoring costs and suggest optimizations
"""
import requests
from datetime import datetime, timedelta

class CostAnalyzer:
    def __init__(self, prometheus_url):
        self.prom = prometheus_url
    
    def analyze_metric_cardinality(self):
        """Find high cardinality metrics"""
        query = 'count by (__name__) ({__name__!=""})'
        response = requests.get(
            f"{self.prom}/api/v1/query",
            params={'query': query}
        )
        
        metrics = response.json()['data']['result']
        high_cardinality = [
            m for m in metrics 
            if int(m['value'][1]) > 10000
        ]
        
        print("\nüîç High Cardinality Metrics (>10k series):")
        for metric in sorted(high_cardinality, 
                           key=lambda x: int(x['value'][1]), 
                           reverse=True)[:10]:
            print(f"  {metric['metric']['__name__']}: {metric['value'][1]} series")
    
    def analyze_unused_metrics(self):
        """Find metrics that are never queried"""
        # This would require query logs analysis
        print("\nüìä Metrics Analysis:")
        print("  Recommendation: Enable Prometheus query logging")
        print("  kubectl edit prometheus -n monitoring")
        print("  Add: --enable-feature=query-logging")
    
    def estimate_storage_costs(self):
        """Estimate storage costs"""
        # Get TSDB size
        response = requests.get(f"{self.prom}/api/v1/status/tsdb")
        tsdb_stats = response.json()['data']
        
        total_series = tsdb_stats['seriesCountByMetricName']
        total_count = sum(total_series[i]['value'] for i in range(len(total_series)))
        
        # Rough estimate: 1-2 bytes per sample
        samples_per_day = total_count * 86400 / 15  # 15s scrape interval
        storage_per_day_gb = (samples_per_day * 1.5) / 1024 / 1024 / 1024
        
        print(f"\nüí∞ Storage Cost Estimate:")
        print(f"  Total series: {total_count:,}")
        print(f"  Est. storage/day: {storage_per_day_gb:.2f} GB")
        print(f"  Est. storage/month: {storage_per_day_gb * 30:.2f} GB")
        print(f"  Est. cost/month: ${storage_per_day_gb * 30 * 0.10:.2f} (at $0.10/GB)")

if __name__ == "__main__":
    analyzer = CostAnalyzer("http://localhost:9090")
    
    analyzer.analyze_metric_cardinality()
    analyzer.analyze_unused_metrics()
    analyzer.estimate_storage_costs()
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 10

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å Observability as Code –ø–æ–¥—Ö–æ–¥
‚úÖ –•—Ä–∞–Ω–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ Git
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Jsonnet –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ dashboards
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ alerts –∏–∑ SLO
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å CI/CD –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚úÖ –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–µ—Ä–µ–¥ deploy
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Prometheus queries
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Terraform –¥–ª—è Grafana
‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å dashboards –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ
‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å routine –∑–∞–¥–∞—á–∏
‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å monitoring infrastructure

**Observability as Code Benefits:**
```

1. –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - history –∏ rollback
2. –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ - DRY principle
3. –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å - –æ–¥–∏–Ω–∞–∫–æ–≤–æ –≤–µ–∑–¥–µ
4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - catch bugs early
5. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è - less manual work
6. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è - –∫–æ–¥ = –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
7. Collaboration - code review –ø—Ä–æ—Ü–µ—Å—Å

```

**Production Checklist:**
- ‚úÖ –í—Å–µ dashboards –≤ Git
- ‚úÖ –í—Å–µ alerts –≤ Git
- ‚úÖ CI/CD pipeline –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- ‚úÖ Validation –ø–µ—Ä–µ–¥ deployment
- ‚úÖ Query testing automated
- ‚úÖ Dashboard generation automated
- ‚úÖ SLO-based alert generation
- ‚úÖ Terraform –¥–ª—è infrastructure
- ‚úÖ Rollback mechanism
- ‚úÖ Cost monitoring enabled
- ‚úÖ Documentation as code
- ‚úÖ Regular reviews –∏ cleanup


## –ú–æ–¥—É–ª—å 11: Database –∏ Storage Monitoring - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (45 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ü–æ—á–µ–º—É Database Monitoring –∫—Ä–∏—Ç–∏—á–µ–Ω:**

```
Database = –°–µ—Ä–¥—Ü–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

–ü—Ä–æ–±–ª–µ–º—ã —Å –ë–î ‚Üí –ü—Ä–æ–±–ª–µ–º—ã –≤–µ–∑–¥–µ:
- –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã ‚Üí Slow API
- –í—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ ‚Üí Timeouts
- Deadlocks ‚Üí Failed transactions
- –ü–æ–ª–Ω—ã–π –¥–∏—Å–∫ ‚Üí Downtime
- Connection pool exhausted ‚Üí Service unavailable

Database downtime = Application downtime
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Performance Metrics               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Query response time (latency)     ‚îÇ
‚îÇ ‚Ä¢ Queries per second (throughput)   ‚îÇ
‚îÇ ‚Ä¢ Slow queries count                ‚îÇ
‚îÇ ‚Ä¢ Cache hit ratio                   ‚îÇ
‚îÇ ‚Ä¢ Index usage                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Resource Metrics                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ CPU usage                         ‚îÇ
‚îÇ ‚Ä¢ Memory usage                      ‚îÇ
‚îÇ ‚Ä¢ Disk I/O (IOPS, throughput)       ‚îÇ
‚îÇ ‚Ä¢ Network I/O                       ‚îÇ
‚îÇ ‚Ä¢ Connection count                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Availability Metrics              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Uptime                            ‚îÇ
‚îÇ ‚Ä¢ Replication lag                   ‚îÇ
‚îÇ ‚Ä¢ Failed connections                ‚îÇ
‚îÇ ‚Ä¢ Lock wait time                    ‚îÇ
‚îÇ ‚Ä¢ Deadlocks                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Capacity Metrics                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Disk space usage                  ‚îÇ
‚îÇ ‚Ä¢ Table/index sizes                 ‚îÇ
‚îÇ ‚Ä¢ Transaction log size              ‚îÇ
‚îÇ ‚Ä¢ Connection pool usage             ‚îÇ
‚îÇ ‚Ä¢ Buffer pool usage                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**PostgreSQL –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ PostgreSQL:**

sql

```sql
-- Active connections
SELECT count(*) FROM pg_stat_activity WHERE state = 'active';

-- Long running queries
SELECT 
  pid,
  now() - query_start AS duration,
  query,
  state
FROM pg_stat_activity
WHERE state != 'idle'
  AND now() - query_start > interval '5 minutes'
ORDER BY duration DESC;

-- Database size
SELECT 
  pg_database.datname,
  pg_size_pretty(pg_database_size(pg_database.datname)) AS size
FROM pg_database
ORDER BY pg_database_size(pg_database.datname) DESC;

-- Table sizes
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;

-- Cache hit ratio (should be > 99%)
SELECT 
  sum(heap_blks_read) as heap_read,
  sum(heap_blks_hit) as heap_hit,
  sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) * 100 AS cache_hit_ratio
FROM pg_statio_user_tables;

-- Index usage
SELECT 
  schemaname,
  tablename,
  indexname,
  idx_scan as index_scans,
  idx_tup_read as tuples_read,
  idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;

-- Unused indexes (candidates for deletion)
SELECT 
  schemaname,
  tablename,
  indexname,
  pg_size_pretty(pg_relation_size(indexrelid)) AS size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexrelid NOT IN (
    SELECT indexrelid FROM pg_index WHERE indisprimary
  )
ORDER BY pg_relation_size(indexrelid) DESC;

-- Bloat estimation
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
  pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - 
                 pg_relation_size(schemaname||'.'||tablename)) AS index_size
FROM pg_tables
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;

-- Locks
SELECT 
  locktype,
  relation::regclass,
  mode,
  transactionid AS tid,
  virtualtransaction AS vtid,
  pid,
  granted
FROM pg_catalog.pg_locks
WHERE NOT granted;

-- Replication lag (if using replication)
SELECT 
  client_addr,
  state,
  pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) / 1024 / 1024 AS lag_mb
FROM pg_stat_replication;

-- Vacuum and analyze stats
SELECT 
  schemaname,
  tablename,
  last_vacuum,
  last_autovacuum,
  last_analyze,
  last_autoanalyze,
  n_dead_tup
FROM pg_stat_user_tables
ORDER BY n_dead_tup DESC
LIMIT 10;
```

**PostgreSQL Exporter –¥–ª—è Prometheus:**

yaml

```yaml
# postgres_exporter configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-exporter-queries
  namespace: monitoring
data:
  queries.yaml: |
    # Custom queries –¥–ª—è postgres_exporter
    
    pg_database:
      query: |
        SELECT 
          datname,
          pg_database_size(datname) as size_bytes,
          numbackends as connections
        FROM pg_database
      metrics:
        - datname:
            usage: "LABEL"
            description: "Database name"
        - size_bytes:
            usage: "GAUGE"
            description: "Database size in bytes"
        - connections:
            usage: "GAUGE"
            description: "Number of backends currently connected"
    
    pg_slow_queries:
      query: |
        SELECT 
          COUNT(*) as count
        FROM pg_stat_activity
        WHERE state != 'idle'
          AND now() - query_start > interval '5 seconds'
      metrics:
        - count:
            usage: "GAUGE"
            description: "Number of queries running longer than 5 seconds"
    
    pg_cache_hit_ratio:
      query: |
        SELECT 
          sum(heap_blks_hit) / NULLIF(sum(heap_blks_hit) + sum(heap_blks_read), 0) * 100 as ratio
        FROM pg_statio_user_tables
      metrics:
        - ratio:
            usage: "GAUGE"
            description: "Cache hit ratio percentage"
    
    pg_replication_lag:
      query: |
        SELECT 
          client_addr,
          pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) as lag_bytes
        FROM pg_stat_replication
      metrics:
        - client_addr:
            usage: "LABEL"
            description: "Replica address"
        - lag_bytes:
            usage: "GAUGE"
            description: "Replication lag in bytes"
    
    pg_table_bloat:
      query: |
        SELECT 
          schemaname || '.' || tablename as table_name,
          n_dead_tup as dead_tuples,
          n_live_tup as live_tuples,
          CASE 
            WHEN n_live_tup > 0 
            THEN n_dead_tup::float / n_live_tup::float 
            ELSE 0 
          END as bloat_ratio
        FROM pg_stat_user_tables
        WHERE n_live_tup > 0
      metrics:
        - table_name:
            usage: "LABEL"
            description: "Table name"
        - dead_tuples:
            usage: "GAUGE"
            description: "Number of dead tuples"
        - live_tuples:
            usage: "GAUGE"
            description: "Number of live tuples"
        - bloat_ratio:
            usage: "GAUGE"
            description: "Ratio of dead to live tuples"
```

**MySQL –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

sql

```sql
-- Connection stats
SHOW STATUS LIKE 'Threads_connected';
SHOW STATUS LIKE 'Max_used_connections';
SHOW VARIABLES LIKE 'max_connections';

-- Query performance
SELECT 
  DIGEST_TEXT as query,
  COUNT_STAR as exec_count,
  AVG_TIMER_WAIT/1000000000000 as avg_time_sec,
  SUM_ROWS_EXAMINED as rows_examined,
  SUM_ROWS_SENT as rows_sent
FROM performance_schema.events_statements_summary_by_digest
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 10;

-- Slow queries
SELECT 
  sql_text,
  current_schema,
  rows_examined,
  rows_sent,
  created
FROM performance_schema.events_statements_history
WHERE timer_wait > 5000000000000  -- 5 seconds in picoseconds
ORDER BY timer_wait DESC;

-- Table sizes
SELECT 
  table_schema,
  table_name,
  ROUND((data_length + index_length) / 1024 / 1024, 2) AS size_mb,
  table_rows
FROM information_schema.TABLES
WHERE table_schema NOT IN ('mysql', 'information_schema', 'performance_schema')
ORDER BY (data_length + index_length) DESC
LIMIT 10;

-- InnoDB buffer pool hit ratio
SHOW STATUS LIKE 'Innodb_buffer_pool_read_requests';
SHOW STATUS LIKE 'Innodb_buffer_pool_reads';
-- Hit ratio = (read_requests - reads) / read_requests * 100
-- Should be > 99%

-- Lock waits
SELECT 
  r.trx_id waiting_trx_id,
  r.trx_mysql_thread_id waiting_thread,
  r.trx_query waiting_query,
  b.trx_id blocking_trx_id,
  b.trx_mysql_thread_id blocking_thread,
  b.trx_query blocking_query
FROM information_schema.innodb_lock_waits w
INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id
INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;

-- Replication status
SHOW SLAVE STATUS\G
-- Check: Seconds_Behind_Master should be 0 or low
```

**Redis –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

bash

```bash
# Redis INFO command sections
redis-cli INFO

# Key metrics:
redis-cli INFO stats | grep -E 'total_commands_processed|instantaneous_ops_per_sec'
redis-cli INFO memory | grep -E 'used_memory|used_memory_peak|mem_fragmentation_ratio'
redis-cli INFO replication | grep -E 'role|connected_slaves|master_repl_offset'
redis-cli INFO clients | grep -E 'connected_clients|blocked_clients'
redis-cli INFO persistence | grep -E 'rdb_last_save_time|aof_enabled'

# Slowlog
redis-cli SLOWLOG GET 10

# Connected clients
redis-cli CLIENT LIST

# Key statistics
redis-cli --bigkeys

# Memory usage by key pattern
redis-cli --memkeys

# Hit rate
redis-cli INFO stats | grep keyspace_hits
redis-cli INFO stats | grep keyspace_misses
# Hit rate = hits / (hits + misses)
```

**MongoDB –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

javascript

```javascript
// Connection stats
db.serverStatus().connections

// Current operations
db.currentOp()

// Slow queries (from profiler)
db.system.profile.find({millis: {$gt: 100}}).sort({ts: -1}).limit(10)

// Database stats
db.stats()

// Collection stats
db.collection.stats()

// Index usage
db.collection.aggregate([
  { $indexStats: {} }
])

// Replication lag
rs.status()
rs.printSlaveReplicationInfo()

// Lock statistics
db.serverStatus().locks

// WiredTiger cache
db.serverStatus().wiredTiger.cache
```

**PromQL –¥–ª—è Database –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

promql

```promql
# PostgreSQL

## Connection usage
pg_stat_database_numbackends / pg_settings_max_connections * 100

## Cache hit ratio
rate(pg_stat_database_blks_hit[5m]) 
/ 
(rate(pg_stat_database_blks_hit[5m]) + rate(pg_stat_database_blks_read[5m])) * 100

## Active queries
pg_stat_activity_count{state="active"}

## Long running queries
pg_slow_queries_count

## Replication lag
pg_replication_lag_bytes / 1024 / 1024  # Convert to MB

## Deadlocks
rate(pg_stat_database_deadlocks[5m])

## Transaction rate
rate(pg_stat_database_xact_commit[5m]) + rate(pg_stat_database_xact_rollback[5m])

## Disk usage
pg_database_size_bytes / 1024 / 1024 / 1024  # Convert to GB

# MySQL

## Connection usage
mysql_global_status_threads_connected / mysql_global_variables_max_connections * 100

## Buffer pool hit ratio
(
  mysql_global_status_innodb_buffer_pool_read_requests
  - 
  mysql_global_status_innodb_buffer_pool_reads
)
/
mysql_global_status_innodb_buffer_pool_read_requests * 100

## Query rate
rate(mysql_global_status_queries[5m])

## Slow queries
rate(mysql_global_status_slow_queries[5m])

## Replication lag
mysql_slave_status_seconds_behind_master

## Table locks
rate(mysql_global_status_table_locks_waited[5m])

# Redis

## Memory usage
redis_memory_used_bytes / redis_memory_max_bytes * 100

## Hit rate
rate(redis_keyspace_hits_total[5m])
/
(rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])) * 100

## Connected clients
redis_connected_clients

## Commands per second
rate(redis_commands_processed_total[5m])

## Evicted keys
rate(redis_evicted_keys_total[5m])

## Replication lag
redis_master_repl_offset - redis_slave_repl_offset

# MongoDB

## Connection usage
mongodb_connections{state="current"} / mongodb_connections{state="available"} * 100

## Operation rate
rate(mongodb_op_counters_total[5m])

## Page faults
rate(mongodb_extra_info_page_faults[5m])

## Replication lag
mongodb_mongod_replset_member_replication_lag

## Lock queue
mongodb_locks_timeAcquiringMicros{type="Global"}
```

**Storage –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (Disk I/O):**

promql

```promql
# Disk usage
(
  node_filesystem_size_bytes{mountpoint="/"}
  -
  node_filesystem_avail_bytes{mountpoint="/"}
)
/
node_filesystem_size_bytes{mountpoint="/"} * 100

# Disk I/O rate
rate(node_disk_read_bytes_total[5m])
rate(node_disk_written_bytes_total[5m])

# IOPS
rate(node_disk_reads_completed_total[5m])
rate(node_disk_writes_completed_total[5m])

# Disk latency
rate(node_disk_read_time_seconds_total[5m]) 
/ 
rate(node_disk_reads_completed_total[5m])

rate(node_disk_write_time_seconds_total[5m])
/
rate(node_disk_writes_completed_total[5m])

# Disk queue length
node_disk_io_time_weighted_seconds_total

# Inode usage
(
  node_filesystem_files{mountpoint="/"}
  -
  node_filesystem_files_free{mountpoint="/"}
)
/
node_filesystem_files{mountpoint="/"} * 100
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ PostgreSQL –∏ Redis:

1. **Deploy PostgreSQL —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º**:

`k8s-manifests/postgres-with-monitoring.yaml`:

yaml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: database

---
# PostgreSQL
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: database
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:16-alpine
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_DB
          value: "testdb"
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2
            memory: 4Gi
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: database
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  clusterIP: None

---
# PostgreSQL Exporter
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-exporter
  namespace: database
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres-exporter
  template:
    metadata:
      labels:
        app: postgres-exporter
    spec:
      containers:
      - name: postgres-exporter
        image: prometheuscommunity/postgres-exporter:latest
        ports:
        - containerPort: 9187
          name: metrics
        env:
        - name: DATA_SOURCE_NAME
          value: "postgresql://postgres:password@postgres:5432/testdb?sslmode=disable"
        - name: PG_EXPORTER_EXTEND_QUERY_PATH
          value: "/etc/postgres-exporter/queries.yaml"
        volumeMounts:
        - name: queries
          mountPath: /etc/postgres-exporter
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: queries
        configMap:
          name: postgres-exporter-queries

---
apiVersion: v1
kind: Service
metadata:
  name: postgres-exporter
  namespace: database
  labels:
    app: postgres-exporter
spec:
  selector:
    app: postgres-exporter
  ports:
  - port: 9187
    targetPort: 9187
    name: metrics

---
# ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: postgres-exporter
  namespace: database
spec:
  selector:
    matchLabels:
      app: postgres-exporter
  endpoints:
  - port: metrics
    interval: 30s

---
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: database
type: Opaque
stringData:
  password: "your-secure-password"
```

2. **Deploy Redis —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º**:

`k8s-manifests/redis-with-monitoring.yaml`:

yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: database
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
          name: redis
        command:
        - redis-server
        - "--maxmemory"
        - "512mb"
        - "--maxmemory-policy"
        - "allkeys-lru"
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
      
      - name: redis-exporter
        image: oliver006/redis_exporter:latest
        ports:
        - containerPort: 9121
          name: metrics
        env:
        - name: REDIS_ADDR
          value: "localhost:6379"
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: database
  labels:
    app: redis
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
    name: redis
  - port: 9121
    targetPort: 9121
    name: metrics

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: redis
  namespace: database
spec:
  selector:
    matchLabels:
      app: redis
  endpoints:
  - port: metrics
    interval: 30s
```

3. **Prometheus alert rules –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö**:

`k8s-manifests/database-alerts.yaml`:

yaml

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: database-alerts
  namespace: monitoring
spec:
  groups:
  - name: postgresql.rules
    interval: 30s
    rules:
    # Connection pool near limit
    - alert: PostgreSQLConnectionPoolNearLimit
      expr: |
        pg_stat_database_numbackends / pg_settings_max_connections * 100 > 80
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL connection pool near limit"
        description: "{{ $labels.instance }} using {{ $value | humanize }}% of connections"
    
    # Cache hit ratio low
    - alert: PostgreSQLLowCacheHitRatio
      expr: |
        rate(pg_stat_database_blks_hit[5m]) 
        / 
        (rate(pg_stat_database_blks_hit[5m]) + rate(pg_stat_database_blks_read[5m])) * 100 < 90
      for: 10m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL low cache hit ratio"
        description: "Cache hit ratio is {{ $value | humanize }}% (should be > 99%)"
    
    # Too many slow queries
    - alert: PostgreSQLTooManySlowQueries
      expr: |
        pg_slow_queries_count > 10
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL has many slow queries"
        description: "{{ $value }} queries running longer than 5 seconds"
    
    # Replication lag high
    - alert: PostgreSQLReplicationLagHigh
      expr: |
        pg_replication_lag_bytes / 1024 / 1024 > 100
      for: 5m
      labels:
        severity: critical
        component: database
      annotations:
        summary: "PostgreSQL replication lag high"
        description: "Replication lag is {{ $value | humanize }} MB"
    
    # Database size growing fast
    - alert: PostgreSQLDatabaseGrowingFast
      expr: |
        predict_linear(pg_database_size_bytes[1h], 24*3600) > 
        pg_database_size_bytes * 1.5
      for: 1h
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL database growing fast"
        description: "Database {{ $labels.datname }} will grow 50% in 24h"
    
    # Deadlocks detected
    - alert: PostgreSQLDeadlocksDetected
      expr: |
        rate(pg_stat_database_deadlocks[5m]) > 0
      for: 1m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "PostgreSQL deadlocks detected"
        description: "{{ $value }} deadlocks/sec in {{ $labels.datname }}"
  
  - name: redis.rules
    interval: 30s
    rules:
    # Memory usage high
    - alert: RedisMemoryUsageHigh
      expr: |
        redis_memory_used_bytes / redis_memory_max_bytes * 100 > 90
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "Redis memory usage high"
        description: "{{ $labels.instance }} using {{ $value | humanize }}% memory"
    
    # Hit rate low
    - alert: RedisHitRateLow
      expr: |
        rate(redis_keyspace_hits_total[5m])
        /
        (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])) * 100 < 80
      for: 10m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "Redis hit rate low"
        description: "Hit rate is {{ $value | humanize }}% (should be > 95%)"
    
    # Too many evicted keys
    - alert: RedisTooManyEvictions
      expr: |
        rate(redis_evicted_keys_total[5m]) > 10
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "Redis evicting too many keys"
        description: "{{ $value }} keys/sec being evicted"
    
    # Replication lag
    - alert: RedisReplicationLag
      expr: |
        redis_master_repl_offset - redis_slave_repl_offset > 1000000
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "Redis replication lag high"
        description: "Slave is {{ $value }} bytes behind master"
    
    # Too many connected clients
    - alert: RedisTooManyClients
      expr: |
        redis_connected_clients > redis_config_maxclients * 0.8
      for: 5m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "Redis too many clients"
        description: "{{ $value }} clients connected (80% of max)"
```

4. **Load generator –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**:

`test/database-load-generator.py`:

python

```python
#!/usr/bin/env python3
"""
Database load generator for testing monitoring
"""
import psycopg2
import redis
import time
import random
from threading import Thread

class PostgreSQLLoadGenerator:
    def __init__(self, host, database, user, password):
        self.conn_params = {
            'host': host,
            'database': database,
            'user': user,
            'password': password
        }
    
    def create_test_data(self):
        """Create test tables and data"""
        conn = psycopg2.connect(**self.conn_params)
        cur = conn.cursor()
        
        # Create test table
        cur.execute("""
            CREATE TABLE IF NOT EXISTS test_data (
                id SERIAL PRIMARY KEY,
                name VARCHAR(100),
                value INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Create index
        cur.execute("""
            CREATE INDEX IF NOT EXISTS idx_test_data_value 
            ON test_data(value)
        """)
        
        # Insert test data
        for i in range(10000):
            cur.execute(
                "INSERT INTO test_data (name, value) VALUES (%s, %s)",
                (f"test_{i}", random.randint(1, 1000))
            )
        
        conn.commit()
        cur.close()
        conn.close()
        print("‚úÖ PostgreSQL test data created")
    
    def generate_normal_load(self):
        """Generate normal database load"""
        while True:
            conn = psycopg2.connect(**self.conn_params)
            cur = conn.cursor()
            
            # Random queries
            query_type = random.choice(['select', 'insert', 'update'])
            
            if query_type == 'select':
                cur.execute(
                    "SELECT * FROM test_data WHERE value > %s LIMIT 10",
                    (random.randint(1, 1000),)
                )
                cur.fetchall()
            
            elif query_type == 'insert':
                cur.execute(
                    "INSERT INTO test_data (name, value) VALUES (%s, %s)",
                    (f"load_test_{random.randint(1, 10000)}", random.randint(1, 1000))
                )
                conn.commit()
            
            elif query_type == 'update':
                cur.execute(
                    "UPDATE test_data SET value = %s WHERE id = %s",
                    (random.randint(1, 1000), random.randint(1, 10000))
                )
                conn.commit()
            
            cur.close()
            conn.close()
            
            time.sleep(random.uniform(0.1, 0.5))
    
    def generate_slow_queries(self):
        """Generate intentionally slow queries"""
        while True:
            conn = psycopg2.connect(**self.conn_params)
            cur = conn.cursor()
            
            # Slow query without index
            cur.execute("""
                SELECT * FROM test_data 
                WHERE name LIKE '%test%' 
                ORDER BY created_at DESC
            """)
            cur.fetchall()
            
            cur.close()
            conn.close()
            
            time.sleep(random.uniform(5, 10))

class RedisLoadGenerator:
    def __init__(self, host, port=6379):
        self.redis_client = redis.Redis(host=host, port=port, decode_responses=True)
    
    def generate_load(self):
        """Generate Redis load"""
        while True:
            operation = random.choice(['set', 'get', 'delete'])
            key = f"test_key_{random.randint(1, 1000)}"
  if operation == 'set':
            self.redis_client.set(key, f"value_{random.randint(1, 10000)}", ex=300)
        
        elif operation == 'get':
            self.redis_client.get(key)
        
        elif operation == 'delete':
            self.redis_client.delete(key)
        
        time.sleep(random.uniform(0.01, 0.1))
# Usage

if **name** == "**main**": # PostgreSQL pg_gen = PostgreSQLLoadGenerator( host='localhost', database='testdb', user='postgres', password='password' )

# Create test data
pg_gen.create_test_data()

# Start load generators
Thread(target=pg_gen.generate_normal_load, daemon=True).start()
Thread(target=pg_gen.generate_slow_queries, daemon=True).start()

# Redis
redis_gen = RedisLoadGenerator(host='localhost')
Thread(target=redis_gen.generate_load, daemon=True).start()

print("üî• Load generation started. Press Ctrl+C to stop...")

try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    print("\n‚èπÔ∏è  Stopped")
```


5. **Deploy –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**:
```bash
# Deploy databases
kubectl apply -f k8s-manifests/postgres-with-monitoring.yaml
kubectl apply -f k8s-manifests/redis-with-monitoring.yaml
kubectl apply -f k8s-manifests/database-alerts.yaml

# Check pods
kubectl get pods -n database

# Port forward –¥–ª—è –¥–æ—Å—Ç—É–ø–∞
kubectl port-forward -n database svc/postgres 5432:5432 &
kubectl port-forward -n database svc/redis 6379:6379 &

# Run load generator
python test/database-load-generator.py

# Check metrics
curl http://localhost:9187/metrics | grep pg_
curl http://localhost:9121/metrics | grep redis_

# Check Grafana dashboards
open http://localhost:3000
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Slow query analyzer**:

`scripts/analyze-slow-queries.py`:
```python
#!/usr/bin/env python3
"""
Analyze slow queries from PostgreSQL logs
"""
import psycopg2
from collections import Counter
import re

def analyze_slow_queries(conn_params, threshold_ms=100):
    """Analyze and report slow queries"""
    conn = psycopg2.connect(**conn_params)
    cur = conn.cursor()
    
    # Get slow queries from pg_stat_statements
    # (requires pg_stat_statements extension)
    cur.execute("""
        SELECT 
          query,
          calls,
          total_exec_time / calls as avg_time_ms,
          total_exec_time,
          rows
        FROM pg_stat_statements
        WHERE total_exec_time / calls > %s
        ORDER BY avg_time_ms DESC
        LIMIT 20
    """, (threshold_ms,))
    
    print(f"\n{'='*80}")
    print(f"SLOW QUERIES REPORT (> {threshold_ms}ms)")
    print(f"{'='*80}\n")
    
    for row in cur.fetchall():
        query, calls, avg_time, total_time, rows = row
        
        # Clean query
        clean_query = re.sub(r'\s+', ' ', query).strip()[:100]
        
        print(f"Query: {clean_query}...")
        print(f"  Calls: {calls:,}")
        print(f"  Avg time: {avg_time:.2f}ms")
        print(f"  Total time: {total_time/1000:.2f}s")
        print(f"  Rows: {rows:,}")
        print()
    
    cur.close()
    conn.close()

if __name__ == "__main__":
    analyze_slow_queries({
        'host': 'localhost',
        'database': 'testdb',
        'user': 'postgres',
        'password': 'password'
    })
```

**2. Index recommendation tool**:
```python
#!/usr/bin/env python3
"""
Recommend indexes based on query patterns
"""
import psycopg2

def recommend_indexes(conn_params):
    """Analyze and recommend missing indexes"""
    conn = psycopg2.connect(**conn_params)
    cur = conn.cursor()
    
    print("\nüîç INDEX RECOMMENDATIONS\n")
    
    # Find tables with sequential scans
    cur.execute("""
        SELECT 
          schemaname,
          tablename,
          seq_scan,
          seq_tup_read,
          idx_scan,
          seq_tup_read / NULLIF(seq_scan, 0) as avg_seq_read
        FROM pg_stat_user_tables
        WHERE seq_scan > 0
        ORDER BY seq_tup_read DESC
        LIMIT 10
    """)
    
    print("Tables with high sequential scans (might need indexes):")
    for row in cur.fetchall():
        schema, table, seq_scan, seq_tup_read, idx_scan, avg = row
        if idx_scan is None or seq_scan > idx_scan * 10:
            print(f"\n  ‚ö†Ô∏è  {schema}.{table}")
            print(f"     Sequential scans: {seq_scan:,}")
            print(f"     Tuples read: {seq_tup_read:,}")
            print(f"     Index scans: {idx_scan or 0:,}")
            print(f"     üí° Consider adding index on frequently queried columns")
    
    # Find unused indexes
    cur.execute("""
        SELECT 
          schemaname,
          tablename,
          indexname,
          pg_size_pretty(pg_relation_size(indexrelid)) AS size
        FROM pg_stat_user_indexes
        WHERE idx_scan = 0
          AND indexrelid NOT IN (
            SELECT indexrelid FROM pg_index WHERE indisprimary
          )
        ORDER BY pg_relation_size(indexrelid) DESC
        LIMIT 10
    """)
    
    print("\n\nUnused indexes (candidates for removal):")
    for row in cur.fetchall():
        schema, table, index, size = row
        print(f"\n  ‚ùå {index} on {schema}.{table}")
        print(f"     Size: {size}")
        print(f"     üí° DROP INDEX IF EXISTS {schema}.{index};")
    
    cur.close()
    conn.close()

if __name__ == "__main__":
    recommend_indexes({
        'host': 'localhost',
        'database': 'testdb',
        'user': 'postgres',
        'password': 'password'
    })
```

**3. Backup monitoring**:

`k8s-manifests/backup-monitoring.yaml`:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: database
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:16-alpine
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              BACKUP_FILE="/backups/backup-$(date +%Y%m%d-%H%M%S).sql.gz"
              
              echo "Starting backup..."
              pg_dump -h postgres -U postgres testdb | gzip > $BACKUP_FILE
              
              # Push metric to Pushgateway
              cat <<EOF | curl --data-binary @- http://pushgateway:9091/metrics/job/postgres_backup
              # TYPE postgres_backup_timestamp gauge
              postgres_backup_timestamp $(date +%s)
              # TYPE postgres_backup_size_bytes gauge
              postgres_backup_size_bytes $(stat -f%z $BACKUP_FILE)
              # TYPE postgres_backup_success gauge
              postgres_backup_success 1
              EOF
              
              echo "Backup completed: $BACKUP_FILE"
              
              # Cleanup old backups (keep last 7 days)
              find /backups -name "backup-*.sql.gz" -mtime +7 -delete
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
            volumeMounts:
            - name: backups
              mountPath: /backups
          volumes:
          - name: backups
            persistentVolumeClaim:
              claimName: postgres-backups
          restartPolicy: OnFailure

---
# Alert –Ω–∞ failed backup
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: backup-alerts
  namespace: monitoring
spec:
  groups:
  - name: backup.rules
    rules:
    - alert: PostgreSQLBackupFailed
      expr: |
        time() - postgres_backup_timestamp > 86400 * 2  # No backup for 2 days
      for: 1h
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL backup failed or not running"
        description: "Last successful backup was {{ $value | humanizeDuration }} ago"
    
    - alert: PostgreSQLBackupSizeAnomaly
      expr: |
        postgres_backup_size_bytes < 
        avg_over_time(postgres_backup_size_bytes[7d]) * 0.5
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL backup size anomaly"
        description: "Backup size is significantly smaller than usual"
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 11

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å PostgreSQL (connections, queries, cache, replication)
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å MySQL/MariaDB
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å Redis (memory, hit rate, clients)
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å MongoDB
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å exporters –¥–ª—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å alerts –¥–ª—è database –º–µ—Ç—Ä–∏–∫
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å slow queries
‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å indexes
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å disk I/O –∏ storage
‚úÖ –û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å backup —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ë–î
‚úÖ –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –¥–æ –∏—Ö –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è

**Key Database Metrics:**

Performance: Query time, throughput, cache hit ratio Resources: CPU, Memory, Disk I/O, connections Availability: Uptime, replication lag, deadlocks Capacity: Disk space, table sizes, connection pool


**Production Checklist:**
- ‚úÖ Exporters –¥–ª—è –≤—Å–µ—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ Alerts –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
- ‚úÖ Slow query monitoring
- ‚úÖ Connection pool monitoring
- ‚úÖ Replication lag alerts
- ‚úÖ Backup monitoring
- ‚úÖ Disk space alerts
- ‚úÖ Index optimization
- ‚úÖ Query performance tracking
- ‚úÖ Capacity planning
- ‚úÖ Regular vacuum/analyze (PostgreSQL)
- ‚úÖ Automated backups with verification

## –ú–æ–¥—É–ª—å 12: –ó–∞–∫–ª—é—á–µ–Ω–∏–µ - Production-Ready Monitoring –∏ Best Practices (30 –º–∏–Ω—É—Ç)

### üéØ –ò—Ç–æ–≥–æ–≤—ã–π –æ–±–∑–æ—Ä

**–ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏ –≤ –∫—É—Ä—Å–µ:**

```
–ú–æ–¥—É–ª—å 1:  –û—Å–Ω–æ–≤—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (Metrics, Golden Signals, USE/RED)
–ú–æ–¥—É–ª—å 2:  Prometheus (TSDB, PromQL, scraping)
–ú–æ–¥—É–ª—å 3:  Grafana (Dashboards, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è)
–ú–æ–¥—É–ª—å 4:  –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (Loki, ELK, —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è)
–ú–æ–¥—É–ª—å 5:  Alerting (Alertmanager, notification)
–ú–æ–¥—É–ª—å 6:  Distributed Tracing (Jaeger, OpenTelemetry, APM)
–ú–æ–¥—É–ª—å 7:  Kubernetes Monitoring (kube-state-metrics, pods, nodes)
–ú–æ–¥—É–ª—å 8:  SRE –ø—Ä–∞–∫—Ç–∏–∫–∏ (SLO/SLI/SLA, Error Budget)
–ú–æ–¥—É–ª—å 9:  Security Monitoring (Falco, audit logs, compliance)
–ú–æ–¥—É–ª—å 10: Observability as Code (GitOps, automation)
–ú–æ–¥—É–ª—å 11: Database Monitoring (PostgreSQL, Redis, slow queries)
```

**The Observability Pyramid:**

```
                    ‚ñ≤
                   ‚ï± ‚ï≤
                  ‚ï±   ‚ï≤
                 ‚ï± SRE ‚ï≤
                ‚ï±Practices‚ï≤
               ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
              ‚ï±             ‚ï≤
             ‚ï±   Security   ‚ï≤
            ‚ï±   Monitoring   ‚ï≤
           ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
          ‚ï±                   ‚ï≤
         ‚ï±  Distributed Trace  ‚ï≤
        ‚ï±                       ‚ï≤
       ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
      ‚ï±                           ‚ï≤
     ‚ï±      Logs & Metrics         ‚ï≤
    ‚ï±                               ‚ï≤
   ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤
  ‚ï±                                   ‚ï≤
 ‚ï±        Infrastructure              ‚ï≤
‚ï±                                       ‚ï≤
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

---

## Production-Ready Monitoring Checklist

### üìä 1. Metrics & Monitoring

**Infrastructure Level:**

```
‚úÖ Node Exporter –Ω–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö
‚úÖ cAdvisor –¥–ª—è Docker/Kubernetes
‚úÖ kube-state-metrics –¥–ª—è K8s objects
‚úÖ Prometheus —Å HA setup (–º–∏–Ω–∏–º—É–º 2 replicas)
‚úÖ Long-term storage (Thanos/Cortex/VictoriaMetrics)
‚úÖ Retention policy –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ (15-30 –¥–Ω–µ–π local, 1+ –≥–æ–¥ remote)
```

**Application Level:**

```
‚úÖ /metrics endpoint –Ω–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–∞—Ö
‚úÖ Custom business metrics —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è
‚úÖ ServiceMonitor/PodMonitor –¥–ª—è auto-discovery
‚úÖ Structured logging (JSON —Ñ–æ—Ä–º–∞—Ç)
‚úÖ Correlation IDs –≤–æ –≤—Å–µ—Ö –ª–æ–≥–∞—Ö
‚úÖ Distributed tracing –Ω–∞—Å—Ç—Ä–æ–µ–Ω
```

**Database Level:**

```
‚úÖ PostgreSQL/MySQL/Redis exporters
‚úÖ Slow query monitoring
‚úÖ Connection pool monitoring
‚úÖ Replication lag alerts
‚úÖ Backup monitoring
```

### üö® 2. Alerting

**Alert Quality:**

```
‚úÖ –ö–∞–∂–¥—ã–π alert –∏–º–µ–µ—Ç:
   - –ü–æ–Ω—è—Ç–Ω–æ–µ –∏–º—è
   - Severity (critical/warning/info)
   - Summary –∏ description
   - Runbook URL
   - Dashboard link
   
‚úÖ Alerts —Å–ª–µ–¥—É—é—Ç –ø—Ä–∏–Ω—Ü–∏–ø—É:
   - Actionable (—Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π)
   - Relevant (–≤–∞–∂–µ–Ω –¥–ª—è –±–∏–∑–Ω–µ—Å–∞)
   - Timely (—Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–æ–≤—Ä–µ–º—è)

‚úÖ Alert fatigue prevention:
   - Grouping –Ω–∞—Å—Ç—Ä–æ–µ–Ω
   - Inhibition rules —Ä–∞–±–æ—Ç–∞—é—Ç
   - For clause –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç flapping
   - Severity levels –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ
```

**Alert Coverage:**

```
‚úÖ Infrastructure alerts:
   - Node down
   - High CPU/Memory/Disk
   - Network issues

‚úÖ Application alerts:
   - High error rate
   - High latency
   - API down
   - Service degradation

‚úÖ SLO alerts:
   - Error budget burn rate
   - SLO violation
   - Latency SLO breach

‚úÖ Security alerts:
   - Authentication failures
   - Unauthorized access
   - Security scan failures

‚úÖ Database alerts:
   - Connection pool exhausted
   - Replication lag
   - Slow queries
   - Backup failures
```

### üìà 3. Dashboards

**Standard Dashboards:**

```
‚úÖ Cluster overview (nodes, pods, resources)
‚úÖ Service dashboards (RED metrics)
‚úÖ Database dashboards
‚úÖ SLO dashboards (error budget tracking)
‚úÖ Security dashboards
‚úÖ Cost dashboards (FinOps)
```

**Dashboard Best Practices:**

```
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π variables –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
‚úÖ –î–æ–±–∞–≤–ª—è–π links –Ω–∞ runbooks
‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–∞–Ω–µ–ª–∏ –ª–æ–≥–∏—á–µ—Å–∫–∏ (rows)
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π consistent naming
‚úÖ –î–æ–±–∞–≤–ª—è–π descriptions –∫ –ø–∞–Ω–µ–ª—è–º
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–π refresh intervals
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π appropriate visualizations
‚úÖ –•—Ä–∞–Ω–∏ dashboards –≤ Git
```

### üîç 4. Logging

**Log Infrastructure:**

```
‚úÖ Centralized logging (Loki/ELK)
‚úÖ Structured logs (JSON)
‚úÖ Log retention policy
‚úÖ Log rotation configured
‚úÖ Log levels –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ
```

**Log Content:**

```
‚úÖ Timestamp (UTC)
‚úÖ Log level
‚úÖ Service name
‚úÖ Correlation ID / Trace ID
‚úÖ Error stack traces
‚úÖ Context –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
‚úÖ No sensitive data (passwords, tokens)
```

### üîê 5. Security

**Security Monitoring:**

```
‚úÖ Falco –∏–ª–∏ –∞–Ω–∞–ª–æ–≥ –¥–ª—è runtime security
‚úÖ Audit logging –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
‚úÖ Vulnerability scanning –≤ CI/CD
‚úÖ Secret scanning
‚úÖ WAF monitoring
‚úÖ Authentication/Authorization monitoring
```

**Compliance:**

```
‚úÖ PCI DSS / GDPR / HIPAA requirements
‚úÖ Audit trails
‚úÖ Access logs
‚úÖ Data retention policies
‚úÖ Regular security reports
```

### üéØ 6. SRE Practices

**SLO/SLI:**

```
‚úÖ SLO –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
‚úÖ SLI –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è
‚úÖ Error budget tracking
‚úÖ Multi-window multi-burn-rate alerts
‚úÖ SLO review –ø—Ä–æ—Ü–µ—Å—Å
```

**On-Call:**

```
‚úÖ On-call rotation —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–∞—è
‚úÖ Runbooks –¥–ª—è –≤—Å–µ—Ö alerts
‚úÖ Escalation policy
‚úÖ Post-mortem –ø—Ä–æ—Ü–µ—Å—Å
‚úÖ Incident response playbooks
```

---

## Best Practices Summary

### üé® Design Principles

**1. Start with SLIs/SLOs**

```
‚ùå –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –≤—Å—ë –ø–æ–¥—Ä—è–¥
‚úÖ –ù–∞—á–∏–Ω–∞—Ç—å —Å user-facing metrics

–í–æ–ø—Ä–æ—Å—ã:
- –ß—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π?
- –ö–∞–∫–æ–π —É—Ä–æ–≤–µ–Ω—å –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –Ω—É–∂–µ–Ω?
- –ö–∞–∫–æ–π error budget –¥–æ–ø—É—Å—Ç–∏–º?
```

**2. Monitor Symptoms, Not Causes**

```
‚ùå Alert: "CPU > 80%"
‚úÖ Alert: "API latency > 1s"

–ü—Ä–∏–Ω—Ü–∏–ø: –ê–ª–µ—Ä—Ç–∏—Ç—å –Ω–∞ —Ç–æ, —á—Ç–æ –≤–∏–¥–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
```

**3. Reduce Alert Fatigue**

```
–ü—Ä–∞–≤–∏–ª–æ: –ï—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π ‚Üí –Ω–µ alert, –∞ dashboard

–¢–µ—Ö–Ω–∏–∫–∏:
- Grouping –ø–æ—Ö–æ–∂–∏—Ö alerts
- Inhibition –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö alerts
- For clause –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è flapping
- –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ thresholds
```

**4. Make Alerts Actionable**

```
–ö–∞–∂–¥—ã–π alert –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞:
- –ß—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å?
- –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?
- –ß—Ç–æ –¥–µ–ª–∞—Ç—å? (runbook)
- –ì–¥–µ —Å–º–æ—Ç—Ä–µ—Ç—å? (dashboard)
```

**5. Automate Everything**

```
‚úÖ Dashboard generation
‚úÖ Alert creation from SLO
‚úÖ Configuration validation
‚úÖ Deployment —á–µ—Ä–µ–∑ CI/CD
‚úÖ Cost optimization
```

### üìè Metric Guidelines

**Naming Conventions:**

```
<namespace>_<name>_<unit>_<type>

Examples:
- http_requests_total (counter)
- http_request_duration_seconds (histogram)
- process_cpu_seconds_total (counter)
- node_memory_MemAvailable_bytes (gauge)
```

**Cardinality Management:**

```
‚ùå High cardinality labels:
   - user_id
   - request_id
   - timestamp
   - IP address (full)

‚úÖ Low cardinality labels:
   - service
   - environment
   - status_code
   - method
   - region
```

**Label Best Practices:**

```
‚úÖ Use labels for dimensions you'll filter/aggregate by
‚úÖ Keep label values bounded (< 100 unique values)
‚úÖ Don't use labels for high cardinality data
‚úÖ Use consistent label names across metrics
```

### üéì Query Optimization

**PromQL Performance:**

```
‚ùå Slow:
rate(http_requests_total[5m])

‚úÖ Fast (with recording rule):
job:http_requests:rate5m

–ü—Ä–∏–Ω—Ü–∏–ø: Pre-compute expensive queries
```

**Recording Rules:**

yaml

````yaml
groups:
  - name: performance
    interval: 30s
    rules:
    # Pre-compute commonly used aggregations
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
    
    - record: job:http_request_duration:p95
      expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job))
````

### üîß Troubleshooting Guide

**Common Problems & Solutions:**

**1. High Cardinality**
```
Problem: Prometheus running out of memory
–ü—Ä–∏—á–∏–Ω–∞: Metrics with too many unique label values

Solution:
- –ù–∞–π—Ç–∏ high cardinality metrics: 
  topk(10, count by (__name__)({__name__!=""}))
- Remove –∏–ª–∏ aggregate labels
- Use recording rules
- Adjust retention
```

**2. Missing Metrics**
```
Problem: Metrics not appearing in Prometheus
Checks:
1. Is target up? Check /targets
2. Is ServiceMonitor correct? Check labels
3. Is exporter working? curl /metrics
4. Are firewall rules correct?
5. Check Prometheus logs
```

**3. Alert Flapping**
```
Problem: Alert firing and resolving repeatedly
Solution:
- Add "for" clause (e.g., for: 5m)
- Adjust threshold
- Add hysteresis (different thresholds for firing/resolving)
```

**4. Dashboard Loading Slow**
```
Problem: Grafana dashboard takes long to load
Solutions:
- Use recording rules for expensive queries
- Reduce time range
- Reduce refresh frequency
- Use query caching
- Optimize PromQL queries (avoid regex, use recording rules)
```

### üí∞ Cost Optimization

**Reduce Storage Costs:**
````
1. Adjust retention:
   - Local: 15-30 days
   - Remote: 1-2 years with downsampling

2. Reduce scrape frequency:
   - Non-critical: 60s instead of 15s
   - Development: 120s

3. Drop unused metrics:
   metric_relabel_configs:
     - source_labels: [__name__]
       regex: 'unused_metric.*'
       action: drop

4. Use recording rules:
   - Pre-aggregate expensive queries
   - Reduce query cost

5. Implement sampling:
   - Not all metrics need 100% samples
````

**Monitor Monitoring Costs:**

promql

````promql
# Prometheus storage size
prometheus_tsdb_storage_blocks_bytes

# Number of time series
prometheus_tsdb_head_series

# Cardinality per metric
count by (__name__) ({__name__!=""})

# Sample ingestion rate
rate(prometheus_tsdb_head_samples_appended_total[5m])

# Estimate cost
# samples_per_day * retention_days * cost_per_sample
````

---

## Production Deployment Architecture

**Recommended Setup:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Load Balancer                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇPrometheus‚îÇ        ‚îÇPrometheus‚îÇ  (HA pair)
‚îÇ Primary  ‚îÇ        ‚îÇSecondary ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                    ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Thanos  ‚îÇ        ‚îÇ  Thanos  ‚îÇ  (Long-term storage)
‚îÇ  Store   ‚îÇ        ‚îÇ  Store   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                    ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  S3/GCS   ‚îÇ  (Object storage)
         ‚îÇ  Bucket   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Grafana HA                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ Grafana  ‚îÇ         ‚îÇ Grafana  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ Instance ‚îÇ         ‚îÇ Instance ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ       ‚îÇ                     ‚îÇ               ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                  ‚îÇ                          ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ            ‚îÇ PostgreSQL‚îÇ  (Shared DB)       ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Alertmanager Cluster                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Alert   ‚îÇ ‚îÇ  Alert   ‚îÇ ‚îÇ  Alert   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ manager  ‚îÇ ‚îÇ manager  ‚îÇ ‚îÇ manager  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    1     ‚îÇ ‚îÇ    2     ‚îÇ ‚îÇ    3     ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Components:**

1. **Prometheus HA:**
   - –ú–∏–Ω–∏–º—É–º 2 replicas
   - –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ scrape configs
   - Load balancer –ø–µ—Ä–µ–¥ –Ω–∏–º–∏

2. **Long-term Storage:**
   - Thanos/Cortex/VictoriaMetrics
   - S3/GCS –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
   - Downsampling –¥–ª—è —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö

3. **Grafana HA:**
   - –ú–∏–Ω–∏–º—É–º 2 instances
   - Shared PostgreSQL/MySQL
   - Session storage –≤ DB

4. **Alertmanager Cluster:**
   - 3+ instances (odd number)
   - Gossip protocol –¥–ª—è sync
   - Deduplication –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è

---

## Migration Strategy

**–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Production Monitoring:**

**Phase 1: Foundation (Week 1-2)**
```
‚úÖ Deploy Prometheus + Grafana
‚úÖ Setup node-exporter
‚úÖ Basic infrastructure dashboards
‚úÖ Critical alerts only
‚úÖ Document everything
```

**Phase 2: Application Monitoring (Week 3-4)**
```
‚úÖ Add /metrics endpoints
‚úÖ ServiceMonitors –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
‚úÖ Application dashboards
‚úÖ SLO definition
‚úÖ Application alerts
```

**Phase 3: Advanced (Week 5-6)**
```
‚úÖ Distributed tracing
‚úÖ Security monitoring
‚úÖ Database monitoring
‚úÖ Cost tracking
‚úÖ Automation (GitOps)
```

**Phase 4: Optimization (Week 7-8)**
```
‚úÖ Alert tuning
‚úÖ Dashboard refinement
‚úÖ Performance optimization
‚úÖ Documentation update
‚úÖ Training team
```

---

## Team Structure & Responsibilities

**Who Does What:**
```
SRE Team:
‚îú‚îÄ Maintain monitoring infrastructure
‚îú‚îÄ Create platform dashboards
‚îú‚îÄ Define SLO framework
‚îú‚îÄ Incident response coordination
‚îî‚îÄ Post-mortem facilitation

Development Teams:
‚îú‚îÄ Instrument applications
‚îú‚îÄ Create service dashboards
‚îú‚îÄ Define service SLOs
‚îú‚îÄ Respond to service alerts
‚îî‚îÄ Fix reliability issues

Security Team:
‚îú‚îÄ Security monitoring rules
‚îú‚îÄ Audit log analysis
‚îú‚îÄ Compliance reports
‚îî‚îÄ Security incident response

Platform Team:
‚îú‚îÄ K8s monitoring
‚îú‚îÄ Infrastructure alerts
‚îú‚îÄ Capacity planning
‚îî‚îÄ Cost optimization
```

---

## Learning Resources

**Books:**
```
üìö "Site Reliability Engineering" - Google
üìö "The Site Reliability Workbook" - Google
üìö "Database Reliability Engineering" - O'Reilly
üìö "Prometheus: Up & Running" - O'Reilly
üìö "Observability Engineering" - O'Reilly
```

**Online:**
```
üåê Prometheus Documentation - prometheus.io
üåê Grafana Tutorials - grafana.com/tutorials
üåê SRE Weekly Newsletter - sreweekly.com
üåê Cloud Native Computing Foundation - cncf.io
üåê PromCon Talks - youtube.com/@PrometheusIo
```

**Community:**
```
üí¨ CNCF Slack - cloud-native.slack.com
üí¨ Prometheus Users - groups.google.com/g/prometheus-users
üí¨ Reddit r/devops, r/sre
üí¨ SRE Conferences: SREcon, KubeCon
```

---

## Career Path

**Monitoring & SRE Career:**
```
Junior DevOps/SRE
‚îú‚îÄ Setup basic monitoring
‚îú‚îÄ Create dashboards
‚îú‚îÄ Respond to alerts
‚îî‚îÄ Learn PromQL

    ‚Üì

Mid-Level SRE
‚îú‚îÄ Design monitoring architecture
‚îú‚îÄ Implement SLO/SLI
‚îú‚îÄ Incident response lead
‚îú‚îÄ Automation
‚îî‚îÄ Mentor juniors

    ‚Üì

Senior SRE
‚îú‚îÄ Define SRE strategy
‚îú‚îÄ Platform reliability
‚îú‚îÄ Capacity planning
‚îú‚îÄ Cross-team leadership
‚îî‚îÄ Incident commander

    ‚Üì

Staff/Principal SRE
‚îú‚îÄ Company-wide observability
‚îú‚îÄ SRE practices evangelism
‚îú‚îÄ Architecture decisions
‚îî‚îÄ Industry thought leadership
```

**Skills to Develop:**
```
Technical:
‚úÖ Prometheus, Grafana, Loki
‚úÖ Kubernetes deep knowledge
‚úÖ PromQL mastery
‚úÖ Distributed systems
‚úÖ Performance analysis
‚úÖ Automation (Python, Go)

Soft Skills:
‚úÖ Incident management
‚úÖ Communication (written & verbal)
‚úÖ Blameless post-mortems
‚úÖ Stakeholder management
‚úÖ Teaching/mentoring
‚úÖ On-call leadership
```

---

## Final Checklist

**Before Going to Production:**
```
Infrastructure:
‚ñ° Prometheus HA setup
‚ñ° Long-term storage configured
‚ñ° Backup strategy defined
‚ñ° Disaster recovery tested

Monitoring:
‚ñ° All critical services monitored
‚ñ° SLOs defined and tracked
‚ñ° Dashboards created and shared
‚ñ° Alerts tuned and documented

Process:
‚ñ° On-call rotation established
‚ñ° Runbooks written
‚ñ° Post-mortem process defined
‚ñ° Training completed

Security:
‚ñ° Access control configured
‚ñ° Audit logging enabled
‚ñ° Compliance requirements met
‚ñ° Security scanning automated

Documentation:
‚ñ° Architecture documented
‚ñ° Runbooks updated
‚ñ° Dashboards documented
‚ñ° Contact list current
```

---

## üéì –ü–æ–∑–¥—Ä–∞–≤–ª—è—é!

–¢—ã –ø—Ä–æ—à—ë–ª –ø–æ–ª–Ω—ã–π –∫—É—Ä—Å –ø–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥—É –¥–ª—è DevOps!

**–ß—Ç–æ —Ç—ã —Ç–µ–ø–µ—Ä—å —É–º–µ–µ—à—å:**
- ‚úÖ –°—Ç—Ä–æ–∏—Ç—å production-ready monitoring systems
- ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Prometheus, Grafana, Loki
- ‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ alerts
- ‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å SRE –ø—Ä–∞–∫—Ç–∏–∫–∏
- ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å observability
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å costs
- ‚úÖ Troubleshoot production issues

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**
1. –ü—Ä–∏–º–µ–Ω–∏ –∑–Ω–∞–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ
2. –ü–æ–¥–µ–ª–∏—Å—å –æ–ø—ã—Ç–æ–º —Å –∫–æ–º–∞–Ω–¥–æ–π
3. –ü—Ä–æ–¥–æ–ª–∂–∞–π —É—á–∏—Ç—å—Å—è (SRE - —ç—Ç–æ journey, –Ω–µ destination)
4. –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Å—è –∫ community
5. –ü–æ–º–æ–≥–∞–π –¥—Ä—É–≥–∏–º —É—á–∏—Ç—å—Å—è

**Remember:**
```
"Hope is not a strategy, but monitoring is!"
"You can't fix what you can't see!"
"Measure twice, deploy once!"
````

–£–¥–∞—á–∏ –≤ —Ç–≤–æ—ë–º SRE/DevOps –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–∏!
